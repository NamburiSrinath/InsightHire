{"docstore/data": {"cf4d1930-a979-4f3a-b327-5fb4d21620f5": {"__data__": {"id_": "cf4d1930-a979-4f3a-b327-5fb4d21620f5", "embedding": null, "metadata": {"page_label": "1", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eed2da4e-525c-4b99-8661-e2e2fe5e591f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "32fc0c77aaef98935a28c68f3361cd02bdcccaa5955cbb1602ad1a8aadb77ee6", "class_name": "RelatedNodeInfo"}}, "text": "The Cost of Compression: Investigating the Impact of Compression on\nParametric Knowledge in Language Models\nSatya Sai Srinath Namburi, Makesh Sreedhar, Srinath Srinivasan, Frederic Sala\nUniversity of Wisconsin - Madison\n{sgnamburi, msreedhar, srinivasan32, fsala}@wisc.edu\nAbstract\nCompressing large language models (LLMs),\noften consisting of billions of parameters, pro-\nvides faster inference, smaller memory foot-\nprints, and enables local deployment. Two\nstandard compression techniques are pruning\nand quantization, with the former eliminating\nredundant connections in model layers and\nthe latter representing model parameters with\nfewer bits. The key tradeoff is between the\ndegree of compression and the impact on the\nquality of the compressed model. Existing\nresearch on LLM compression primarily fo-\ncuses on performance in terms of general met-\nrics like perplexity or downstream task accu-\nracy. More \ufb01ne-grained metrics, such as those\nmeasuring parametric knowledge, remain sig-\nni\ufb01cantly underexplored. To help bridge this\ngap, we present a comprehensive analysis\nacross multiple model families ( ENCODER ,\nENCODER -DECODER , and DECODER ) using\nthe LAMA and LM-HARNESS benchmarks in\norder to systematically quantify the effect of\ncommonly employed compression techniques\non model performance. A particular focus is\non tradeoffs involving parametric knowledge,\nwith the goal of providing practitioners with\npractical insights to help make informed deci-\nsions on compression. We release our code-\nbase1to enable further research.\n1 Introduction\nLarge language models (LLMs) have demon-\nstrated exceptional performance across diverse\ntasks. However, their deployment in real-world ap-\nplications is hindered by their substantial size and\nthe associated costs, even for inference ( Schwartz\net al. ,2020 ;Strubell et al. ,2019 ). For in-\nstance, the LLama-65B model ( Touvron et al. ,\n2023 ), a pioneering open-sourced LLM, uses ap-\nproximately 130GB of RAM for 16-bit inference.\nTo address this challenge, recent research has\n1https://github.com/NamburiSrinath/LLMCompressionfocused on developing novel compression tech-\nniques that enable ef\ufb01cient local deployment and\ninference. Notable examples of such techniques\ninclude SparseGPT ( Frantar and Alistarh ,2023 )\nand LLM.int8() ( Dettmers et al. ,2022 ).\nThe tradeoff between model compression and\nquality is typically studied either through general\nmetrics like perplexity ( See et al. ,2016 ;Michel\net al. ,2019 ) or standardized benchmark task accu-\nracy ( Liang et al. ,2021 ;Du et al. ,2021 ) on, e.g.,\nGLUE ( Wang et al. ,2018 ). Furthermore, much of\nthe literature studies such tradeoffs for one model\nor a particular class of models. Unfortunately, as a\nresult, practitioners do not have access to reliable\ninsights or rules-of-thumb to ensure they can make\nan informed decision for compression in their own\nmodels. This is because\n\u2022Metrics like perplexity are too general, while\nbenchmark prediction metrics are too easy\nto fool. For example, recent \ufb01ndings sug-\ngest that distilled versions of foundational\nLLMs, known as imitation models, may ex-\nhibit stylistic similarities but potentially lack\nknowledge when compared to the models\nthey seek to imitate ( Gudibande et al. ,2023 ).\n\u2022Most recent research on compression tech-\nniques has primarily focused on DECODER\nmodels. The applicability and effectiveness\nof such techniques for large ENCODER and\nENCODER -DECODER models ( Chung et al. ,\n2022 ) has yet to be extensively studied.\nThese dif\ufb01culties suggest that there is a need for\na more \ufb01ne-grained understanding of the effects\nof compression schemes, comparing a variety of\nmodel families, compression techniques, and spe-\ncialized measurements.\nWe address these challenges, speci\ufb01cally focus-\ning on the preservation of parametric knowledge ,\ni.e., knowledge acquired during pretraining, that\nis stored in model weights. This is particularly", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28d0d73f-4ca9-4068-95ca-811fcf5e954f": {"__data__": {"id_": "28d0d73f-4ca9-4068-95ca-811fcf5e954f", "embedding": null, "metadata": {"page_label": "2", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7777fc36-c020-4733-a508-10f829ab52cb", "node_type": "4", "metadata": {"page_label": "2", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "a0d509a568bc0f13ce747c04ab6ae07a75bc88dcce3580c61e54315580f5857d", "class_name": "RelatedNodeInfo"}}, "text": "crucial for tasks involving reasoning and for spe-\ncialized applications. Concretely, we examine\nthe impact of different compression schemes on\nparametric knowledge across multiple model fam-\nilies ( ENCODER ,ENCODER -DECODER and DE-\nCODER ) where we apply pruning and quantization\napproaches and analyze the performance of such\ntechniques on downstream reasoning tasks. To the\nbest of our knowledge, this work represents one of\nthe \ufb01rst large-scale investigations in this direction.\nAmong the crucial observations resulting from this\nstudy include:\n\u2022Pruning all modules together has the most\nsigni\ufb01cant impact on parametric knowledge,\ncompared to pruning speci\ufb01c modules,\n\u2022At pruning levels of >50%, the parametric\nknowledge of all the models declines rapidly,\n\u2022Quantizing attention modules has less impact\non performance compared to quantizing feed-\nforward networks for all the models,\n\u2022Across all models, structured pruning at the\n\ufb01nal layer has detrimental effects compared\nto unstructured pruning.\n2 Background\nIn this section, we brie\ufb02y discuss the various com-\npression techniques we use in our study.\n2.1 Pruning\nPruning involves reducing the model size by\neliminating unnecessary or redundant connections\nbetween neurons or entire neurons altogether.\nBroadly speaking, pruning approaches can be clas-\nsi\ufb01ed into two types (Fig. 1):\nUnstructured Pruning: Each connection is\ntreated as an individual entity, and sparsity is\nattained by eliminating connections with lower\nsaliency. Although this approach enables the re-\nmoval of less important connections without com-\npromising performance, it leads to sparse matrix\noperations, which may not be optimal for certain\nhardware accelerators2(Buluc and Gilbert ,2008 ;\nGale et al. ,2019 ).\nStructured Pruning: This involves removing\na group of connections, such as channels or entire\nneurons, instead of individual connections. Unlike\nunstructured pruning, this approach avoids intro-\nducing sparse matrix operations. However, aggres-\n2The current landscape is evolving as advanced accelera-\ntors are emerging that provide support for sparse multiplica-\ntions.\nFigure 1: An illustration of unstructured (left) vs struc-\ntured (right) pruning.\nsive structured pruning may disproportionately im-\npact the model\u2019s performance ( Yao et al. ,2019 ).\nChoosing Saliency of Weights: When choos-\ning the criterion to determine saliency, various fac-\ntors can be taken into account, such as weight mag-\nnitude, importance to the overall network function-\nality, or contribution to speci\ufb01c tasks. Typically,\nthe saliency of weights is determined based on\ntheir magnitudes when selecting which ones to re-\nmove during pruning. A sparsity of k% means that\nthe least salient k% connections are removed.\nThe most commonly used pruning types are:\n1.L1-Unstructured: Connections between\nneurons are eliminated individually, and their\nsaliency is determined by their L1-norm, i.e.,\nthe smallest weights are removed.\n2.Lp-Structured: Connections are elimi-\nnated in a structured way, i.e., an entire\nlayer/channel is removed, and saliency is de-\ntermined by their Lp-norm where pis a hy-\nperparameter.\n2.2 Quantization\nModel parameters can be categorized into weights\nand activations, which are typically represented us-\ning 32 bits. Quantization aims to reduce the num-\nber of bits used for representing these parameters.\nA popular choice for this mapping is3:\nQ(r) =Int(r/S)\u2212Z,\nwhere Qis the quantization operator, ris a real-\nvalued input (weight or activation), Sis a real-\nvalued scaling factor, and Zis an integer zero-\npoint. An important factor in mapping rto an in-\nteger is the scaling factor S. This is usually given\nby\nS=\u03b2\u2212\u03b1\n2b\u22121. (1)\n3Uniform quantization maps real values to equally spaced\nintegers", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e409526-8d9f-4ae8-b9dd-54ab38d8956f": {"__data__": {"id_": "6e409526-8d9f-4ae8-b9dd-54ab38d8956f", "embedding": null, "metadata": {"page_label": "3", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f86c2140-1ce0-4d9f-9a94-1b34d315622a", "node_type": "4", "metadata": {"page_label": "3", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "750ec69d72889778f3906fa788097bf8fdd16b309f8e83ec29685d8d5e2d6621", "class_name": "RelatedNodeInfo"}}, "text": "Here [ \u03b1,\u03b2] denotes the clipping range and bis\nthe quantization bandwidth. The process of de-\ntermining the clipping range is known as calibra-\ntion. Extensive research has been conducted to de-\ntermine the optimal range to reduce the bit repre-\nsentation while balancing accuracy, computational\nef\ufb01ciency, and inference speed ( Gholami et al. ,\n2021 ). In most cases, statistics for weights are\nprecomputed as they remain constant during in-\nference. Often, it may be necessary to \ufb01ne-tune\nthe quantized model parameters to enhance perfor-\nmance on task-speci\ufb01c datasets. Taking these fac-\ntors into account, various methods have been pro-\nposed ( Nagel et al. ,2021 ):\nPost Training Static Quantization (PTSQ):\nThe clipping range for activations is pre-calculated\nusing a representative dataset, which is a small\nsubset derived from the task-speci\ufb01c dataset. Us-\ning this clipping range, the activations are quan-\ntized in advance and thus remain static during in-\nference.\nPost Training Dynamic Quantization\n(PTDQ): The clipping range is dynamically\ncalculated for each activation during inference.\nAlthough this introduces additional computational\noverhead during inference, it yields improved\nperformance compared to Post Training Static\nQuantization (PTSQ) as the signal range is exactly\ncalculated for each input.\nQuantization Aware Training (QAT): The\nmodel undergoes a process known as fake-\nquantization, i.e., during training all the calcula-\ntions involving forward and backward passes are\nperformed in full-precision. Subsequently, after\nupdating the weight parameters through gradient\ndescent, the weights are quantized to a lower bit.\nWhile this approach achieves the highest perfor-\nmance, it requires \ufb01netuning the model.\nWe note that while a huge diversity of often so-\nphisticated and specialized compression methods\nhave been proposed, we focus on a subset of stan-\ndard approaches. This enables us to seek more\ngeneral insights on compression tradeoffs.\n3 Experimental Setup\nIn this section, we present a comprehensive\noverview of our experimental setup, including the\nrationale behind our design choices, along with the\nselection of models and datasets.\nFigure 2: Block diagram of a simpli\ufb01ed Transformer\ndescribing modules compressed in our experiments.\n3.1 Settings Under Consideration\nThe general transformer block consists of an atten-\ntion module followed by a feed-forward network.\nAs a result, we consider three choices for compres-\nsion: compress the attention module alone \u00a7 3.2,\ncompress the feed-forward network alone \u00a7 3.3, or\ncompress both together \u00a7 3.4. Figure 2contains a\nvisual representation of these modules.\nOur chosen compression techniques include\npruning, quantization, and a combination of prun-\ning and quantization. Following the methodology\nproposed in Han et al. (2015 ), we adhere to the\nsequential order of pruning the selected group of\nmodules \ufb01rst and then applying quantization. In\naddition, we also investigate the impact on dis-\ntilled models and explore the effects of employing\nvarious combined compression techniques.\n3.2 Attention-only Global Compression\nWe include all the linear layers within all the\nattention modules of the model. For encoder-\ndecoder models, we also consider the cross-\nattention blocks.\nAttention-only Global Pruning, ( AttGP):\nWe apply pruning to all the linear layers within the\nattention modules.\nAttention-only Global Quantization,\n(AttGQ):We quantize all the linear layers\nwithin the attention modules.\nAttention-only Global Pruning + Quantiza-\ntion, ( AttGPQ):We prune the linear layers in", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d6f4c5e-f753-4c6c-90e5-8a5d61beca95": {"__data__": {"id_": "1d6f4c5e-f753-4c6c-90e5-8a5d61beca95", "embedding": null, "metadata": {"page_label": "4", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f6f45d7-cf06-4331-b46d-a91e0286cecd", "node_type": "4", "metadata": {"page_label": "4", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "36c7d296a0ede58c8c957b5b6c36c914f9d15ea5ccdea5c8bf4853702f9d9be6", "class_name": "RelatedNodeInfo"}}, "text": "the attention modules and subsequently quantize\nthem.\n3.3 Feed-forward-only Global Compression\nWe include all the linear layers within all the feed-\nforward networks of the model.\nFeed-forward-only Global Pruning, ( FFGP):\nWe employ pruning to all the linear layers within\nthe feed-forward networks.\nFeed-forward-only Global Quantization,\n(FFGQ):We quantize all the linear layers within\nthe feed-forward networks.\nFeed-forward-only Global Pruning + Quan-\ntization, ( FFGPQ):We prune all the linear lay-\ners from feed-forward networks and subsequently\nquantize them.\n3.4 Overall Global Compression\nWe speci\ufb01cally target the linear layers within the\nattention and feed-forward network. Under this\ncompression, the different setups are:\nOverall Global Pruning, ( Overall GP):We\nemploy pruning to all the linear layers (except the\n\ufb01nal dense layer).\nOverall Global Quantization,( Overall GQ)\n+:We apply quantization to all the linear lay-\ners (including the \ufb01nal dense layer).\nOverall Global Pruning + Quantization\n(Overall GPQ)+:We \ufb01rst apply pruning to\nall the linear layers (except the \ufb01nal dense layer),\nand subsequently, we quantize all the linear layers.\n3.5 Final Dense Layer Pruning, ( FLP)\nRecent studies ( Mitchell et al. ,2021 ,2022 ;Meng\net al. ,2022 ) provide evidence suggesting that the\n\ufb01nal layers of a language model play a signi\ufb01-\ncant role in storing information. Given its im-\nportance, we focus on understanding how knowl-\nedge is encoded in the \ufb01nal layer. Therefore, we\ntreat the \ufb01nal layer as an individual module in\nour experimental setup and prune it. We consider\nL1-structured and L1-unstructured pruning as out-\nlined in \u00a7 2.1.\nWe note that the number of parameters com-\npressed differs for different settings. We record\nall of the values required for normalizing mea-\nsurements. However, our focus is predominantly\naimed at understanding the effects of compressing\nmodules and their combinations rather than pre-\nsenting normalized results, and our insights re\ufb02ect\nthis framing. We provide full parameter countsthat permit normalized quantities that can be used\nby practitioners who seek to directly apply our\nwork and refer the readers to Sec. A.2for more\ndetails.\n3.6 Design Choices\n\u2022In our global pruning experiments\n(Overall GP,AttGP,FFGP), we use\nL1-Unstructured and apply pruning per-\ncentages ranging from 10% to 90% with\nincrements of 10%.\n\u2022For quantization experiments, as we seek\nto investigate the zero-shot capabilities of\nLLMs, we select post-training dynamic quan-\ntization \u00a7 2.2, eliminating the need for \ufb01netun-\ning (unlike quantization-aware training; QAT\n\u00a72.2) or calibration of the model to a repre-\nsentative dataset (unlike post-training static\nquantization; PTSQ \u00a7 2.2) and quantize to 8\nbits (int8).\n\u2022Since the quantization of activations occurs\nduring inference, which is dynamic in nature,\nthe order of inputs within a batch has a minor\nimpact on the \ufb01nal accuracy ( <1%). There-\nfore, we seed the experiments to ensure con-\nsistent and reproducible results (\u00a7 A.1).\n\u2022Previous studies ( Gordon et al. ,2020 ;Michel\net al. ,2019 ) suggest that pruning levels of\n30%-40% do not affect the model on down-\nstream tasks. Such rules-of-thumb may or\nmay not hold for parametric knowledge. In\nour experimental settings ( GPQ ,FLP), we\nselect 20% and 40% as the levels to under-\nstand when a similar result holds.\n3.7 Model Zoo\nWe consider the following models for our study.\nWhere available, we choose both the base and\nlarge versions of the model to understand if larger\nmodels exhibit different behavior.\n3.7.1 Encoder-only:\n\u2022BERT (Devlin et al. ,2019 ): Pretrained on\nmasked language modeling (MLM) and next\nsentence prediction (NSP) objective.\n\u2022RoBERTa (Liu et al. ,2019 ): Similar to\nBERT with different training choices (larger\ntraining dataset and removed NSP).\n\u2022DistilBERT (Sanh et al. ,2020 ): Distilled ver-\nsion of BERT whose training objective in-\ncludes MLM, a distillation loss, and a cosine\nembedding loss.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3963, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da85bd08-e678-4b8f-9303-fa07ed507789": {"__data__": {"id_": "da85bd08-e678-4b8f-9303-fa07ed507789", "embedding": null, "metadata": {"page_label": "5", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d68a8964-64ff-427c-88ff-94cc0279be9b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "272214b7dec15b06e4128e4aa7b36d90afbf3cbba6ce1228747dc7d7a213cc2f", "class_name": "RelatedNodeInfo"}}, "text": "\u2022ALBERT (Lan et al. ,2019 ): Parameter-\nreduced version of BERT using cross-layer\nparameter sharing and factorized embedding\nparameterization.\n3.7.2 Encoder-Decoder:\n\u2022Flan-T5 (Chung et al. ,2022 ): Instruction-\n\ufb01netuned encoder-decoder model with\nmasked span corruption objective.\n\u2022Lamini-Flan-T5 (Wu et al. ,2023 ): Flan-\nT5 model \ufb01netuned on LaMini instruction\ndataset4which is generated and distilled us-\ning ChatGPT output.\n3.7.3 Decoder only:\n\u2022Vicuna-7B (Chiang et al. ,2023 ): An\ninstruction-based LLama derived model \ufb01ne-\ntuned on user-shared conversations collected\nfrom ShareGPT.\n\u2022WizardLM-7B (Xu et al. ,2023 ): An\ninstruction-based LLama derived model with\ninstructions generated by LLMs (rather than\nhumans) using the Evol-Instruct mechanism.\n3.8 Datasets\nWe use the following datasets for our empirical\nanalysis:\nLAMA: To examine the effects of compression\non encoder-only models, we use the LAMA (LAn-\nguage Model Analysis) benchmark ( Petroni et al. ,\n2019 ). LAMA assesses the factual and common-\nsense knowledge of language models. Each ex-\nample in LAMA is formulated as a cloze-style\nquestion, where either the subject or object is\nmasked. By predicting the masked word, we can\nevaluate the model\u2019s ability to recover real-world\nfacts. Speci\ufb01cally, we probe the encoder-only\nmodels with LAMA to investigate the impact of\ncompression on various knowledge tasks. This\nbenchmark consists of four datasets, namely TRex,\nGoogle-RE, ConceptNet, and SQUAD, each de-\nsigned to assess speci\ufb01c types of relational knowl-\nedge. These datasets provide valuable insights into\nthe model\u2019s performance and its understanding of\ndifferent types of information.\nLanguage model evaluation harness: To\nexamine the effects of compression on encoder-\ndecoder and decoder-only models, we use a sub-\nset of evaluation harness tasks ( Gao et al. ,2021 ):\nthe BoolQ dataset ( Clark et al. ,2019 ), the PIQA\n4https://huggingface.co/datasets/MBZUAI/LaMini-\ninstructiondataset ( Bisk et al. ,2020 ), and the Winogrande\ndataset ( Sakaguchi et al. ,2021 ). These datasets\nprovide a range of challenging prompts for each\nmodel type. We refer the reader to Table 2for ex-\namples of samples from each dataset.\n4 Experimental Results and Insights\nTo facilitate our discussion, we categorize pruning\nlevels as follows:\n\u2022plow: Sparsity levels of 10-30%\n\u2022pmedium : Sparsity levels of 30-50%\n\u2022phigh: Sparsity levels of >50%\nFor encoder-only models, we report the % drop\nin top-1 accuracy, averaged across all the probes\nin LAMA. For the decoder-only and encoder-\ndecoder models, we report the % drop in accuracy,\naveraged across BoolQ, PIQA and Winogrande. In\nthe decoder-only and encoder-decoder plots, the\nmajority-baseline indicates the accuracy when all\nthe predictions are assigned to the majority class.\n4.1 Global Pruning\nWe observe that for encoder-only models (Fig. 3,\n19), there is a minimal decline in performance\natplow. At pmedium , the drop in performance is\nmore signi\ufb01cant for pruning feed-forward net-\nworks ( FFGP) as compared to attention modules\n(AttGP).\nFinding: Atpmedium , for encoder-\nonly models, pruning attention modules\n(AttGP) has a smaller impact compared to\npruning feed-forward networks ( FFGP).\nWe observe that for encoder-decoder (Fig. 5,\n13) and decoder-only models (Fig. 4), there is a\nminimal decline in performance at plow. However,\natpmedium , the drop in performance is more\nsigni\ufb01cant for pruning attention modules ( AttGP)\ncompared to feed-forward networks ( FFGP).\nFinding: Atpmedium , for encoder-\ndecoder and decoder-only models, pruning\nthe attention module ( AttGP) has more im-\npact on performance compared to pruning\nfeed-forward networks ( FFGP).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3676, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56123766-ad09-4aee-9ac8-39c206d5c833": {"__data__": {"id_": "56123766-ad09-4aee-9ac8-39c206d5c833", "embedding": null, "metadata": {"page_label": "6", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3caa7c3f-3a64-4fba-9945-6376d0cc5e46", "node_type": "4", "metadata": {"page_label": "6", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "0baa68aacb6931a34a372a8220ff4bc565ab6df5397c01b7f0f6c2eef2d35eea", "class_name": "RelatedNodeInfo"}}, "text": "0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200BER T -base\n0 20 40 60 80 100\n% prunedBER T -large\n0 20 40 60 80 100\n% prunedDistilBER T -base% loss\nOverall\nGPAtt\nGPFF\nGPFigure 3: Averaged drop in Top-1 accuracy for encoder-only models for global pruning.\n0 20 40 60 80 100\n% pruned\u221240\u221230\u221220\u2212100V icuna -7B\n0 20 40 60 80 100\n% prunedW izardLM-7B% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\nFigure 4: Averaged drop in accuracy for decoder-only\nmodels for global pruning.\n0 20 40 60 80 100\n% pr ned\u221250\u221240\u221230\u221220\u221210010Flan -T5-base\n0 20 40 60 80 100\n% prunedFlan-T5-large% loss\nOverall\nGPAtt\nGPFF\nGPMajority BaselineFigure 5: Averaged drop in accuracy for encoder-decoder\nmodels for global pruning.\nWe note that the number of parameters in the\nfeed-forward networks is signi\ufb01cantly higher than\nthe number of parameters in the attention mod-\nules for all these models (Table 3). This observa-\ntion provides a likely explanation for the pattern\nobserved in encoder-only models, where pruning\nmore parameters results in a higher loss of para-\nmetric knowledge. However, this \ufb01nding is coun-\nterintuitive for encoder-decoder and decoder-only\nmodels , as we would expect that pruning the larger\nfeed-forward networks would have a more signif-\nicant impact on the parametric knowledge. We\nsuspect that the feed-forward networks are over-\nparameterized and thus they can be pruned without\na signi\ufb01cant drop in performance.\nFinding: For all the models, pruning all\nthe modules together ( Overall GP) has the\nmost signi\ufb01cant impact on performance.\nAmong all the models analyzed, pruning all\nmodules together ( Overall GP) has the most sig-\nni\ufb01cant negative impact on performance. This\n\ufb01nding suggests that when compressing models,\npruning all modules simultaneously leads to a\ngreater loss of parametric knowledge compared to\npruning speci\ufb01c modules or components individu-\nally. Therefore, it is crucial to carefully considerthe implications of employing global pruning tech-\nniques. We additionally note that at phigh, the per-\nformance goes to zero as expected.\nAdditional results for global pruning on individ-\nual datasets for encoder-only models are shown in\nFig20,21,22; for decoder-only models at Fig 23;\nfor encoder-decoder models at Fig 13,25.\n4.2 Global Quantization\nWe observe that across all the models (Fig. 6,\n15,16), the performance drop is less signi\ufb01cant\nwhen quantizing attention modules ( AttGQ) com-\npared to quantizing feed-forward networks alone\n(FFGQ). This contrasts with the results from\nglobal pruning (\u00a7 4.1), where pruning attention-\nonly modules had a more detrimental effect on\nencoder-decoder and decoder-only models.\nFinding: For all the models, quantizing\nattention modules ( AttGQ) has lesser im-\npact compared to quantizing feed-forward\nnetworks ( FFGQ).\nWe hypothesize that in the case of quantiza-\ntion where all connections are preserved, the para-\nmetric knowledge in cross-attention modules may\nremain relatively intact . However, in pruning,\nas connections are eliminated, there may have", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f06e4faa-5c9d-4dc6-9929-e6bb84f5a950": {"__data__": {"id_": "f06e4faa-5c9d-4dc6-9929-e6bb84f5a950", "embedding": null, "metadata": {"page_label": "7", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3562449d-3892-47a6-85ec-331b7ef9e6fb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "765db636071d5255edcb8395284845f087585ece1b1eaa1a05bec8e75a94db03", "class_name": "RelatedNodeInfo"}}, "text": "BER T -baseBER T -large\nDistilBER T -base\u2212100\u221280\u221260\u221240\u2212200% drop in acc\nRoBER T a-base\nRoBER T a-largeALBER T -baseALBER T -large\nOverall\nGQFF\nGQAtt\nGQFigure 6: Averaged drop in Top-1 accuracy for encoder-\nonly models for global quantization.\nBER T -baseBER T -large\nDistilBER T -base\u2212100\u221280\u221260\u221240\u2212200% drop in acc\nRoBER T a-base\nRoBER T a-largeALBER T -baseALBER T -large\nOverall\nGPQFF\nGPQAtt\nGPQ20% 40%\nFigure 7: Averaged drop in Top-1 accuracy for encoder-\nonly models for global pruning+quantization.\na greater impact on the parametric knowledge\nin cross-attention modules, thereby affecting the\noverall capabilities of the model. It is also interest-\ning to observe that the performance drop during\nquantization is almost similar to that of pmedium .\nFinding: For all the models, quantizing all\nthe modules together ( Overall GQ) hurts\nthe most.\nIt is intuitive that quantizing all the modules to-\ngether ( Overall GQ) has the most signi\ufb01cant nega-\ntive impact. Additional results are shown in Tables.\n4,5,6.\n4.3 Global Pruning + Quantization\nFor all the models (Fig. 7,17,18), at 20%\nsparsity, compressing attention modules ( AttGPQ)\nresults in a smaller performance drop compared\nto compressing feed-forward networks ( FFGPQ).\nAt 40% sparsity, the same trend is observed for\nencoder-only and decoder-only models. However,\nwe notice the reverse for ENCODER -DECODER\nmodels i.e., that compressing feed-forward net-\nworks affects performance lessthan compressing\nthe attention modules at 40% sparsity.\nFinding: For all the models, at 20% spar-\nsity level, AttGPQ hurts less compared to\nFFGPQ.\nWe hypothesize that the sequential effects of\nFlanT5-base\u221220\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajorit  BaselineFlanT5 -base\nFlanT5-large\u221225\u221220\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajorit  BaselineFlanT5 -largeFigure 8: Averaged drop in accuracy for encoder-\ndecoder models for different attention modules\ncompression. Self AttGPQ: Compressing only\nself-attention modules, Cross AttGPQ: Compress-\ning only cross-attention modules, Encoder AttGPQ:\nCompressing attention modules in encoder only,\nDecoder AttGPQ: Compressing attention modules in\ndecoder only.\npruning and quantization on the cross-attention\nmodules could be responsible for this change in\nthe order of impact. To test this hypothesis, we se-\nlectively prune and quantize the self-attention and\ncross-attention modules separately and found out\nthat it is indeed the case (Fig. 8) and aligns with\nthe claim made in Michel et al. (2019 ). Additional\nresults for compressing attention-only modules are\nshown in Fig 12,24. For \ufb01ne-grained analysis on\nindividual datasets, we refer the interested reader\nto Table 4,5,6.\n4.4 Final Dense layer Pruning\nFor encoder-only models (Fig. 9), L1-\nunstructured pruning has a smaller impact\ncompared to L1-structured pruning. We hypoth-\nesize that the \ufb01nal layer of the encoder-only\nmodels might encode knowledge in a structured\nor modular manner, and any form of structured\ncompression would disrupt this encoding , result-\ning in a larger performance drop. Such a result\nwould be consistent with existing approaches that\nenable editing knowledge in language models and\nrely on structure ( Mitchell et al. ,2021 ).\nFinding: For encoder-only models, L1-\nunstructured leads to a smaller decrease in\nperformance than L1-structured.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd302b04-d174-45e1-8d61-1e9898867131": {"__data__": {"id_": "dd302b04-d174-45e1-8d61-1e9898867131", "embedding": null, "metadata": {"page_label": "8", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c22366ef-212a-4fad-b03c-ead4c4bb36e4", "node_type": "4", "metadata": {"page_label": "8", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "c96f2a6fcca8ecb44b9b70ef89c44adceeff403fb2187c5b4fee9cebab6891aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ea4623c-f737-4078-8668-43c626cf33b1", "node_type": "1", "metadata": {}, "hash": "b107a1d196bf3784f2a0890555a94f70b906f42538112134bff30017acee4072", "class_name": "RelatedNodeInfo"}}, "text": "BER T -base BER T -large DistilBER T -base-70.0-60.0-50.0-40.0-30.0-20.0-10.00.0%  loss\nRoBER T a-base RoBER T a-largeALBER T -base ALBER T -large\nLn-Structured L1-Unstructured 20% Pruning 40% PruningFigure 9: Averaged drop in Top-1 accuracy for encoder-\nonly models for \ufb01nal layer pruning.\nV icuna-35.0-30.0-25.0-20.0-15.0-10.0-5.00.0%  loss\nWizardLM 7B\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning Majority Baseline\nFigure 10: Averaged drop in Top-1 accuracy for\ndecoder-only models for \ufb01nal layer pruning.\nFor decoder-only (Fig. 10) and encoder-decoder\n(Fig. 14) models, even at a sparsity level of 20%,\nthe predicted accuracy is very close to the major-\nity baseline. This \ufb01nding aligns with the claims\nmade in Mitchell et al. (2022 ) that \ufb01nal layers en-\ncode signi\ufb01cant amount of information. The dras-\ntic performance drop observed suggests that the \ufb01-\nnal layers play a crucial role in encoding knowl-\nedge. Additional results for pruning the \ufb01nal layer\nare shown in Fig. 26,27,28.\n5 Related Work\nEarly works seeking to understand large language\nmodel behavior focused on contextual represen-\ntations and how such models gain linguistic ca-\npabilities ( Goldberg ,2019 ;Ettinger et al. ,2018 ;\nJawahar et al. ,2019 ). More recently, some lines\nof work have steered towards understanding how\nthese models acquire factual and commonsense\nknowledge. Techniques such as probing evolved\nas a way to understand the knowledge capabili-\nties of these models ( Petroni et al. ,2019 ;Kassner\nand Sch\u00fctze ,2020 ;Talmor et al. ,2020 ;Weir et al. ,\n2020 ;Wallat et al. ,2021 ).\nPrevious works including Gordon et al. (2020 );\nMichel et al. (2019 ) pruned BERT and showed that\nit is resilient to a medium level of pruning. For ex-\nample, Michel et al. (2019 ) showed that after \ufb01ne-\ntuning for a particular downstream task, it is pos-\nsible to prune about 40% of the attention weights\nwithout any loss in performance. A particular fo-cus has been to understand the importance of the\nattention mechanism ( V oita et al. ,2019 ;Michel\net al. ,2019 ) by pruning the heads. In a similar\nfashion, works such as Zafrir et al. (2019 );Bai\net al. (2020 );Zadeh et al. (2020 );Tao et al. (2022 );\nPrato et al. (2019 );Frantar et al. (2022 );Dettmers\net al. (2023 ) pushed the limits of quantization on\nlanguage models. Most of these works have fo-\ncused on one model class or a particular metric.\nIn another line of work, a variety of approaches\n(Li and Liang ,2021 ;Hu et al. ,2021 ;Liu et al. ,\n2021 ;Lester et al. ,2021 ) focus on alternatives to\ntraditional \ufb01netuning of the model due to its scale.\nIn contrast to these works, our paper primarily fo-\ncuses on the in-built parametric knowledge present\nin the model. This means we do not \ufb01netune and\ninstead seek to understand whether some of the\npreviously described phenomenona are applicable\nto other models as well.\nAlso connected to this work are techniques that\nedit factual knowledge in models. The goal for\nsuch works is to avoid retraining or even \ufb01netun-\ning models, instead seeking to directly change pa-\nrameters connected to certain facts ( Mitchell et al. ,\n2021 ,2022 ;Meng et al. ,2022 ). However, given\nour focus on compression, the main theme of our\nwork differs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ea4623c-f737-4078-8668-43c626cf33b1": {"__data__": {"id_": "7ea4623c-f737-4078-8668-43c626cf33b1", "embedding": null, "metadata": {"page_label": "8", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c22366ef-212a-4fad-b03c-ead4c4bb36e4", "node_type": "4", "metadata": {"page_label": "8", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "c96f2a6fcca8ecb44b9b70ef89c44adceeff403fb2187c5b4fee9cebab6891aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd302b04-d174-45e1-8d61-1e9898867131", "node_type": "1", "metadata": {"page_label": "8", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "d5eb1b05dfa52d7c07fd85fd4000622afbd5afb6e8b834eb045a0bf7bdf8689d", "class_name": "RelatedNodeInfo"}}, "text": ",2021 ;Liu et al. ,\n2021 ;Lester et al. ,2021 ) focus on alternatives to\ntraditional \ufb01netuning of the model due to its scale.\nIn contrast to these works, our paper primarily fo-\ncuses on the in-built parametric knowledge present\nin the model. This means we do not \ufb01netune and\ninstead seek to understand whether some of the\npreviously described phenomenona are applicable\nto other models as well.\nAlso connected to this work are techniques that\nedit factual knowledge in models. The goal for\nsuch works is to avoid retraining or even \ufb01netun-\ning models, instead seeking to directly change pa-\nrameters connected to certain facts ( Mitchell et al. ,\n2021 ,2022 ;Meng et al. ,2022 ). However, given\nour focus on compression, the main theme of our\nwork differs. Nevertheless, it would be interesting\nto understand the impact of relying on compressed\nmodels when using such editing techniques.\n6 Conclusion\nCompression is crucial in deploying and using\nlarge language models. Despite its importance,\nexisting empirical studies predominantly rely on\ngeneric measurements such as perplexity or stan-\ndardized benchmark metrics when investigating\nthe effects of compression. These coarse measure-\nments are challenging to interpret. As a result, it is\ndif\ufb01cult to use them to develop meaningful heuris-\ntics for practitioners.\nTo address these limitations, we provided a\nlarge-scale study that focused on \ufb01ne-grained ef-\nfects on quantities like parametric knowledge. We\nstudied a variety of compression choices across\nmultiple model families, providing usable insights\ninto what types of compression schemes have the\nleast and most signi\ufb01cant impact on models. We\nhope this work serves as a useful step towards de-\nveloping users\u2019 intuition for rules-of-thumb when\nselecting appropriate compression techniques in\nlarge language models. For future work, we hope", "mimetype": "text/plain", "start_char_idx": 2471, "end_char_idx": 4324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a55e2f7a-e341-4d29-95df-c82ca1a35285": {"__data__": {"id_": "a55e2f7a-e341-4d29-95df-c82ca1a35285", "embedding": null, "metadata": {"page_label": "9", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcbe8023-a431-4f98-8588-367448f77f40", "node_type": "4", "metadata": {"page_label": "9", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "ddff0228adcca1a58f7e3f3536db3464b9279b52ca3f750b6bad9331789c0804", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e51f29a3-e2d3-40e3-87f8-8461870f437e", "node_type": "1", "metadata": {}, "hash": "e3dadf82ae3e73601c912e0932dfe78b4046da53d7b6ccd2449ee5c7d305bf43", "class_name": "RelatedNodeInfo"}}, "text": "to add additional, more specialized techniques for\nlarge language model compression.\n7 Limitations\nOur research has tackled a diverse combination of\nmodels, compression schemes, and compression\ntargets within the vast large language model re-\nsearch area. We note that sophisticated and spe-\ncialized compression techniques tailored to spe-\nci\ufb01c objectives for a particular class of models\nmay exhibit distinct behavior compared to the \ufb01nd-\nings presented in this study. Hence, our work does\nnot aim to present an exhaustive set of \ufb01ndings that\nuniversally characterize the impact on parametric\nknowledge across all conceivable models and com-\npression approaches. We believe that our study\nserves as a valuable starting point, offering a nu-\nanced examination of prevalent methodologies.\nWe note, additionally, that we do not directly\naddress the tradeoff between wall-clock inference\ntime versus compression. While this is also an\nimportant tradeoff, the impact of compression on\ninference time contains many intricacies that are\nbest treated with a separate large-scale study.\nReferences\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2020. Binarybert: Pushing the limit of bert quanti-\nzation. arXiv preprint arXiv:2012.15701 .\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin\nChoi, et al. 2020. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings\nof the AAAI conference on arti\ufb01cial intelligence , vol-\nume 34, pages 7432\u20137439.\nAydin Buluc and John R Gilbert. 2008. Challenges and\nadvances in parallel sparse matrix-matrix multiplica-\ntion. In 2008 37th International Conference on Par-\nallel Processing , pages 503\u2013510. IEEE.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality .\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-\ufb01netuned language mod-\nels.arXiv preprint arXiv:2210.11416 .\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and KristinaToutanova. 2019. Boolq: Exploring the surprising\ndif\ufb01culty of natural yes/no questions. arXiv preprint\narXiv:1905.10044 .\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339 .\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian,\nDenis Kuznedelev, Elias Frantar, Saleh Ashkboos,\nAlexander Borzunov, Torsten Hoe\ufb02er, and Dan Al-\nistarh. 2023. Spqr: A sparse-quantized representa-\ntion for near-lossless llm weight compression. arXiv\npreprint arXiv:2306.03078 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nMengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad\nShokouhi, Xia Hu, and Ahmed Hassan Awadallah.\n2021. What do compressed large language models\nforget? robustness challenges in model compression.\narXiv e-prints , pages arXiv\u20132110.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e51f29a3-e2d3-40e3-87f8-8461870f437e": {"__data__": {"id_": "e51f29a3-e2d3-40e3-87f8-8461870f437e", "embedding": null, "metadata": {"page_label": "9", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcbe8023-a431-4f98-8588-367448f77f40", "node_type": "4", "metadata": {"page_label": "9", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "ddff0228adcca1a58f7e3f3536db3464b9279b52ca3f750b6bad9331789c0804", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a55e2f7a-e341-4d29-95df-c82ca1a35285", "node_type": "1", "metadata": {"page_label": "9", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "5facf0b714f8ecf20b5fba19362e742d5c5ecd42d1093d968f68a748749cb216", "class_name": "RelatedNodeInfo"}}, "text": "2023. Spqr: A sparse-quantized representa-\ntion for near-lossless llm weight compression. arXiv\npreprint arXiv:2306.03078 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nMengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad\nShokouhi, Xia Hu, and Ahmed Hassan Awadallah.\n2021. What do compressed large language models\nforget? robustness challenges in model compression.\narXiv e-prints , pages arXiv\u20132110.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations . In Proceedings of\nthe 27th International Conference on Computational\nLinguistics , pages 1790\u20131801, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nElias Frantar and Dan Alistarh. 2023. Massive lan-\nguage models can be accurately pruned in one-shot.\narXiv preprint arXiv:2301.00774 .\nElias Frantar, Saleh Ashkboos, Torsten Hoe\ufb02er, and\nDan Alistarh. 2022. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\narXiv preprint arXiv:2210.17323 .\nTrevor Gale, Erich Elsen, and Sara Hooker. 2019. The\nstate of sparsity in deep neural networks. arXiv\npreprint arXiv:1902.09574 .\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPo\ufb01, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish\nThite, Ben Wang, Kevin Wang, and Andy Zou. 2021.\nA framework for few-shot language model evalua-\ntion.\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao,\nMichael W. Mahoney, and Kurt Keutzer. 2021. A\nsurvey of quantization methods for ef\ufb01cient neural\nnetwork inference .\nYoav Goldberg. 2019. Assessing bert\u2019s syntactic abili-\nties. arXiv preprint arXiv:1901.05287 .", "mimetype": "text/plain", "start_char_idx": 2745, "end_char_idx": 4576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15554537-b5cf-49e3-9833-81924da2ffb1": {"__data__": {"id_": "15554537-b5cf-49e3-9833-81924da2ffb1", "embedding": null, "metadata": {"page_label": "10", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25b0cbe2-a016-42b0-ad32-bd4e2d16b4c5", "node_type": "4", "metadata": {"page_label": "10", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7884836411f2cf966237c04b2f997eeefe851f1baba66c06b06eafa1e3765fc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f754099e-f3ec-4316-947a-52725f2724f7", "node_type": "1", "metadata": {}, "hash": "c2880134946e9a6e5af3acf3b2ce52554b84d6f88b8afc2744be718c3f3e7048", "class_name": "RelatedNodeInfo"}}, "text": "Mitchell A Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing bert: Studying the effects of\nweight pruning on transfer learning. arXiv preprint\narXiv:2002.08307 .\nArnav Gudibande, Eric Wallace, Charlie Snell,\nXinyang Geng, Hao Liu, Pieter Abbeel, Sergey\nLevine, and Dawn Song. 2023. The false promise\nof imitating proprietary llms .\nSong Han, Huizi Mao, and William J Dally. 2015.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. arXiv preprint arXiv:1510.00149 .\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685 .\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah.\n2019. What does BERT learn about the structure\nof language? InProceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics , pages 3651\u20133657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nNora Kassner and Hinrich Sch\u00fctze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot \ufb02y . InProceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics , pages 7811\u20137818, Online. As-\nsociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942 .\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-ef\ufb01cient prompt\ntuning. arXiv preprint arXiv:2104.08691 .\nXiang Lisa Li and Percy Liang. 2021. Pre\ufb01x-tuning:\nOptimizing continuous prompts for generation . In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582\u20134597, Online. Association for Computational\nLinguistics.\nChen Liang, Simiao Zuo, Minshuo Chen, Haoming\nJiang, Xiaodong Liu, Pengcheng He, Tuo Zhao,\nand Weizhu Chen. 2021. Super tickets in pre-\ntrained language models: From model compres-\nsion to improving generalization. arXiv preprint\narXiv:2105.12002 .\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385 .Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach .\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262 .\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems , 32.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. arXiv preprint arXiv:2110.11309 .\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f754099e-f3ec-4316-947a-52725f2724f7": {"__data__": {"id_": "f754099e-f3ec-4316-947a-52725f2724f7", "embedding": null, "metadata": {"page_label": "10", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25b0cbe2-a016-42b0-ad32-bd4e2d16b4c5", "node_type": "4", "metadata": {"page_label": "10", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7884836411f2cf966237c04b2f997eeefe851f1baba66c06b06eafa1e3765fc5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15554537-b5cf-49e3-9833-81924da2ffb1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "0702c6de7f16e2b6f9b3273a0371811c785ff72d6804755279eb57775d460abe", "class_name": "RelatedNodeInfo"}}, "text": "2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach .\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262 .\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems , 32.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. arXiv preprint arXiv:2110.11309 .\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D Manning, and Chelsea Finn. 2022. Memory-\nbased model editing at scale. In International Con-\nference on Machine Learning , pages 15817\u201315831.\nPMLR.\nMarkus Nagel, Marios Fournarakis, Rana Ali Amjad,\nYelysei Bondarenko, Mart van Baalen, and Tijmen\nBlankevoort. 2021. A white paper on neural network\nquantization .\nFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases?\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh. 2019. Fully quantized trans-\nformer for machine translation. arXiv preprint\narXiv:1910.10485 .\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM , 64(9):99\u2013106.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter .\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM , 63(12):54\u201363.\nAbigail See, Minh-Thang Luong, and Christopher D\nManning. 2016. Compression of neural machine\ntranslation models via pruning. arXiv preprint\narXiv:1606.09274 .\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP . In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 3645\u20133650, Florence, Italy.\nAssociation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics \u2013 on what language\nmodel pre-training captures .", "mimetype": "text/plain", "start_char_idx": 2568, "end_char_idx": 4807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0335f57-bd08-4bd3-b7e8-8f429f376fb1": {"__data__": {"id_": "c0335f57-bd08-4bd3-b7e8-8f429f376fb1", "embedding": null, "metadata": {"page_label": "11", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5108d8cd-9c76-4a63-87d1-5ed724affa87", "node_type": "4", "metadata": {"page_label": "11", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "2dfc15023e945f2eb3d77b8bd60067aea386f729398e18efde0d7e696dbdd797", "class_name": "RelatedNodeInfo"}}, "text": "Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang,\nXin Jiang, Qun Liu, Ping Luo, and Ngai Wong.\n2022. Compression of generative pre-trained lan-\nguage models via quantization. arXiv preprint\narXiv:2203.10705 .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and ef\ufb01-\ncient foundation language models. arXiv preprint\narXiv:2302.13971 .\nElena Voita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned . In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics , pages 5797\u20135808, Florence,\nItaly. Association for Computational Linguistics.\nJonas Wallat, Jaspreet Singh, and Avishek Anand.\n2021. Bertnesia: Investigating the capture and for-\ngetting of knowledge in bert .\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461 .\nNathaniel Weir, Adam Poliak, and Benjamin\nVan Durme. 2020. Probing neural language\nmodels for human tacit assumptions. arXiv preprint\narXiv:2004.04877 .\nMinghao Wu, Abdul Waheed, Chiyu Zhang, Muham-\nmad Abdul-Mageed, and Alham Fikri Aji. 2023.\nLamini-lm: A diverse herd of distilled mod-\nels from large-scale instructions. arXiv preprint\narXiv:2304.14402 .\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244 .\nZhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang,\nand Lanshun Nie. 2019. Balanced sparsity for ef-\n\ufb01cient dnn inference on gpu. In Proceedings of\nthe AAAI Conference on Arti\ufb01cial Intelligence , vol-\nume 33, pages 5676\u20135683.\nAli Hadi Zadeh, Isak Edo, Omar Mohamed Awad,\nand Andreas Moshovos. 2020. Gobo: Quantiz-\ning attention-based nlp models for low latency and\nenergy ef\ufb01cient inference. In 2020 53rd Annual\nIEEE/ACM International Symposium on Microarchi-\ntecture (MICRO) , pages 811\u2013824. IEEE.\nO\ufb01r Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Ef\ufb01cient Machine\nLearning and Cognitive Computing-NeurIPS Edi-\ntion (EMC2-NIPS) , pages 36\u201339. IEEE.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acc4cf1c-475b-4622-9027-4b3555b45466": {"__data__": {"id_": "acc4cf1c-475b-4622-9027-4b3555b45466", "embedding": null, "metadata": {"page_label": "12", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a2971fa-17f5-4fc9-a0e2-b5157e05635c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "ba8b7c554b06fdeab67cfd10be18d74907eda91732f48b80eb9120b81b337d02", "class_name": "RelatedNodeInfo"}}, "text": "A Appendix\nThe appendix contains all of the results we could\nnot include in the body of the paper. We \ufb01rst\ndiscuss the statistical approach of the experiments\nand the performance drop against compression ra-\ntio. Then, we show individual plots for a set of ex-\nperiments that track decrease in accuracy for sev-\neral types of compression and models. Next, we\nprovide a table that contains information on the\ndatasets used in our experiments. Afterwards, we\nprovide tables with model details, including pa-\nrameter counts, and explicit results for compres-\nsion results across model families. Afterwards, we\npresent a large-scale comparison across datasets\nfor encoder-decoder models under various atten-\ntion module compression approaches. We pro-\nvide LAMA probe results and \ufb01nally, change-in-\naccuracy plots for a variety of datasets for different\nmodel classes.\nA.1 Experimental Approach\nOur experiments fall into two categories: deter-\nministic and stochastic. Our experiments on prun-\ning are deterministic as we used L1-unstructured\npruning. On the other hand, our quantization ex-\nperiments have an element of randomness. This\nis due to our use of PTDQ, which computes a\ndynamic clipping range. We deliberately struck\na balance between the number of trials per set-\nting and the overall number of settings studied.\nConsequently, we ran experiments with multiple\nseeds and recorded con\ufb01dence intervals, as demon-\nstrated in Table 1.\nA.2 Performance drop against compression\nratio\nNormalizing the x-axis to account for the param-\neter ratio results in the same plots, but with a\nskewed x-axis (Fig. 11). Given a speci\ufb01c perfor-\nmance drop percentage, it is highly likely that we\ncan achieve greater parameter compression by tar-\ngeting feedforward modules rather than attention\nmodules. It is worth noting that across all the\nmodels studied, feedforward modules have more\nparameters than attention module\n1.9 3.9 5.8 7.7 9.6\n11.6 13.5 15.4 17.35.9\n11.7 17.6 23.5 29.4 35.2 41.1 47.0 52.8\nCompression Ra io\u221230\u221225\u221220\u221215\u221210\u221250% drop in accBER T -base on TREx\nAtt\nGP\nFF\nGP\n4.0 8.1\n12.1 16.1 20.1 24.2 28.2 32.2 36.24.1 8.3\n12.4 16.6 20.7 24.9 29.0 33.2 37.3\nCompression  Rati \u22120.5\u22120.4\u22120.3\u22120.2\u22120.10.0% dr p in accFlan - T5-large on BoolQ\nAtt\nGP\nFF\nGP\nMajority Baseline\n3.2 6.5 9.7\n13.0 16.2 19.5 22.7 26.0 29.26.6\n13.1 19.7 26.2 32.8 39.3 45.9 52.4 59.0\nCompression  Ratio\u22120.4\u22120.3\u22120.2\u22120.10.0% d op in accVicuna -7B on BoolQ\nAtt\nGP\nFF\nGP\nMajority BaselineFigure 11: Performance drop vs compression ratios for\ndifferent model families. Models and Datasets are ran-\ndomly chosen for representation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4e278da-f7e5-44dc-bb0c-9dcc92619a9e": {"__data__": {"id_": "b4e278da-f7e5-44dc-bb0c-9dcc92619a9e", "embedding": null, "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "170c0da2-9c4c-4b95-ad57-2de6b8ee262b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "c7945aabab6766245721477e8cef21c62d7c96ccc9bffe73aca4bd47213c6e7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f4e6455-6045-4617-8cd4-094ab3f449dc", "node_type": "1", "metadata": {}, "hash": "910b18d20472886906880bc623673c2da1e5a392f300991158f6d717c1ca35dd", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Top-1 Accuracy from quantizing BERT on SQUAD. Left: BERT-Base, Right: BERT-Large. Baseline for\nBERT-Base is 12.987 and BERT-Large is 15.909\nOverall GQ AttGQ FFGQ\nSeed 40 5.844 13.312 3.896\nSeed 50 5.844 13.312 5.195\nSeed 60 6.169 12.987 4.221\nMean 5.952 13.204 4.437\nStandard Deviation (sample) 0.188 0.188 0.676\nStandard Deviation (population) 0.153 0.153 0.676Overall GQ AttGQ FFGQ\nSeed 40 2.273 16.883 2.597\nSeed 50 2.922 14.286 3.896\nSeed 60 2.922 15.909 4.545\nMean 2.706 15.693 3.679\nStandard Deviation (sample) 0.375 1.312 0.992\nStandard Deviation (population) 0.306 1.071 0.992\nTable 2: Datasets in our experiments (we use dev sets for BoolQ, PIQA, and Winogrande)\nProbe Type #Egs Question Answer\nTRex Factual 34k Francesco Bartolomeo Conti was born in [MASK]. Florence\nGoogle-RE Factual 5.5k Mareva is a [MASK] actress & former beauty Queen French Polynesia\nSquad Factual 305 Newton played a [MASK] during Super Bowl 50. Quarterback\nConceptNet Commonsense 11k Joke would make you want to [MASK]. laugh\nBoolQ Mix 3.2k Is there any dollar bill higher than a 100? No\nPIQA Commonsense 1.8kGoal: \"ice box\"\nSoln1 Soln1: will turn into a cooler if you add water to it\nSoln2: will turn into a cooler if you add soda to it\nWinogrande Commonsense 1.2kThe trophy doesnt \ufb01t into the brown suitcase because itstoo small. suitcase\nThe trophy doesnt \ufb01t into the brown suitcase because itstoo large. trophy\nTable 3: Number of parameters (in million) across all the models\nModel Name Overall GP AttGP,GQ,GPQ FFGP,GQ,GPQ FLP Total trainable parameters (for Overall GQ,GPQ )\nBert-base 86 21 64 23 109\nBert-large 303 75 226 31 334\nRoberta-base 86 21 64 39 124\nRoberta-large 303 75 226 51 355\nDistilbert-base 43 14 28 23 66\nAlbert-base-v2 85 28 57 4 89\nAlbert-large-v2 302 101 201 4 306\nFlanT5-base 198 85 113 25 223\nDistil-FlanT5-base 198 85 113 25 223\nFlanT5-large 717 302 311 33 750\nDistil-FlanT5-large 717 302 311 33 750\nVicuna 6476 2147 4329 131 6607\nWizard-LM 6476 2147 4329 131 6607\nTable 4: Results from compressing different modules for encoder-only models (numbers represent top-1 accuracy)\nModel Dataset BaselineOverall GQ AttGQ FFGQ Overall GPQ AttGPQ FFGPQ\n20% 40% 20% 40% 20% 40%\nBERT-baseTREx 30.27 11.536 29.903 16.614 5.552 4.233 29.675 29.217 14.691 14.628\nGoogleRe 10.29 4.374 10.109 5.662 2.377 1.96 9.873 9.673 4.628 5.753\nSquad 12.987 5.844 13.312 3.896 2.597 0.974 12.338 10.39 4.87 4.87\nConceptnet 16.33 6.02 15.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f4e6455-6045-4617-8cd4-094ab3f449dc": {"__data__": {"id_": "2f4e6455-6045-4617-8cd4-094ab3f449dc", "embedding": null, "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "170c0da2-9c4c-4b95-ad57-2de6b8ee262b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "c7945aabab6766245721477e8cef21c62d7c96ccc9bffe73aca4bd47213c6e7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4e278da-f7e5-44dc-bb0c-9dcc92619a9e", "node_type": "1", "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "b6ebbea2dde3ce47ba5dfdeb9f7868fceaaf1ac431554bcd94690c7e8ad7ca15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7a2e3ea-e3af-4b8c-b0dc-e3212f8df20a", "node_type": "1", "metadata": {}, "hash": "504d3892c2aacf2a61535fd4ffacca4283e38466f00c567394c42eeac680823c", "class_name": "RelatedNodeInfo"}}, "text": "27 11.536 29.903 16.614 5.552 4.233 29.675 29.217 14.691 14.628\nGoogleRe 10.29 4.374 10.109 5.662 2.377 1.96 9.873 9.673 4.628 5.753\nSquad 12.987 5.844 13.312 3.896 2.597 0.974 12.338 10.39 4.87 4.87\nConceptnet 16.33 6.02 15.897 8.677 3.919 3.69 16.224 15.553 8.262 8.324\nBERT-largeTREx 30.485 4.376 30.539 4.592 1.277 0.848 29.624 29.195 8.145 6.673\nGoogleRe 10.472 3.829 10.309 3.612 1.434 0.436 10.018 9.964 4.247 3.376\nSquad 15.909 2.273 16.883 2.597 1.948 0.325 15.584 14.935 3.247 2.597\nConceptnet 19.534 4.652 19.048 5.649 1.818 1.342 18.801 18.086 5.164 4.864\nRoBERTa-baseTREx 11.9 3.566 8.014 8.421 1.424 0.006 11.529 4.663 6.489 0.048\nGoogleRe 4.102 1.143 2.668 1.234 0.617 0 3.249 1.779 1.053 0.091\nSquad 8.442 0.325 4.545 2.922 0 0 5.195 1.623 1.299 0\nConceptnet 17.036 3.769 14.467 7.247 0.83 0.026 14.865 8.147 5.746 0.856\nRoBERTa-largeTREx 16.862 6.264 16.159 11.885 0.175 0.019 15.845 15.714 5.355 0.057\nGoogleRe 3.811 1.488 3.829 2.868 0.036 0 2.196 1.869 0.926 0.054\nSquad 13.636 4.87 13.312 7.468 0 0 12.013 10.714 1.299 0\nConceptnet 19.861 8.324 19.119 15.244 1.006 0.079 18.775 17.036 7.15 0.494\nDistilBERT-baseTREx 28.082 14.186 28.184 20.383 18.285 12.673 27.021 25.229 20.367 18.593\nGoogleRe 10.181 4.791 10.073 8.766 5.681 3.92 9.111 8.403 8.113 7.241\nSquad 10.39 5.195 10.714 6.494 6.494 2.273 11.688 9.74 5.519 5.195\nConceptnet 14.308 6.391 14.132 9.101 9.339 5.976 14.105 13.346 9.727 7.238\nALBERT-baseTREx 13.016 0 9.213 0.003 0 0.003 7.595 0.845 0 0.003\nGoogleRe 1.307 0 0.762 0 0 0 0.436 0.036 0 0\nSquad 3.896 0 1.623 0 0 0 1.299 0.649 0 0\nConceptnet 9.86 0.018 6.682 0.009 0 0 5.517 1.077 0.", "mimetype": "text/plain", "start_char_idx": 2207, "end_char_idx": 3830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7a2e3ea-e3af-4b8c-b0dc-e3212f8df20a": {"__data__": {"id_": "a7a2e3ea-e3af-4b8c-b0dc-e3212f8df20a", "embedding": null, "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "170c0da2-9c4c-4b95-ad57-2de6b8ee262b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "c7945aabab6766245721477e8cef21c62d7c96ccc9bffe73aca4bd47213c6e7f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f4e6455-6045-4617-8cd4-094ab3f449dc", "node_type": "1", "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "9d93d49af7867771af6f594c80fbc193355868dbe4e513729481f1fe8a08f62d", "class_name": "RelatedNodeInfo"}}, "text": "273 11.688 9.74 5.519 5.195\nConceptnet 14.308 6.391 14.132 9.101 9.339 5.976 14.105 13.346 9.727 7.238\nALBERT-baseTREx 13.016 0 9.213 0.003 0 0.003 7.595 0.845 0 0.003\nGoogleRe 1.307 0 0.762 0 0 0 0.436 0.036 0 0\nSquad 3.896 0 1.623 0 0 0 1.299 0.649 0 0\nConceptnet 9.86 0.018 6.682 0.009 0 0 5.517 1.077 0.009 0.018\nALBERT-largeTREx 22.057 0 0 0.133 0 0.013 0.003 0.006 0 0.003\nGoogleRe 2.686 0 0 0 0 0 0 0 0.018 0\nSquad 9.74 0 0 0 0 0 0 0 0 0\nConceptnet 14.794 0.009 0.071 0.132 0 0.009 0.026 0.044 0 0.009", "mimetype": "text/plain", "start_char_idx": 3523, "end_char_idx": 4031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebd0f469-eeaa-4439-ab70-d4f5e07ddbb7": {"__data__": {"id_": "ebd0f469-eeaa-4439-ab70-d4f5e07ddbb7", "embedding": null, "metadata": {"page_label": "14", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11909171-aa5f-4797-afa7-f0a42f099fec", "node_type": "4", "metadata": {"page_label": "14", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "69a3064c7b436a3e2cd6a431818b7c9e9709380e96d09e807e657405f1de35dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72c3682b-82d4-43bb-9d50-8c0af0c8e969", "node_type": "1", "metadata": {}, "hash": "0c7692b8a1fbfdfedfec90ffc85962fc2b5631a7b2a663f69725195ca0dd8a72", "class_name": "RelatedNodeInfo"}}, "text": "Table 5: Results from compressing different modules of decoder-only models (numbers represent accuracy).\nMajority baselines are - BoolQ : 0.621, PIQA : 0.504, Winogrande : 0.504\nModel Dataset BaselineOverall GQ AttGQ FFGQ Overall GPQ AttGPQ FFGPQ\n20% 40% 20% 40% 20% 40%\nVicuna-7BBoolq 0.7657 0.5211 0.7645 0.5437 0.5272 0.4398 0.7125 0.556 0.5346 0.4495\nPiqa 0.778 0.611 0.7671 0.6556 0.5979 0.5332 0.7617 0.7421 0.6202 0.6257\nWinogrande 0.6725 0.5391 0.678 0.5825 0.5043 0.4925 0.663 0.6298 0.5612 0.5367\nWizardLM-7BBoolq 0.7844 0.6073 0.7841 0.6003 0.5817 0.4453 0.7514 0.6801 0.6141 0.5547\nPiqa 0.7622 0.6518 0.7508 0.6654 0.623 0.5593 0.7481 0.728 0.6556 0.6371\nWinogrande 0.6646 0.5517 0.6638 0.588 0.5312 0.5193 0.6622 0.6346 0.5738 0.5817\nTable 6: Results from compressing different modules of encoder-decoder models (numbers represent accuracy).\nMajority baselines are - BoolQ : 0.621, PIQA : 0.504, Winogrande : 0.504\nModel Dataset BaselineOverall GQ AttGQ FFGQ Overall GPQ AttGPQ FFGPQ\n20% 40% 20% 40% 20% 40%\nFlanT5-BaseBoolq 0.7887 0.618 0.7841 0.6352 0.5049 0.482 0.7609 0.5058 0.6275 0.6125\nPiqa 0.6621 0.6251 0.6665 0.6415 0.5724 0.5419 0.6605 0.5631 0.6393 0.6077\nWinogrande 0.5422 0.4862 0.5359 0.5138 0.5051 0.4949 0.5272 0.5241 0.5075 0.498\nFlanT5-LargeBoolq 0.8645 0.8034 0.8615 0.8165 0.6498 0.5618 0.856 0.4107 0.819 0.7969\nPiqa 0.7138 0.6638 0.716 0.6942 0.6181 0.5555 0.7214 0.6616 0.6953 0.6921\nWinogrande 0.5991 0.5375 0.5896 0.573 0.5185 0.5067 0.5864 0.4957 0.5596 0.5572\nLamini-Flan-T5-248MBoolq 0.7982 0.7297 0.8015 0.7346 0.667 0.4349 0.7569 0.6266 0.7315 0.7263\nPiqa 0.6676 0.6393 0.6594 0.6507 0.6208 0.5462 0.6627 0.6208 0.6534 0.6338\nWinogrande 0.5304 0.5257 0.5083 0.513 0.543 0.5051 0.5099 0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72c3682b-82d4-43bb-9d50-8c0af0c8e969": {"__data__": {"id_": "72c3682b-82d4-43bb-9d50-8c0af0c8e969", "embedding": null, "metadata": {"page_label": "14", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11909171-aa5f-4797-afa7-f0a42f099fec", "node_type": "4", "metadata": {"page_label": "14", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "69a3064c7b436a3e2cd6a431818b7c9e9709380e96d09e807e657405f1de35dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebd0f469-eeaa-4439-ab70-d4f5e07ddbb7", "node_type": "1", "metadata": {"page_label": "14", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "6b652d9186245ba4bc37ec02ee8e3996052c0be8cb141f0daf04ecfb81a1cd2c", "class_name": "RelatedNodeInfo"}}, "text": "5375 0.5896 0.573 0.5185 0.5067 0.5864 0.4957 0.5596 0.5572\nLamini-Flan-T5-248MBoolq 0.7982 0.7297 0.8015 0.7346 0.667 0.4349 0.7569 0.6266 0.7315 0.7263\nPiqa 0.6676 0.6393 0.6594 0.6507 0.6208 0.5462 0.6627 0.6208 0.6534 0.6338\nWinogrande 0.5304 0.5257 0.5083 0.513 0.543 0.5051 0.5099 0.5004 0.4964 0.5028\nLamini-Flan-T5-783MBoolq 0.8306 0.7982 0.8294 0.7979 0.7716 0.6211 0.8226 0.6783 0.7994 0.7982\nPiqa 0.7073 0.673 0.7051 0.6899 0.6855 0.6192 0.7008 0.6937 0.6882 0.6866\nWinogrande 0.5549 0.5241 0.5454 0.5517 0.5193 0.4878 0.5478 0.5114 0.5288 0.5201\nLaMini-Flan-T5-248M\u221217.5\u221215.0\u221212.5\u221210.0\u22127.5\u22125.0\u22122.50.0% d op in acc\n20%\n40%Self\nAtt\nGPQ\nC oss\nAtt\nGPQEncode \nAtt\nGPQ\nDecode \nAtt\nGPQMajo ity BaselineLaMini -Flan-T5-248M\nLaMini-Flan-T5-783M\u221220\u221215\u221210\u221250% d op in acc\n20%\n40%Self\nAtt\nGPQ\nC oss\nAtt\nGPQEncode \nAtt\nGPQ\nDecode \nAtt\nGPQMajo ity BaselineLaMini -Flan-T5-783M\nFigure 12: Averaged drop in accuracy for Lamini mod-\nels under various attention modules compression (\u00a7 4.3)\n0 20 40 60 80 100\n% pr ned\u221240\u221230\u221220\u2212100LaMini -Flan-T5-248M\n0 20 40 60 80 100\n% prunedLaMini-Flan-T5-783M% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\nFigure 13: Averaged drop in accuracy for global prun-\ning (\u00a7 4.1)\nFlanT5-base\u221240\u221235\u221230\u221225\u221220\u221215\u221210\u221250% lo  \nFlanT5 -large LaMini-Flan-T5-248M LaMini-Flan-T5-783M\nLn-Structured L1-Unstructured 20%  Pruning 40% Pruning Majority BaselineFigure 14: Averaged drop in accuracy for encoder-\ndecoder models for various local pruning (\u00a7 4.4)\nV icuna-7B\u221225\u221220\u221215\u221210\u221250% drop in acc\nW izardLM -7B\nOverall\nGQFF\nGQAtt\nGQMajority  Baseline\nFigure 15: Averaged drop in accuracy for global quan-\ntization for decoder-only models (\u00a7 4.2)", "mimetype": "text/plain", "start_char_idx": 1441, "end_char_idx": 3096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41e0740c-7900-4363-81f9-151ee1ed08b4": {"__data__": {"id_": "41e0740c-7900-4363-81f9-151ee1ed08b4", "embedding": null, "metadata": {"page_label": "15", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "28a00dfc-e0b7-460e-a13e-9a18a375b027", "node_type": "4", "metadata": {"page_label": "15", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "694e3232fa41de41309d49ca971f746536772b78906ba24b4c650f5f084162b3", "class_name": "RelatedNodeInfo"}}, "text": "Flan-T5-base\u221225\u221220\u221215\u221210\u221250% drop in acc\nFlan -T5-largeLaMini-Flan-T5-248M LaMini-Flan-T5-783M\nOverall\nGQFF\nGQAtt\nGQMajority BaselineFigure 16: Averaged drop in accuracy for global quanti-\nzation for encoder-decoder models (\u00a7 4.2).\nV icuna-7B\u221235\u221230\u221225\u221220\u221215\u221210\u221250% drop in acc\nW izardLM -7B\nOverall\nGPQFF\nGPQAtt\nGPQMajority  BaselineFigure 17: Averaged drop in accuracy for global prun-\ning+quantization for decoder-only models (\u00a7 4.3).\nFlan-T5-base\u221220\u221215\u221210\u221250% drop in acc\nFlan -T5-largeLaMini-Flan-T5-248M LaMini-Flan-T5-783M\nOverall\nGPQFF\nGPQAtt\nGPQ20% 40% Majority Baseline\nFigure 18: Averaged drop in accuracy for global pruning+quantization for encoder-decoder models (\u00a7 4.3).\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200RoBER T a -base\n0 20 40 60 80 100\n% prunedRoBER T a-large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200ALBER T -base\n0 20 40 60 80 100\n% prunedALBER T -large% loss\nOverall\nGPAtt\nGPFF\nGP\nFigure 19: Averaged drop in Top-1 accuracy for encoder-only models (\u00a7 4.1).\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200BER T -base\n0 20 40 60 80 100\n% prunedBER T -large\n0 20 40 60 80 100\n% prunedDistilBER T -base% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200BER T -base\n0 20 40 60 80 100\n% prunedBER T -large\n0 20 40 60 80 100\n% prunedDistilBER T -base% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u221220020BER T -base\n0 20 40 60 80 100\n% prunedBER T -large\n0 20 40 60 80 100\n% prunedDistilBER T -base% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200BER T -base\n0 20 40 60 80 100\n% prunedBER T -large\n0 20 40 60 80 100\n% prunedDistilBER T -base% loss\nOverall\nGPAtt\nGPFF\nGP\nFigure 20: Drop in Top-1 accuracy for respective LAMA probes. Left-to-Right, Top-to-bottom: TREx, Google-\nRE, SQUAD, Conceptnet (\u00a7 4.1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9aa3fdd8-eeaa-40ff-ba7e-6dd0587b8145": {"__data__": {"id_": "9aa3fdd8-eeaa-40ff-ba7e-6dd0587b8145", "embedding": null, "metadata": {"page_label": "16", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c91681d8-f0aa-4b82-a09f-677a047669b4", "node_type": "4", "metadata": {"page_label": "16", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "f6b647099c5061bc4048611868f6e7a7dcd8bdc43ae95973fe2467692bbaac23", "class_name": "RelatedNodeInfo"}}, "text": "0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200RoBER T a -base\n0 20 40 60 80 100\n% prunedRoBER T a-large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200RoBER T a -base\n0 20 40 60 80 100\n% prunedRoBER T a-large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200RoBER T a -base\n0 20 40 60 80 100\n% prunedRoBER T a-large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200RoBER T a -base\n0 20 40 60 80 100\n% prunedRoBER T a-large% loss\nOverall\nGPAtt\nGPFF\nGPFigure 21: Drop in Top-1 accuracy for respective LAMA probes. Left-to-Right, Top-to-bottom: TREx, Google-\nRE, SQUAD, Conceptnet (\u00a7 4.1).\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200ALBER T -base\n0 20 40 60 80 100\n% prunedALBER T -large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200ALBER T -base\n0 20 40 60 80 100\n% prunedALBER T -large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u221220020ALBER T -base\n0 20 40 60 80 100\n% prunedALBER T -large% loss\nOverall\nGPAtt\nGPFF\nGP\n0 20 40 60 80 100\n% pruned\u2212100\u221280\u221260\u221240\u2212200ALBER T -base\n0 20 40 60 80 100\n% prunedALBER T -large% loss\nOverall\nGPAtt\nGPFF\nGP\nFigure 22: Drop in Top-1 accuracy for respective LAMA probes. Left-to-Right, Top-to-bottom: TREx, Google-\nRE, SQUAD, Conceptnet (\u00a7 4.1).\n0 20 40 60 80 100\n% pruned\u221250\u221240\u221230\u221220\u2212100V icuna -7B\n0 20 40 60 80 100\n% prunedWizardLM-7B% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pruned\u221240\u221230\u221220\u2212100V icuna -7B\n0 20 40 60 80 100\n% prunedW izardLM-7B% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pruned\u221230\u221220\u2212100V icuna -7B\n0 20 40 60 80 100\n% prunedW izardLM-7B% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\nFigure 23: Drop in accuracy for decoder-only models. Left-to-Right, Top-to-Bottom: BoolQ, PIQA, and Wino-\ngrande (\u00a7 4.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c837539e-cf42-48af-8559-8fb5aaa95263": {"__data__": {"id_": "c837539e-cf42-48af-8559-8fb5aaa95263", "embedding": null, "metadata": {"page_label": "17", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2157e198-ca13-4dda-832f-ab06c9941905", "node_type": "4", "metadata": {"page_label": "17", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7635f725c197744e217b5aa75566581bc50a46c84db6ff941848727a80babef6", "class_name": "RelatedNodeInfo"}}, "text": "FlanT5-base\u221240\u221230\u221220\u2212100% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajori y BaselineFlanT5 -base\nLaMini-Flan-T5-248M\u221235\u221230\u221225\u221220\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCro  \nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajority Ba elineLaMini -Flan-T5-248M\nFlanT5-large\u221240\u221230\u221220\u2212100% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajori y BaselineFlanT5 -large\nLaMini-Flan-T5-783M\u221235\u221230\u221225\u221220\u221215\u221210\u221250% d op in acc\n20%\n40%Self\nAtt\nGPQ\nC oss\nAtt\nGPQEncode \nAtt\nGPQ\nDecode \nAtt\nGPQMajo ity BaselineLaMini -Flan-T5-783M\nFlanT5-base\u221225\u221220\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajorit  BaselineFlanT5 -base\nLaMini-Flan-T5-248M\u221225\u221220\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajori y BaselineLaMini -Flan-T5-248M\nFlanT5-large\u221230\u221225\u221220\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajori y BaselineFlanT5 -large\nLaMini-Flan-T5-783M\u221230\u221225\u221220\u221215\u221210\u221250% d op in acc\n20%\n40%Self\nAtt\nGPQ\nC oss\nAtt\nGPQEncode \nAtt\nGPQ\nDecode \nAtt\nGPQMajo ity BaselineLaMini -Flan-T5-783M\nFlanT5-base\u22126\u22124\u221220% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajorit  BaselineFlanT5 -base\nLaMini-Flan-T5-248M\u22127\u22126\u22125\u22124\u22123\u22122\u221210% dro  in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajority BaselineLaMini -Flan-T5-248M\nFlanT5-large\u221215\u221210\u221250% drop in acc\n20%\n40%Self\nAtt\nGPQ\nCross\nAtt\nGPQEncoder\nAtt\nGPQ\nDecoder\nAtt\nGPQMajorit  BaselineFlanT5 -large\nLaMini-Flan-T5-783M\u22128\u22126\u22124\u221220% d op in acc\n20%\n40%Self\nAtt\nGPQ\nC oss\nAtt\nGPQEncode \nAtt\nGPQ\nDecode \nAtt\nGPQMajo ity BaselineLaMini -Flan-T5-783MFigure 24: Drop in accuracy across various datasets for encoder-decoder models under various attention modules\ncompression. Top-to-Bottom: BoolQ, PIQA, Winogrande (\u00a7 4.3)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4da55d8f-6015-4fcb-a2da-ed023c8f9d3e": {"__data__": {"id_": "4da55d8f-6015-4fcb-a2da-ed023c8f9d3e", "embedding": null, "metadata": {"page_label": "18", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "849c87ee-1a10-402f-9c15-526ced6333dd", "node_type": "4", "metadata": {"page_label": "18", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "0115f2f6d17295146c53ae0e3f2d2f04add9544d1fc0b85668d65f9ecca52fcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f337e99f-6b50-4244-b6ba-d9f3bf87caf2", "node_type": "1", "metadata": {}, "hash": "f9a39e2010d6bfe437e13096ed0a6bef9653e4fbdf6e2c957e7f205df606aeb6", "class_name": "RelatedNodeInfo"}}, "text": "0 20 40 60 80 100\n% pr ned\u221260\u221250\u221240\u221230\u221220\u221210010FlanT5 -base\n0 20 40 60 80 100\n% prunedFlanT5-large% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pr ned\u221260\u221250\u221240\u221230\u221220\u2212100LaMini -Flan-T5-248M\n0 20 40 60 80 100\n% prunedLaMini-Flan-T5-783M% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pr ned\u221230\u221220\u221210010Flan -T5-base\n0 20 40 60 80 100\n% prunedFlan-T5-large% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pr ned\u221230\u221220\u2212100LaMini -Flan-T5-248M\n0 20 40 60 80 100\n% prunedLaMini-Flan-T5-783M% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pruned\u221225\u221220\u221215\u221210\u221250510Flan -T5-base\n0 20 40 60 80 100\n% prunedFlan-T5-large% loss\nOverall\nGPAtt\nGPFF\nGPMajority Baseline\n0 20 40 60 80 100\n% pr ned\u221220\u221215\u221210\u2212505LaMini -Flan-T5-248M\n0 20 40 60 80 100\n% prunedLaMini-Flan-T5-783M% loss\nOverall\nGPAtt\nGPFF\nGPMajority BaselineFigure 25: Drop in accuracy across various datasets for encoder-decoder models. Top-to-Bottom: BoolQ, PIQA,\nWinogrande (\u00a7 4.1)\nBER T -base BER T -large DistilBER T -base-70.0-60.0-50.0-40.0-30.0-20.0-10.00.0%  loss\nRoBER T a-base RoBER T a-largeALBER T -base ALBER T -large\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning\nBER T -base BER T -large DistilBER T -base-80.0-70.0-60.0-50.0-40.0-30.0-20.0-10.00.0% loss\nRoBER T a-base RoBER T a-largeALBER T -base ALBER T -large\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning\nBER T -base BER T -large DistilBER T -base-70.0-60.0-50.0-40.0-30.0-20.0-10.00.010.0%  loss\nRoBER T a-base RoBER T a-largeALBER T -base ALBER T -large\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning\nBER T -base BER T -large DistilBER T -base-80.0-70.0-60.0-50.0-40.0-30.0-20.0-10.00.0% loss\nRoBER T a-base RoBER T a-largeALBER T -base ALBER T -large\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning\nFigure 26: Drop in Top-1 accuracy across various datasets for encoder-only models for FLP.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f337e99f-6b50-4244-b6ba-d9f3bf87caf2": {"__data__": {"id_": "f337e99f-6b50-4244-b6ba-d9f3bf87caf2", "embedding": null, "metadata": {"page_label": "18", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "849c87ee-1a10-402f-9c15-526ced6333dd", "node_type": "4", "metadata": {"page_label": "18", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "0115f2f6d17295146c53ae0e3f2d2f04add9544d1fc0b85668d65f9ecca52fcd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4da55d8f-6015-4fcb-a2da-ed023c8f9d3e", "node_type": "1", "metadata": {"page_label": "18", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "87a452f8efc499af245a3ad2f83b80e0e3e0aef64b31d252dbda572abe6ed83e", "class_name": "RelatedNodeInfo"}}, "text": "Left-to-Right, Top-\nto-Bottom: TRex, Google-RE, SQUAD, ConceptNet (\u00a7 4.4)\nV icuna-7B-40.0-30.0-20.0-10.00.0% loss\nWizardLM-7B\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning Majority Baseline\nV icuna-7B-35.0-30.0-25.0-20.0-15.0-10.0-5.00.0%  loss\nWizardLM-7B\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning Majority Baseline\nV icuna-7B-30.0-25.0-20.0-15.0-10.0-5.00.0%  loss\nWizardLM-7B\nLn-Structured L1-Unstructured 20% Pruning 40% Pruning Majority Baseline\nFigure 27: Drop in accuracy across various datasets for decoder-only models for FLP. Left-to-Right, Top-to-\nBottom: BoolQ, PIQA, and Winogrande (\u00a7 4.4)", "mimetype": "text/plain", "start_char_idx": 1913, "end_char_idx": 2535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59b00f39-92ba-4abd-a84f-9d21694ef09b": {"__data__": {"id_": "59b00f39-92ba-4abd-a84f-9d21694ef09b", "embedding": null, "metadata": {"page_label": "19", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5c08f50-f87f-4888-afd8-7bf9455cccd5", "node_type": "4", "metadata": {"page_label": "19", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "412bb7bce76bd6956a01201a31c87a290d13c0984ab546bde39c343961142045", "class_name": "RelatedNodeInfo"}}, "text": "FlanT5-base\u221260\u221250\u221240\u221230\u221220\u2212100% loss\nFlanT5 -large LaMini-Flan-T5-248M LaMini-Flan-T5-783M\nLn-Structured L1-Unstructured 20%  Pruning 40% Pruning Majority Baseline\nFlanT5-base\u221230\u221225\u221220\u221215\u221210\u221250% lo  \nFlanT5 -large LaMini-Flan-T5-248M LaMini-Flan-T5-783M\nLn-Structured L1-Unstructured 20%  Pruning 40% Pruning Majority Baseline\nFlanT5-base\u221220.0\u221217.5\u221215.0\u221212.5\u221210.0\u22127.5\u22125.0\u22122.50.0% loss\nFlanT5 -large LaMini-Flan-T5-248M LaMini-Flan-T5-783M\nLn-Structured L1-Unstructured 20%  Pruning 40% Pruning Majority BaselineFigure 28: Drop in accuracy across various datasets for encoder-decoder models for FLP. Left-to-Right: BoolQ,\nPIQA, and Winogrande (\u00a7 4.4)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b938455-04fd-45fd-b715-8cb9e35c8276": {"__data__": {"id_": "2b938455-04fd-45fd-b715-8cb9e35c8276", "embedding": null, "metadata": {"page_label": "207", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43d2a3c4-b08b-4b17-8a6e-2caa53e215b3", "node_type": "4", "metadata": {"page_label": "207", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "88b61866598101b6e842fb39394173cd2f83dc4af6f68a2ba7daacee308746c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "036e2208-93d9-4619-b0a3-eb6e5d23be37", "node_type": "1", "metadata": {}, "hash": "307a349fa0466643a030428ed529a4c79869e03358cb04452848753a51e38e37", "class_name": "RelatedNodeInfo"}}, "text": "ScienceDirect\nAvailable online at www.sciencedirect.com\nProcedia Computer Science 171 (2020)  207\u2013216\n1877-0509 \u00a9 2020 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ )\nPeer-review under responsibility of the scientific committee of the Third International Conference on Computing and Network  \nCommunications (CoCoNet\u201919).\n10.1016/j.procs.2020.04.022\n10.1016/j.procs.2020.04.022 1877-0509\u00a9 2020 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ )\nPeer-review under responsibility of the scientific committee of the Third International Conference on Computing and Network  \nCommunications (CoCoNet\u201919).Available online at www.sciencedirect.com\nProcedia Computer Science 00 (2019) 000\u2013000\nwww.elsevier.com/locate/procedia\nThird International Conference on Computing and Network Communications (CoCoNet\u201919)\nNITCAD - Developing an object detection, classi\ufb01cation and stereo\nvision dataset for autonomous navigation in Indian roads\nNamburi GNVV Satya Sai Srinath, Athul Zac Joseph, S Umamaheswaran, Ch. Lakshmi Priyanka, Malavika Nair M,\nPraveen Sankaran\nNational Institute of Technology, Calicut, Kerala, India\nAbstract\nAutonomous vehicles with various levels of autonomy are becoming popular in developed countries due to their e\ufb00ectiveness in\nreducing the fatalities caused by road accidents. A developing country like India with the second largest population in the world,creates unique road scenarios for an autonomous car which requires a lot of testing and \ufb01ne tuning before implementation. This\nleads to the importance of datasets providing information about various tra\ufb03c situations in India. For planning its path ahead,\nautonomous vehicles have to detect, classify and estimate the depth of obstacles that they encounter on roads. The purpose of this\npaper is to provide a dataset for object classi\ufb01cation, detection and stereo vision corresponding to Indian roads which can serve\nas a platform for developing e\ufb00ective algorithms for autonomous cars in Indian roads. In this work, we benchmarked the object\nclassi\ufb01cation by using confusion matrix obtained from various deep learning models, evaluated detection using Faster R-CNN and\ncompared depth estimation processed by Realsense stereo camera by applying convolutional neural network based algorithms.\nc\u20dd2020 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)\nPeer-review under responsibility of the scienti\ufb01c committee of the Third International Conference on Computing and Network\nCommunications (CoCoNet\u201919).\nKeywords: Dataset, Object classi\ufb01cation, Object detection, Stereo vision, Indian roads\n1. Introduction\nThe future of transportation lies in the development of autonomous vehicles that can eliminate the factor of human\nerror thereby reducing the chance of road accidents. The technology will only be e\ufb00ective if implemented across all\nthe vehicles on the roads thereby reducing the uncertainties associated with drivers. For perfecting the technologyto be implemented on a large scale, it should be tested across various tra\ufb03c situations that will be encountered by\nthese vehicles. Several large datasets like Imagenet [1] and COCO [2], are available for image classi\ufb01cation but\n\u2217Corresponding author. Tel.: +0495-2286721.\nE-mail address: psankaran@nitc.ac.in\n1877-0509 c\u20dd2020 The Authors. Published by Elsevier B.V .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "036e2208-93d9-4619-b0a3-eb6e5d23be37": {"__data__": {"id_": "036e2208-93d9-4619-b0a3-eb6e5d23be37", "embedding": null, "metadata": {"page_label": "207", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43d2a3c4-b08b-4b17-8a6e-2caa53e215b3", "node_type": "4", "metadata": {"page_label": "207", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "88b61866598101b6e842fb39394173cd2f83dc4af6f68a2ba7daacee308746c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b938455-04fd-45fd-b715-8cb9e35c8276", "node_type": "1", "metadata": {"page_label": "207", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "b598e786e90cbf9906931f164030017906e41adcd5687a680b111f49fdf47f80", "class_name": "RelatedNodeInfo"}}, "text": "Keywords: Dataset, Object classi\ufb01cation, Object detection, Stereo vision, Indian roads\n1. Introduction\nThe future of transportation lies in the development of autonomous vehicles that can eliminate the factor of human\nerror thereby reducing the chance of road accidents. The technology will only be e\ufb00ective if implemented across all\nthe vehicles on the roads thereby reducing the uncertainties associated with drivers. For perfecting the technologyto be implemented on a large scale, it should be tested across various tra\ufb03c situations that will be encountered by\nthese vehicles. Several large datasets like Imagenet [1] and COCO [2], are available for image classi\ufb01cation but\n\u2217Corresponding author. Tel.: +0495-2286721.\nE-mail address: psankaran@nitc.ac.in\n1877-0509 c\u20dd2020 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)\nPeer-review under responsibility of the scienti\ufb01c committee of the Third International Conference on Computing and Network Communications\n(CoCoNet\u201919).Available online at www.sciencedirect.com\nProcedia Computer Science 00 (2019) 000\u2013000\nwww.elsevier.com/locate/procedia\nThird International Conference on Computing and Network Communications (CoCoNet\u201919)\nNITCAD - Developing an object detection, classi\ufb01cation and stereo\nvision dataset for autonomous navigation in Indian roads\nNamburi GNVV Satya Sai Srinath, Athul Zac Joseph, S Umamaheswaran, Ch. Lakshmi Priyanka, Malavika Nair M,\nPraveen Sankaran\nNational Institute of Technology, Calicut, Kerala, India\nAbstract\nAutonomous vehicles with various levels of autonomy are becoming popular in developed countries due to their e\ufb00ectiveness in\nreducing the fatalities caused by road accidents. A developing country like India with the second largest population in the world,creates unique road scenarios for an autonomous car which requires a lot of testing and \ufb01ne tuning before implementation. This\nleads to the importance of datasets providing information about various tra\ufb03c situations in India. For planning its path ahead,\nautonomous vehicles have to detect, classify and estimate the depth of obstacles that they encounter on roads. The purpose of this\npaper is to provide a dataset for object classi\ufb01cation, detection and stereo vision corresponding to Indian roads which can serve\nas a platform for developing e\ufb00ective algorithms for autonomous cars in Indian roads. In this work, we benchmarked the object\nclassi\ufb01cation by using confusion matrix obtained from various deep learning models, evaluated detection using Faster R-CNN and\ncompared depth estimation processed by Realsense stereo camera by applying convolutional neural network based algorithms.\nc\u20dd2020 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)\nPeer-review under responsibility of the scienti\ufb01c committee of the Third International Conference on Computing and Network\nCommunications (CoCoNet\u201919).\nKeywords: Dataset, Object classi\ufb01cation, Object detection, Stereo vision, Indian roads\n1. Introduction\nThe future of transportation lies in the development of autonomous vehicles that can eliminate the factor of human\nerror thereby reducing the chance of road accidents. The technology will only be e\ufb00ective if implemented across all\nthe vehicles on the roads thereby reducing the uncertainties associated with drivers. For perfecting the technologyto be implemented on a large scale, it should be tested across various tra\ufb03c situations that will be encountered by\nthese vehicles. Several large datasets like Imagenet [1] and COCO [2], are available for image classi\ufb01cation but\n\u2217Corresponding author. Tel.: +0495-2286721.\nE-mail address: psankaran@nitc.ac.in\n1877-0509 c\u20dd2020 The Authors. Published by Elsevier B.V .\nThis is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)\nPeer-review under responsibility of the scienti\ufb01c committee of the Third International Conference on Computing and Network Communications\n(CoCoNet\u201919).", "mimetype": "text/plain", "start_char_idx": 2786, "end_char_idx": 6922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fe560a9-5a9d-4d25-93d2-97872a1f74f5": {"__data__": {"id_": "7fe560a9-5a9d-4d25-93d2-97872a1f74f5", "embedding": null, "metadata": {"page_label": "208", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18652fc4-e458-4978-8e4f-c40595a1e447", "node_type": "4", "metadata": {"page_label": "208", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "3afb88d4b02f02d44058ed02d7277e195c6bd64b968c403f973049056878cc0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7be9412-e657-450c-8c1b-5bc5b748fa76", "node_type": "1", "metadata": {}, "hash": "6c9288c8c15a1c34c6e8fcec66be461ba3e95e1cecc403bae7f455f57081c434", "class_name": "RelatedNodeInfo"}}, "text": "208 Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216\n2 Author name /Procedia Computer Science 00 (2019) 000\u2013000\nfor localisation and segmentation, which are primary tasks for autonomous vehicles, only a few datasets like Pascal\nVOC [ 3] can be used. But it has a wide variety of classes that are not necessary for autonomous navigation. There\nare some datasets available exclusively for pedestrian detection such as Caltech Pedestrian Dataset [4], Citypersonsdataset [5], Daimler Pedestrain [6]. Krause et al. [7] created a dataset which can be used for classi\ufb01cation of cars. Butautonomous vehicles need to detect di\ufb00erent classes of obstacles present in its path. For that, Oxford robotcar dataset[8] has collected data in the city of Oxford, UK at various times in the same location. KITTI [9] is another dataset\nwhich is collected on the urban roads of Karlsruhe, Germany. Cityscapes [10], and Mapillary Vistas [11] created\ndatasets for semantic understanding of urban streets. More recently Apolloscapes [12], BDD100k [13], nuScenes [14]provided datasets that are collected across various weather conditions, times and places, along with labels which arecrowd sourced.\nTo test an autonomous vehicle in India, the above mentioned datasets cannot be used due to the lack of information\nabout classes like auto rickshaws that are exclusively found on Indian roads. Also, unstructured tra\ufb03c scenarios can\nbe observed frequently on Indian roads. These factors make it essential to have a dataset exclusively for Indian roads\nwhich can give information regarding the same. Varma et al. [15] has studied about the situations in Indian roadsand created a dataset named IDD. For an autonomous vehicle to plan its path, it should be aware of distances toother vehicles in its vicinity so that the velocity of other vehicles can be estimated. This can be achieved by using\na 3D LiDAR which can scan its surroundings and give distances to di\ufb00erent obstacles but this is rather expensive.\nA cost e\ufb00ective alternative is to use a stereo camera which can provide the depth information about the obstacles.\nThe performance of this method purely depends upon the algorithms for evaluating depth information. Due to the\nintroduction of better algorithms this method will be more suitable for a price conscious market like India.\nIn this context, there is a requirement for a new dataset, that can be used to develop autonomous navigation systems\nfor Indian roads. So, a dataset named National Institute of Technology, Calicut Autonomous Driving (NITCAD) ispresented in this paper. NITCAD primarily consists of NITCAD object dataset which can be used for classi\ufb01cation,\ndetection and NITCAD stereo vision dataset for depth estimation on Indian roads, thereby leading to the development\nof level 3 autonomous vehicles capable of handling Indian road scenarios.\n2. Methodology\nThe autonomous vehicles that are being developed for Indian roads should be able to detect and classify di\ufb00erent\nvehicle classes that are exclusively found here. This will be advantageous for performing the path planning operation\nsince di\ufb00erent vehicle classes behave di\ufb00erently on roads. NITCAD object dataset provided here was created underthis objective.\nInorder to keep track of the detected objects on the road, the autonomous vehicle needs to evaluate the velocity\nof these objects. For developing stereo vision based velocity estimation algorithms, NITCAD stereo vision datasetprovides image data collected from synchronised left and right cameras having global shutter.\nThe NITCAD object dataset was evaluated with di\ufb00erent deep learning architectures and the respective confusion\nmatrix, precision, recall values were found out. For the NITCAD stereo vision dataset, the relative di\ufb00erence betweenthe disparity maps obtained by di\ufb00erent methods are evaluated and interpreted.\n2.1. Tra\ufb03c scenario in India\nIndia has one of the the highest number of road fatalities in the world. Autonomous navigation is one of the solu-\ntions to reduce the number of fatalities due to road accidents. To build an e\ufb03cient and reliable system, the knowledge\nof the tra\ufb03c structure in India is highly essential.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7be9412-e657-450c-8c1b-5bc5b748fa76": {"__data__": {"id_": "c7be9412-e657-450c-8c1b-5bc5b748fa76", "embedding": null, "metadata": {"page_label": "208", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18652fc4-e458-4978-8e4f-c40595a1e447", "node_type": "4", "metadata": {"page_label": "208", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "3afb88d4b02f02d44058ed02d7277e195c6bd64b968c403f973049056878cc0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fe560a9-5a9d-4d25-93d2-97872a1f74f5", "node_type": "1", "metadata": {"page_label": "208", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "fb1cf853da771beb9b19fc03c41a8f3fbf73a87393253554f04a965a97373b5a", "class_name": "RelatedNodeInfo"}}, "text": "For developing stereo vision based velocity estimation algorithms, NITCAD stereo vision datasetprovides image data collected from synchronised left and right cameras having global shutter.\nThe NITCAD object dataset was evaluated with di\ufb00erent deep learning architectures and the respective confusion\nmatrix, precision, recall values were found out. For the NITCAD stereo vision dataset, the relative di\ufb00erence betweenthe disparity maps obtained by di\ufb00erent methods are evaluated and interpreted.\n2.1. Tra\ufb03c scenario in India\nIndia has one of the the highest number of road fatalities in the world. Autonomous navigation is one of the solu-\ntions to reduce the number of fatalities due to road accidents. To build an e\ufb03cient and reliable system, the knowledge\nof the tra\ufb03c structure in India is highly essential. Tra\ufb03c in India is highly heterogeneous and the models trained for\nautonomous navigation in other countries may not be su\ufb03cient to characterize Indian tra\ufb03c. Indias roads are func-tionally classi\ufb01ed as expressways, national highways, state highways, district roads, and rural roads. Out of this, only\nexpressways and some of the national highways are four-lane or six-lane. An example of unlaned tra\ufb03c junction can\nbe observed in Fig. 1(a). Most roads are unpaved with potholes and with ambiguous boundaries. Compared to other\ncountries, India has low road density per 1000 people. This has led to many problems like tra\ufb03c congestion and\nirregular tra\ufb03c speeds. The Indian roads carry almost 90 per cent of the countrys passenger tra\ufb03c. Passengers use amultitude of vehicles for transport daily which include cars, two-wheelers, and auto-rickshaws, etc. Auto-rickshaws\nAuthor name /Procedia Computer Science 00 (2019) 000\u2013000 3\n(a) An unlaned tra\ufb03c junction. (b) Unstructured environment prevalent in India.\nFig. 1: General tra\ufb03c scenarios on Indian Roads.\nare a class of vehicles that are truly unique to the Indian tra\ufb03c system. Further, the frequency and variety of trucks\nand buses are also high as compared to other countries. Another huge bottleneck for autonomous navigation in India\nis that the pedestrians and drivers are less likely to follow tra\ufb03c rules. Pedestrians often cross the road at arbitrary\nlocations and drivers sometimes overtake from the wrong side. An example of this unstructuredness in tra\ufb03c can be\nobserved in Fig. 1(b).\n2.2. Collection of data\nTo collect data, one RGB camera and a Stereo camera were mounted on a car and was made to travel in the rural\nand urban roads of Kerala where various tra\ufb03c situations arise. The route in which the data was collected is shown inFig. 2\nFig. 2: Route followed for data collection.\n2.3. NITCAD Object dataset\nFor training the system to classify di\ufb00erent objects that could be encountered on an Indian road, a dataset including\na variety of classes needs to be created. By using Noise Play 2 Action camera, tra\ufb03c in and around Kottayam district,\nKerala was recorded along the route shown in Fig.2 in 720p at 30fps. A set of images at a rate of 5 images per second\nwere generated from this recorded video footage. These images were annotated using an online tool - Label box and\na text \ufb01le was created for each scene which has information about the location of various objects in that particularframe. These \ufb01les can then be used for the visualization of the dataset as well as for training a system to detect and", "mimetype": "text/plain", "start_char_idx": 3388, "end_char_idx": 6779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88c43357-a485-4077-8b71-c1c4561a25d4": {"__data__": {"id_": "88c43357-a485-4077-8b71-c1c4561a25d4", "embedding": null, "metadata": {"page_label": "209", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "86700f1d-e8b7-4f46-9a08-28feafe4e597", "node_type": "4", "metadata": {"page_label": "209", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "a013191827502a2fbff03efbc1fdb2db08657f260f3c3baf373993c07ef079c8", "class_name": "RelatedNodeInfo"}}, "text": "Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216 209\nAuthor name /Procedia Computer Science 00 (2019) 000\u2013000 3\n(a) An unlaned tra\ufb03c junction. (b) Unstructured environment prevalent in India.\nFig. 1: General tra\ufb03c scenarios on Indian Roads.\nare a class of vehicles that are truly unique to the Indian tra\ufb03c system. Further, the frequency and variety of trucks\nand buses are also high as compared to other countries. Another huge bottleneck for autonomous navigation in India\nis that the pedestrians and drivers are less likely to follow tra\ufb03c rules. Pedestrians often cross the road at arbitrary\nlocations and drivers sometimes overtake from the wrong side. An example of this unstructuredness in tra\ufb03c can beobserved in Fig. 1(b).\n2.2. Collection of data\nTo collect data, one RGB camera and a Stereo camera were mounted on a car and was made to travel in the rural\nand urban roads of Kerala where various tra\ufb03c situations arise. The route in which the data was collected is shown inFig. 2\nFig. 2: Route followed for data collection.\n2.3. NITCAD Object dataset\nFor training the system to classify di\ufb00erent objects that could be encountered on an Indian road, a dataset including\na variety of classes needs to be created. By using Noise Play 2 Action camera, tra\ufb03c in and around Kottayam district,\nKerala was recorded along the route shown in Fig.2 in 720p at 30fps. A set of images at a rate of 5 images per second\nwere generated from this recorded video footage. These images were annotated using an online tool - Label box and\na text \ufb01le was created for each scene which has information about the location of various objects in that particularframe. These \ufb01les can then be used for the visualization of the dataset as well as for training a system to detect and\nAuthor name /Procedia Computer Science 00 (2019) 000\u2013000 3\n(a) An unlaned tra\ufb03c junction. (b) Unstructured environment prevalent in India.\nFig. 1: General tra\ufb03c scenarios on Indian Roads.\nare a class of vehicles that are truly unique to the Indian tra\ufb03c system. Further, the frequency and variety of trucks\nand buses are also high as compared to other countries. Another huge bottleneck for autonomous navigation in India\nis that the pedestrians and drivers are less likely to follow tra\ufb03c rules. Pedestrians often cross the road at arbitrary\nlocations and drivers sometimes overtake from the wrong side. An example of this unstructuredness in tra\ufb03c can beobserved in Fig. 1(b).\n2.2. Collection of data\nTo collect data, one RGB camera and a Stereo camera were mounted on a car and was made to travel in the rural\nand urban roads of Kerala where various tra\ufb03c situations arise. The route in which the data was collected is shown inFig. 2\nFig. 2: Route followed for data collection.\n2.3. NITCAD Object dataset\nFor training the system to classify di\ufb00erent objects that could be encountered on an Indian road, a dataset including\na variety of classes needs to be created. By using Noise Play 2 Action camera, tra\ufb03c in and around Kottayam district,\nKerala was recorded along the route shown in Fig.2 in 720p at 30fps. A set of images at a rate of 5 images per second\nwere generated from this recorded video footage. These images were annotated using an online tool - Label box and\na text \ufb01le was created for each scene which has information about the location of various objects in that particularframe. These \ufb01les can then be used for the visualization of the dataset as well as for training a system to detect and", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bd6c1bc-d781-4809-a7bf-4bca566ae187": {"__data__": {"id_": "7bd6c1bc-d781-4809-a7bf-4bca566ae187", "embedding": null, "metadata": {"page_label": "210", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6c816e5-234e-44af-af10-3c30e3040d2c", "node_type": "4", "metadata": {"page_label": "210", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "908435d837b6fa2af91ca1ef384a11e743434ec9b0cac8984c54112cb2836f84", "class_name": "RelatedNodeInfo"}}, "text": "210 Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216\n4 Author name /Procedia Computer Science 00 (2019) 000\u2013000\n(a) Number of objects per class. (b) Number of images having a class.\nFig. 3: Details of NITCAD object dataset\nFig. 4: An example of various classes present in NITCAD object dataset. From left to right: Car, Bus, Pedestrian, Two wheeler, Truck, Van and\nAuto rickshaw\nclassify the objects on road. There are seven classes in the dataset namely car, pedestrian, auto rickshaw, truck, two\nwheeler, bus and van. A total of 11000 images were collected under di\ufb00erent tra\ufb03c conditions out of which 4800images were manually labelled.\nFig. 3(a) gives details about the frequency of each class. From Fig. 3(a), it can be observed that the number of\ncars present in the dataset is maximum and the number of vans present is minimum. Fig. 3(b) gives details about the\nnumber of images or frames having a particular class from which it can be inferred that cars, auto rickshaws and twowheelers are almost present in all the frames while vans occur at rare instances on roads. Fig. 4gives a typical example\nof all the classes present in our dataset.\n2.3.1. Camera Calibration:\nThe images obtained from Noise play action camera were subjected to radial and tangential distortion because\nof the wide angle lens employed in the camera. For preserving the information in each frame the intrinsic camera", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f099a76-4b64-48c2-a38e-191c92f62c3f": {"__data__": {"id_": "5f099a76-4b64-48c2-a38e-191c92f62c3f", "embedding": null, "metadata": {"page_label": "211", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bffa0247-445a-4879-9764-43d456e665b3", "node_type": "4", "metadata": {"page_label": "211", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "444abb70f7a001997fc7422ee72a5b701e0b2a3af166489800928458bba0d348", "class_name": "RelatedNodeInfo"}}, "text": "Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216 211\nAuthor name /Procedia Computer Science 00 (2019) 000\u2013000 5\n(a) distorted image. (b) undistorted image.\nFig. 5: Camera Calibration results.\nTable 1: Comparison with di\ufb00erent datasets.\nDataset Year Avg.Resolution Train/Validation/Split Location Unstructured\nKITTI 2012 1245x375 7.5k/-/7.5k Karlsruhe\nBDD100k 2017 1280x720 84M/36M NY, SF\nVistas 2017 1920x1080 18k/2k/5k 6 continents\nIDD 2019 1920x1080 31k/10k/4k Hyderabad, Bangalore \u2713\nNITCAD 2019 1280x720* 4.8k/2.7k Kerala \u2713\n* indicates the dimension before the undistortion operation. After undistortion, it is 1172x544.\nmatrix and distortion coe\ufb03cients of the camera where computed according to Zhang [18] and were obtained as below:\ncamera intrinsic matrix =\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0791.6965 0 632.9851\n0 791.4219 347.7182\n001\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nradial distortion coe\ufb03cients =[\n\u22120.3454 0.1593 \u22120.0344]\ntangential distortion coe\ufb03cients =[\n0.0021 0.0016]\nWith the above obtained camera intrinsic matrix and distortion coe\ufb03cients the images taken by Noise play action\ncamera were undistorted and provided in the dataset. An example of distorted and corresponding undistorted image\nis shown in Fig.5 where the distortion is clearly visible near the edges. Around 10,000 undistorted images (of which\n3600 are labelled) are also provided.\n2.4. NITCAD Stereo vision dataset\nFor performing the task of depth estimation, data was collected in and around Kottayam district, Kerala using Intel\nRealSense Depth camera D435. This depth camera has 2 infrared cameras having global shutter that are triggered\nsimultaneously so that calculation of disparity for a particular scene is possible. By using its inbuilt vision processor\na disparity map can be generated which can be used for depth estimation. More e\ufb03cient algorithms can be developed\nto improve its accuracy so that it becomes a cost e\ufb00ective depth approximation technique using stereo vision. Depth\nestimation obtained by the inbuilt vision processor can be used for validation of the results obtained after performing\nstereo algorithms. An example image is shown in Fig. 6where a small disparity can be observed.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 2179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "285ced6c-d49f-4772-a669-85f038fd62d1": {"__data__": {"id_": "285ced6c-d49f-4772-a669-85f038fd62d1", "embedding": null, "metadata": {"page_label": "212", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "220e706a-84d9-4816-8cd8-ac85cc98b1cb", "node_type": "4", "metadata": {"page_label": "212", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "30c0ed50e1bd0eaeffe9b490d3133ceacd02e35312b4a0a80256c0f24a5567e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20e94ebe-d4ed-4ad7-a6a9-fe92ef8a1db5", "node_type": "1", "metadata": {}, "hash": "92a8abf03bde151e711a82c22feeedcd5e91043f5236972072d18c5f8366aec8", "class_name": "RelatedNodeInfo"}}, "text": "212 Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216\n6 Author name /Procedia Computer Science 00 (2019) 000\u2013000\n(a) Left image. (b) Right image.\nFig. 6: An example image pair from NITCAD stereo vision dataset.\nTable 2: Evaluation of various architectures on NITCAD object dataset.\nArchitecture Accuracy* Precision F1 score\nDenseNet [19] 0.828 0.795 0.782\nInceptionv3 [20] 0.836 0.801 0.805\nMobilenet [21] 0.839 0.78 0.796\nNASNet [22] 0.811 0.757 0.760\nVGG16 [23] 0.789 0.779 0.750\nXception [24] 0.854 0.832 0.825\n*Accuracy and Recall values are same as micro-averaging\nis considered for multiclass confusion matrix\n3. Evaluation\nNITCAD can be considered challenging only if the classes present in it are di\ufb03cult to classify. Thus the dataset is\nprocessed accordingly and the cropped images are fed to various classi\ufb01cation algorithms to obtain confusion matrix.\nTo evaluate the detection algorithms on our dataset, Faster R-CNN is used. The stereo dataset is evaluated on the basisof average of the relative di\ufb00erence between the disparity maps generated (R\ndi f f) by taking the output obtained from\nIntel RealSense as the ground truth.\n3.1. NITCAD object dataset- Evaluation for classi\ufb01cation\nTo evaluate how good the classi\ufb01cation algorithms perform on the dataset, confusion matrix for di\ufb00erent deep\nlearning architectures was computed. Each image is cropped to extract all individual classes and a subset of the\nlabelled data was considered for training, validation and testing. Pre-trained models on Imagenet were taken and\ninitial layers were frozen as they learn about simple features like edges and lines and these are common in all theobjects. Validation accuracy is used as a metric while training for 20 epochs and the best weights are used for testing.The confusion matrix for various architectures are represented in Fig. 7.\nIt can be inferred that the class van is being confused with car/auto by these architectures. Also, the auto-rickshaw\nwhich is the most common class in Indian roads has been classi\ufb01ed well as the dataset collected has enough numberof autos to train networks. The accuracy and precision values of various classes and models can be seen in Table 2\nand Table 36 Author name /Procedia Computer Science 00 (2019) 000\u2013000\n(a) Left image. (b) Right image.\nFig. 6: An example image pair from NITCAD stereo vision dataset.\nTable 2: Evaluation of various architectures on NITCAD object dataset.\nArchitecture Accuracy* Precision F1 score\nDenseNet [19] 0.828 0.795 0.782\nInceptionv3 [20] 0.836 0.801 0.805\nMobilenet [21] 0.839 0.78 0.796\nNASNet [22] 0.811 0.757 0.760\nVGG16 [23] 0.789 0.779 0.750\nXception [24] 0.854 0.832 0.825\n*Accuracy and Recall values are same as micro-averaging\nis considered for multiclass confusion matrix\n3. Evaluation\nNITCAD can be considered challenging only if the classes present in it are di\ufb03cult to classify. Thus the dataset is\nprocessed accordingly and the cropped images are fed to various classi\ufb01cation algorithms to obtain confusion matrix.\nTo evaluate the detection algorithms on our dataset, Faster R-CNN is used. The stereo dataset is evaluated on the basisof average of the relative di\ufb00erence between the disparity maps generated (R\ndi f f) by taking the output obtained from\nIntel RealSense as the ground truth.\n3.1. NITCAD object dataset- Evaluation for classi\ufb01cation\nTo evaluate how good the classi\ufb01cation algorithms perform on the dataset, confusion matrix for di\ufb00erent deep\nlearning architectures was computed. Each image is cropped to extract all individual classes and a subset of the\nlabelled data was considered for training, validation and testing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20e94ebe-d4ed-4ad7-a6a9-fe92ef8a1db5": {"__data__": {"id_": "20e94ebe-d4ed-4ad7-a6a9-fe92ef8a1db5", "embedding": null, "metadata": {"page_label": "212", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "220e706a-84d9-4816-8cd8-ac85cc98b1cb", "node_type": "4", "metadata": {"page_label": "212", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "30c0ed50e1bd0eaeffe9b490d3133ceacd02e35312b4a0a80256c0f24a5567e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "285ced6c-d49f-4772-a669-85f038fd62d1", "node_type": "1", "metadata": {"page_label": "212", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "5822d39b1dd2d5b2f74b73eeeee8985ff7d9e54018000bbcf95ef9a2e820cbd1", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation\nNITCAD can be considered challenging only if the classes present in it are di\ufb03cult to classify. Thus the dataset is\nprocessed accordingly and the cropped images are fed to various classi\ufb01cation algorithms to obtain confusion matrix.\nTo evaluate the detection algorithms on our dataset, Faster R-CNN is used. The stereo dataset is evaluated on the basisof average of the relative di\ufb00erence between the disparity maps generated (R\ndi f f) by taking the output obtained from\nIntel RealSense as the ground truth.\n3.1. NITCAD object dataset- Evaluation for classi\ufb01cation\nTo evaluate how good the classi\ufb01cation algorithms perform on the dataset, confusion matrix for di\ufb00erent deep\nlearning architectures was computed. Each image is cropped to extract all individual classes and a subset of the\nlabelled data was considered for training, validation and testing. Pre-trained models on Imagenet were taken and\ninitial layers were frozen as they learn about simple features like edges and lines and these are common in all theobjects. Validation accuracy is used as a metric while training for 20 epochs and the best weights are used for testing.The confusion matrix for various architectures are represented in Fig. 7.\nIt can be inferred that the class van is being confused with car/auto by these architectures. Also, the auto-rickshaw\nwhich is the most common class in Indian roads has been classi\ufb01ed well as the dataset collected has enough numberof autos to train networks. The accuracy and precision values of various classes and models can be seen in Table 2\nand Table 3", "mimetype": "text/plain", "start_char_idx": 655, "end_char_idx": 2233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a300dcbc-ee74-4493-adc2-e2652c7eb1c9": {"__data__": {"id_": "a300dcbc-ee74-4493-adc2-e2652c7eb1c9", "embedding": null, "metadata": {"page_label": "213", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d05d065-454f-4d27-b3f6-7757b378e924", "node_type": "4", "metadata": {"page_label": "213", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7a4042a9c2538f6a2e03033e985218462dc387b0c6006e47edfad63b2d201b74", "class_name": "RelatedNodeInfo"}}, "text": "Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216 213Author name /Procedia Computer Science 00 (2019) 000\u2013000 7\nFig. 7: Confusion matrix for various architectures.\nDenseNet, Inceptionv3, Mobilenet, NASNet, VGG16 and Xception(from left to right)\n0-Auto rickshaw, 1-Bus, 2-Car, 3-Pedestrian, 4-Truck, 5-Two Wheeler, 6-Van", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 356, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc29145a-472c-4582-9056-f85fcbe6b4d1": {"__data__": {"id_": "fc29145a-472c-4582-9056-f85fcbe6b4d1", "embedding": null, "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e187671c-580a-4e56-9b1e-e69b7794150a", "node_type": "4", "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "666f206773c52b36bb164bb6708191e5c12870c3cc34575cf31b76940e9813e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77181f23-c61b-45be-a8b0-d2917134b3c4", "node_type": "1", "metadata": {}, "hash": "7456864a5e82cd446260a0a7c76e218f08d49c37292842623fcdc1963cbf3a59", "class_name": "RelatedNodeInfo"}}, "text": "214 Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216\n8 Author name /Procedia Computer Science 00 (2019) 000\u2013000\nTable 3: Precision, Recall values for di\ufb00erent classes present in NITCAD object dataset.\nArchitecturePrecision Recall\nA B C P Tr Tw V A B C P Tr Tw V\nDenseNet 0.77 1.00 0.82 0.53 0.65 0.97 0.29 0.98 0.03 0.95 0.95 0.4 0.94 0.02\nInceptionv3 0.86 0.94 0.77 0.76 0.4 0.96 0.27 0.99 0.25 0.89 0.94 0.69 0.96 0.07\nMobilenet 0.9 0.48 0.72 0.82 0.39 0.96 0.2 0.99 0.25 0.94 0.97 0.17 0.96 0.02\nNASNet 0.98 0.2 0.79 0.8 1.00 0.76 0.26 0.91 0.04 0.97 0.5 0.32 0.99 0.01\nVGG16 0.66 0.73 0.87 0.51 0.42 0.94 0.4 0.98 0.17 0.9 0.96 0.59 0.85 0.01\nXception 0.88 0.88 0.92 0.84 0.14 0.94 0.21 0.99 0.25 0.94 0.99 0.69 0.99 0.009\nA-auto rickshaw,B-bus,C-car,P-pedestrian,Tr-truck,Tw-two wheeler,V-van\n3.2. NITCAD object dataset - Evaluation for detection\nTo evaluate the detection, Faster R-CNN is chosen. 1200 images are trained for 70 epochs with each epoch having\n200 iterations in 4GB GPU system. Resnet is used as the base architecture to train, extract features and the metrics\nare tabulated in Table 4\nTable 4: Di\ufb00erent metrics obtained after training Faster R-CNN.\nClassi\ufb01er Accuracy 0.894\nLoss RPN Classi\ufb01er 0.057\nLoss RPN Regression 0.0846\nLoss Detector Classi\ufb01er 0.26\nLoss Detector Regression 0.11\n3.3. Evaluation of NITCAD Stereo vision dataset\nIntel Realsense stereo camera generates a depth map of the scene that is being recorded using the built in vision\nprocessor. This was taken as the ground truth of depth. For improving the depth estimation, a neural network basedapproach MC-CNN [16] was applied. For a pair of images corresponding to a scene MC-CNN generates a disparity\nmap which is used to obtain the depth information. Pre-trained network on KITTI was chosen to estimate the disparity\nmaps. Disparity map for that particular scene was also obtained using inbuilt functions provided by OpenCV library.LetD\nIntel(x,y) and Dmethod (x,y) corresponds to the disparity maps generated by Intel Realsense stereo camera and by\ntwo of the above mentioned methods respectively for a particular scene. The average of the relative di\ufb00erence between\nthe disparity maps generated(R di f f) can be obtained as\nRdi f f=1\nw\u00d7h\u2211\nx\u03f5w,y\u03f5 h|DIntel(x,y)\u2212Dmethod (x,y)| (1)\nIf the value of Rdi f fis less, then it can be implied that the disparity map obtained by the method is accurate.\nObtaining the depth information is useful in estimating the velocity of the objects in the scene.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77181f23-c61b-45be-a8b0-d2917134b3c4": {"__data__": {"id_": "77181f23-c61b-45be-a8b0-d2917134b3c4", "embedding": null, "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e187671c-580a-4e56-9b1e-e69b7794150a", "node_type": "4", "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "666f206773c52b36bb164bb6708191e5c12870c3cc34575cf31b76940e9813e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc29145a-472c-4582-9056-f85fcbe6b4d1", "node_type": "1", "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "cd7a924f87c38875cc9eeb27c582d155fc367b9d2d5deae6182627fb049257fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4f045dc-3404-4f0c-ada8-73ede9b0ea7f", "node_type": "1", "metadata": {}, "hash": "a50895d823753f47766282c714d6a352bea72251b89ff2ce32e551efbf81dfd6", "class_name": "RelatedNodeInfo"}}, "text": "Pre-trained network on KITTI was chosen to estimate the disparity\nmaps. Disparity map for that particular scene was also obtained using inbuilt functions provided by OpenCV library.LetD\nIntel(x,y) and Dmethod (x,y) corresponds to the disparity maps generated by Intel Realsense stereo camera and by\ntwo of the above mentioned methods respectively for a particular scene. The average of the relative di\ufb00erence between\nthe disparity maps generated(R di f f) can be obtained as\nRdi f f=1\nw\u00d7h\u2211\nx\u03f5w,y\u03f5 h|DIntel(x,y)\u2212Dmethod (x,y)| (1)\nIf the value of Rdi f fis less, then it can be implied that the disparity map obtained by the method is accurate.\nObtaining the depth information is useful in estimating the velocity of the objects in the scene. The output obtainedfrom Intel Realsense is taken as ground truth and the R\ndi f fobtained with MC-CNN is 14 and with OpenCV is 18\nwhere it is the average of about 100 image pairs.8 Author name /Procedia Computer Science 00 (2019) 000\u2013000\nTable 3: Precision, Recall values for di\ufb00erent classes present in NITCAD object dataset.\nArchitecturePrecision Recall\nA B C P Tr Tw V A B C P Tr Tw V\nDenseNet 0.77 1.00 0.82 0.53 0.65 0.97 0.29 0.98 0.03 0.95 0.95 0.4 0.94 0.02\nInceptionv3 0.86 0.94 0.77 0.76 0.4 0.96 0.27 0.99 0.25 0.89 0.94 0.69 0.96 0.07\nMobilenet 0.9 0.48 0.72 0.82 0.39 0.96 0.2 0.99 0.25 0.94 0.97 0.17 0.96 0.02\nNASNet 0.98 0.2 0.79 0.8 1.00 0.76 0.26 0.91 0.04 0.97 0.5 0.32 0.99 0.01\nVGG16 0.66 0.73 0.87 0.51 0.42 0.94 0.4 0.98 0.17 0.9 0.96 0.59 0.85 0.01\nXception 0.88 0.88 0.92 0.84 0.14 0.94 0.21 0.99 0.25 0.94 0.99 0.69 0.99 0.009\nA-auto rickshaw,B-bus,C-car,P-pedestrian,Tr-truck,Tw-two wheeler,V-van\n3.2. NITCAD object dataset - Evaluation for detection\nTo evaluate the detection, Faster R-CNN is chosen. 1200 images are trained for 70 epochs with each epoch having\n200 iterations in 4GB GPU system. Resnet is used as the base architecture to train, extract features and the metricsare tabulated in Table 4\nTable 4: Di\ufb00erent metrics obtained after training Faster R-CNN.\nClassi\ufb01er Accuracy 0.894\nLoss RPN Classi\ufb01er 0.057\nLoss RPN Regression 0.0846\nLoss Detector Classi\ufb01er 0.26\nLoss Detector Regression 0.11\n3.3. Evaluation of NITCAD Stereo vision dataset\nIntel Realsense stereo camera generates a depth map of the scene that is being recorded using the built in vision\nprocessor. This was taken as the ground truth of depth. For improving the depth estimation, a neural network basedapproach MC-CNN [16] was applied. For a pair of images corresponding to a scene MC-CNN generates a disparity\nmap which is used to obtain the depth information. Pre-trained network on KITTI was chosen to estimate the disparity\nmaps. Disparity map for that particular scene was also obtained using inbuilt functions provided by OpenCV library.", "mimetype": "text/plain", "start_char_idx": 1779, "end_char_idx": 4569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4f045dc-3404-4f0c-ada8-73ede9b0ea7f": {"__data__": {"id_": "e4f045dc-3404-4f0c-ada8-73ede9b0ea7f", "embedding": null, "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e187671c-580a-4e56-9b1e-e69b7794150a", "node_type": "4", "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "666f206773c52b36bb164bb6708191e5c12870c3cc34575cf31b76940e9813e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77181f23-c61b-45be-a8b0-d2917134b3c4", "node_type": "1", "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "5c10e5ba40d1c4074fa69741e2b1cb12f0237907ab48e0a39a2bd98f0e2e01ff", "class_name": "RelatedNodeInfo"}}, "text": "Classi\ufb01er Accuracy 0.894\nLoss RPN Classi\ufb01er 0.057\nLoss RPN Regression 0.0846\nLoss Detector Classi\ufb01er 0.26\nLoss Detector Regression 0.11\n3.3. Evaluation of NITCAD Stereo vision dataset\nIntel Realsense stereo camera generates a depth map of the scene that is being recorded using the built in vision\nprocessor. This was taken as the ground truth of depth. For improving the depth estimation, a neural network basedapproach MC-CNN [16] was applied. For a pair of images corresponding to a scene MC-CNN generates a disparity\nmap which is used to obtain the depth information. Pre-trained network on KITTI was chosen to estimate the disparity\nmaps. Disparity map for that particular scene was also obtained using inbuilt functions provided by OpenCV library.\nLetD\nIntel(x,y) and Dmethod (x,y) corresponds to the disparity maps generated by Intel Realsense stereo camera and by\ntwo of the above mentioned methods respectively for a particular scene. The average of the relative di\ufb00erence between\nthe disparity maps generated(R di f f) can be obtained as\nRdi f f=1\nw\u00d7h\u2211\nx\u03f5w,y\u03f5 h|DIntel(x,y)\u2212Dmethod (x,y)| (1)\nIf the value of Rdi f fis less, then it can be implied that the disparity map obtained by the method is accurate.\nObtaining the depth information is useful in estimating the velocity of the objects in the scene. The output obtained\nfrom Intel Realsense is taken as ground truth and the Rdi f fobtained with MC-CNN is 14 and with OpenCV is 18\nwhere it is the average of about 100 image pairs.", "mimetype": "text/plain", "start_char_idx": 3816, "end_char_idx": 5310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "593a1adf-6011-4407-aad0-e3d92bc8fe54": {"__data__": {"id_": "593a1adf-6011-4407-aad0-e3d92bc8fe54", "embedding": null, "metadata": {"page_label": "215", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d692da94-4d4b-4121-ac84-d45039a6112f", "node_type": "4", "metadata": {"page_label": "215", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "590b490a55acf889e840fd6563cc00cbaef1e83e2cb7beec9b6011fdef281201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "956c4cfb-e930-4627-9ee8-ff41f174e26f", "node_type": "1", "metadata": {}, "hash": "4c502767a00976394e9a784282ef7bca46831f49cc609ac52fcb7303bf743aba", "class_name": "RelatedNodeInfo"}}, "text": "Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216 215\nAuthor name /Procedia Computer Science 00 (2019) 000\u2013000 9\n(a) Intel Realsense (b) MC-CNN (c) OpenCV\nFig. 8: Disparity maps obtained from di\ufb00erent algorithms of the image pair as shown in Fig.6. (Images thresholded for visual analysis)\n4. Conclusion and Future Work\nA challenging dataset is presented which includes various test cases that are frequently present in Indian roads. To\nget the information regarding velocity, a stereo dataset is presented which can be used to develop algorithms to obtain\ndepth information. Various classes are labelled for object classi\ufb01cation and is evaluated with confusion matrix which\nis obtained by di\ufb00erent architectures. For detection, Faster R-CNN is used. The stereo dataset is evaluated by absolute\nsum of error between the output obtained from the Intel camera and with the methods described i.e MC-CNN andOpenCV. Our dataset can be further extended by collecting data which can include new classes like animals, lorry,\nsign boards etc. Research on the development of novel architectures that can detect and classify in various conditions\nincluding many edge cases needs to be carried on.\n5. Acknowledgements\nWe would like to thank TEQIP - III for providing fund to acquire Intel RealSense D435 Depth camera.\nReferences\n[1]Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: Computer Vision and\nPattern Recognition, 2009. CVPR 2009. IEEE Conference on. pp. 248255. IEEE (2009)\n[2]Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:\nEuropean conference on computer vision. pp. 740755. Springer (2014)\n[3]Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International journal\nof computer vision 88(2), 303338(2010)\n[4]Dollar, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: A benchmark. In: Computer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on. pp. 304311. IEEE (2009)\n[5]Zhang, Shanshan, Rodrigo Benenson, and Bernt Schiele. \u201cCitypersons: A diverse dataset for pedestrian detection.\u201d Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. 2017.\n[6]Keller, Christoph Gustav, Markus Enzweiler, and Dariu M. Gavrila. \u201cA new benchmark for stereo-based pedestrian detection.\u201d 2011 IEEE\nIntelligent Vehicles Symposium (IV). IEEE, 2011.\n[7]Krause, Jonathan, et al. \u201c3d object representations for \ufb01ne-grained categorization.\u201d Proceedings of the IEEE International Conference on\nComputer Vision Workshops. 2013.\n[8]Maddern, W., Pascoe, G., Linegar, C., Newman, P.: 1 year, 1000 km: The oxford robotcar dataset. IJ Robotics Res. 36(1), 315 (2017)\n[9]Geiger, Andreas, et al. \u201cVision meets robotics: The KITTI dataset.\u201d The International Journal of Robotics Research 32.11 (2013): 1231-1237.\n[10] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U.,Roth, S., Schiele, B.: The cityscapes dataset for\nsemantic urban scene understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3213 3223\n(2016)\n[11] Neuhold, G., Ollmann, T., Bul, S.R., Kontschieder, P.: The mapillary vistas dataset for semantic understanding of street scenes.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "956c4cfb-e930-4627-9ee8-ff41f174e26f": {"__data__": {"id_": "956c4cfb-e930-4627-9ee8-ff41f174e26f", "embedding": null, "metadata": {"page_label": "215", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d692da94-4d4b-4121-ac84-d45039a6112f", "node_type": "4", "metadata": {"page_label": "215", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "590b490a55acf889e840fd6563cc00cbaef1e83e2cb7beec9b6011fdef281201", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "593a1adf-6011-4407-aad0-e3d92bc8fe54", "node_type": "1", "metadata": {"page_label": "215", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "b8430c48decc4d72326c6e839fa8c4d7427eb3c921fe0822e3652a57db700366", "class_name": "RelatedNodeInfo"}}, "text": "IJ Robotics Res. 36(1), 315 (2017)\n[9]Geiger, Andreas, et al. \u201cVision meets robotics: The KITTI dataset.\u201d The International Journal of Robotics Research 32.11 (2013): 1231-1237.\n[10] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U.,Roth, S., Schiele, B.: The cityscapes dataset for\nsemantic urban scene understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3213 3223\n(2016)\n[11] Neuhold, G., Ollmann, T., Bul, S.R., Kontschieder, P.: The mapillary vistas dataset for semantic understanding of street scenes. In: International\nConference on Computer Vision (ICCV) (2017)\n[12] Huang, Xinyu, et al. \u201cThe apolloscape dataset for autonomous driving.\u201d Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops. 2018.\n[13] Yu, Fisher, et al. \u201cBDD100K: A diverse driving video database with scalable annotation tooling.\u201d arXiv preprint arXiv:1805.04687 (2018).\n[14] Caesar, Holger, et al. \u201cnuScenes: A multimodal dataset for autonomous driving.\u201d arXiv preprint arXiv:1903.11027 (2019).\n[15] Varma, Girish, et al. \u201cIDD: A dataset for exploring problems of autonomous navigation in unconstrained environments.\u201d 2019 IEEE Winter\nConference on Applications of Computer Vision (WACV). IEEE, 2019.", "mimetype": "text/plain", "start_char_idx": 2829, "end_char_idx": 4128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72a09241-c77e-4d88-b2e9-90b3704e8956": {"__data__": {"id_": "72a09241-c77e-4d88-b2e9-90b3704e8956", "embedding": null, "metadata": {"page_label": "216", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6421fc3d-f80f-409d-a277-10b22f986828", "node_type": "4", "metadata": {"page_label": "216", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "4b46fbecb9e6d6ae3468c608d825444d59d3b1a630661f9ff95354f7e3923cc8", "class_name": "RelatedNodeInfo"}}, "text": "216 Namburi GNVV Satya Sai Srinath  et al. / Procedia Computer Science 171 (2020) 207\u2013216\n10 Author name /Procedia Computer Science 00 (2019) 000\u2013000\n[16] J. Zbontar, Y . LeCunet al., \u201cStereo matching by training a convolutional neuralnetwork to compare image patches.Journal of Machine Learning\nResearch,vol. 17, no. 1-32, p. 2, 2016.\n[17] E. Rosten and T. Drummond, \u201cMachine learning for high-speed cornerdetection, in European Conference on Computer Vision, vol. 1, May\n2006,pp. 430443. [Online]. Available: http://www.edwardrosten.com/work/rosten2006machine.pdf\n[18] Z. Zhang, \u201cA \ufb02exible new technique for camera calibration,IEEE Transactions onpattern analysis and machine intelligence, vol. 22, 2000\n[19] Huang, Gao, et al. \u201cDensely connected convolutional networks.\u201d Proceedings of the IEEE conference on computer vision and pattern recogni-\ntion. 2017.\n[20] Szegedy, Christian, et al. \u201cRethinking the inception architecture for computer vision.\u201d Proceedings of the IEEE conference on computer vision\nand pattern recognition. 2016.\n[21] Howard, Andrew G., et al. \u201cMobilenets: E\ufb03cient convolutional neural networks for mobile vision applications.\u201d arXiv preprint\narXiv:1704.04861 (2017).\n[22] Zoph, Barret, et al. \u201cLearning transferable architectures for scalable image recognition.\u201d Proceedings of the IEEE conference on computer\nvision and pattern recognition. 2018.\n[23] Simonyan, Karen, and Andrew Zisserman. \u201cVery deep convolutional networks for large-scale image recognition.\u201d arXiv preprint\narXiv:1409.1556 (2014).\n[24] Chollet, Franois. \u201cXception: Deep learning with depthwise separable convolutions.\u201d Proceedings of the IEEE conference on computer vision\nand pattern recognition. 2017.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26f0fb54-6415-483a-ab09-d16aa17974e1": {"__data__": {"id_": "26f0fb54-6415-483a-ab09-d16aa17974e1", "embedding": null, "metadata": {"page_label": "1", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b831842-611e-42c8-8355-073bf1a175ee", "node_type": "4", "metadata": {"page_label": "1", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "b3d27f975dc8d56305d23f8cdc18049e7043b0d171ba43ed61672b456f38d5db", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated\nAssessment of Actions\nAbrar Majeedi , Viswanatha Reddy Gajjala , Satya Sai Srinath Namburi\nGNVV, and Yin Li\nUniversity of Wisconsin-Madison, Madison Wisconsin 53706 USA\n{majeedi,vgajjala,sgnamburi,yin.li}@wisc.edu\nAbstract. The ability to quantify how well an action is carried out, also\nknown as action quality assessment (AQA), has attracted recent interest\nin the vision community. Unfortunately, prior methods often ignore the\nscore rubric used by human experts and fall short of quantifying the\nuncertainty of the model prediction. To bridge the gap, we present RICA2\n\u2014 a deep probabilistic model that integrates score rubric and accounts for\nprediction uncertainty for AQA. Central to our method lies in stochastic\nembeddings of action steps, defined on a graph structure that encodes the\nscore rubric. The embeddings spread probabilistic density in the latent\nspace and allow our method to represent model uncertainty. The graph\nencodes the scoring criteria, based on which the quality scores can be\ndecoded. We demonstrate that our method establishes new state of the art\non public benchmarks, including FineDiving, MTL-AQA, and JIGSAWS,\nwith superior performance in score prediction anduncertainty calibration .\nOur code is available at https://abrarmajeedi.github.io/rica2_aqa/ .\nKeywords: Action Quality Assessment \u00b7Video Understanding\n1 Introduction\nAction quality assessment (AQA), aiming at quantifying how well an action is\ncarried out, has been widely studied across scientific disciplines due to its broad\nrange of applications. AQA is key to sports science and analytics. The right way\nof performing actions maximizes an athlete\u2019s performance and minimizes injury\nrisk. AQA is crucial to occupational safety and health. High-quality actions\nmitigate the physical stress and strain in the workspace. AQA is pivotal for\nphysical therapies. The quality of actions reveals the progress in rehabilitation.\nAQA also plays a major role in surgical education. Proficient actions improve\nthe outcome and reduce complications.\nObservational methods for AQA have been well established for various tasks,\ne.g., gymnastics [34], manual material handling [49], and surgery [25]. These\nmethods involve a human expert observing an action and decomposing it into a\nseries of key steps. Each of these steps, or a subset of them, can be grouped into a\nfactor and then evaluated using a Likert scale [21] following a pre-defined criterion.\nRatings for individual factors, sometimes complemented with impression-basedarXiv:2408.02138v2  [cs.CV]  6 Aug 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0dbd966-58b2-4926-aeba-14a7434f0f78": {"__data__": {"id_": "e0dbd966-58b2-4926-aeba-14a7434f0f78", "embedding": null, "metadata": {"page_label": "2", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44e62ba5-bdd5-44bd-8357-77fd59d5f236", "node_type": "4", "metadata": {"page_label": "2", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "e79fddb35cdc811d3b7f45730d13b91041ab4c021f2f734b3efb910db3a49e3d", "class_name": "RelatedNodeInfo"}}, "text": "2 A. Majeedi et al.\nuncertain actionsPrior AQA \nMethods\nprediction error: high\nconfidence: highVideo\nuncertain actions\nRICA2\n(Ours)prediction error: medium\nconfidence:  lowVideo Rubric\nfwd.\npike\ntwist\nentrydivetakeoff\nflight\nfinish\nFig. 1:RICA2integrates score rubric used by human experts and accounts for prediction\nuncertainty, resulting in accurate predictions and calibrated uncertainty estimates.\nglobal ratings, are then summarized into a final quality score [25]. Multiple expert\nratings are often considered to account for the variance in the scores. While\nthese methods are commonly adopted, they require significant input from human\nexperts and are thus costly and inefficient.\nThere is a burgeoning interest in the vision community to develop video-based\nAQA [32,33,42,51]. While current solutions have made steady progress across\nbenchmarks [11,31,52], their decision-making processes differ largely from prior\nobservational methods. Almost all prior solutions learn deep models to directly\nmap input videos to scores. Many of them employ an exemplar-based approach,\nin which a model predicts relative scores by referencing exemplar videos with\nsimilar actions and known scores [2,52,56]. Few of them have considered the\nstructure of the actions or their scoring criteria used by observational methods.\nFurther, existing AQA methods face a key challenge in the accurate quan-\ntification of model uncertainty i.e., the uncertainty of model prediction that is\ncalibrate to the expected error [13]. Knowing this uncertainty is particularly\nhelpful for AQA, e.g., when assessing the quality of high-stakes competitions or\nsurgical procedures. With proper calibration, videos that have uncertain predic-\ntions can be passed to human experts for a thorough evaluation. Several recent\nworks have started to consider the variance among scores from multiple human\nexperts [42,57,59,60]. Unfortunately, they still fall short of considering prediction\nuncertainty, leaving this challenge largely unaddressed.\nTo bridge the gap, we develop a deep probabilistic model for AQA by inte-\ngrating score rubrics and modeling the uncertainty of the prediction (see Fig. 1).\nCentral to our method lies in the stochastic embedding of action steps, defined\non a graph structure that encodes the score rubric. The embeddings spread\nprobabilistic density in latent space and allow our method to represent model\nuncertainty. The graph encodes the scoring criteria, based on which the quality\nscores can be decoded. We also present a training scheme and describe an ap-\nproach to estimate uncertainty. Putting things together, our method, dubbed\nRICA2(Rubric-informed, Calibrated Assessment of Actions), yields accurate\naction scores with additional uncertainty estimates.\nWe evaluate RICA2on several public AQA datasets, covering sports and\nsurgical videos. Particularly, RICA2establishes new state of the art on FineDiv-\ning [52], MTL-AQA [31] and JIGSAWS [11]. On FineDiving [52] \u2013 the largest and\nmost challenging AQA benchmark, RICA2outperforms latest methods in predic-\ntion accuracy (a boost of +0.94% in Spearman\u2019s Rank Correlation Coefficient", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93c66004-e75f-451d-b72e-abb3ee4c6b21": {"__data__": {"id_": "93c66004-e75f-451d-b72e-abb3ee4c6b21", "embedding": null, "metadata": {"page_label": "3", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae68b3c8-cf0f-410a-9f04-59e911ddec88", "node_type": "4", "metadata": {"page_label": "3", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "33ae0e1e68a6fc2f6b7229adac54a4367c85df71cf6e623d9b13d96dfe5ca362", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 3\n(SRCC)) and demonstrates significantly improved uncertainty calibration (a gain\nof+0.178in Kendall Tau [17]). Similarly, on MTL-AQA [31], the most commonly\nused dataset for AQA, RICA2attains state-of-the-art SRCC, and again largely\nimproved calibration (a gain of +0.444in Kendall Tau). On JIGSAWS [11],\nRICA2beats the previous best results by a relative margin of +3.37%inSRCC.\nFurther, we present extensive experiments to evaluate the key design of RICA2.\nOur main contributions are summarized into three folds.\n\u2022We propose RICA2, a novel deep probabilistic method that incorporates\nscoring rubrics and uncertainty modeling for AQA, resulting in accurate\nscores and calibrated uncertainty estimates.\n\u2022Ourtechnicalinnovationsliein(a)agraphneuralnetworktomodelthescoring\nrubric in conjunction with stochastic embeddings on the graph to account\nfor prediction uncertainty and (b) a training scheme under the variational\ninformation bottleneck framework.\n\u2022Our extensive set of experiments demonstrates that RICA2achieves state-\nof-the-art results in AQA, significantly outperforming prior methods in both\nprediction accuracy and calibration of uncertainty estimates.\n2 Related Work\nAction quality assessment (AQA). Early works in AQA [12,33] employed\nhandcrafted features to estimate quality scores in videos. More recent methods\ndeveloped various deep models, including convolutional [42,56], graph [30], re-\ncurrent [32,51], and Transformer [2,52] networks. AQA has also been widely\nconsidered in surgical education [22], rehabilitation [35], and ergonomics [5].\nRecently, exemplar-based methods [2,52,56] have emerged as a promising\nsolution for AQA due to their impressive performance across benchmarks. These\nmethods predict the relative score of an input video by comparing it to selected\nexemplar videos with similar action steps and known scores. A limitation of this\nparadigm is the requirement of exemplar videos at inference time. This strategy\nlargely deviates from existing observational methods used by human experts and\nleads to significantly higher computational costs. While RICA2also uses action\nsteps in the input video, it further integrates the scoring rubric of these steps\nand offers a solution for no-reference AQA i.e.without using exemplars .\nSeveral recent works have started to consider the modeling of score uncertainty\nin AQA [42,57,59,60]. For example, Tang et al. [42] proposed to model the\nfinal scores using a Gaussian distribution. They presented a model (MUSDL)\ntrained to predict the score distribution. This distribution learning idea was\nfurther extended in [57,59,60]. However, modeling the score distribution does not\nwarrant the quantification of model uncertainty, as the output distributions might\nnot be calibrated with prediction errors. While RICA2also predicts a Gaussian\ndistribution for the scores, our key design is to consider stochastic embeddings\nto quantify prediction uncertainty, resulting in calibrated uncertainty estimates .\nThe most relevant work is IRIS [26]. IRIS incorporates score rubric into a\nconvolutional network for AQA. This is done by segmenting key steps in the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "815d8f04-4092-40d3-8816-b0f1701a1efc": {"__data__": {"id_": "815d8f04-4092-40d3-8816-b0f1701a1efc", "embedding": null, "metadata": {"page_label": "4", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ab73041-59b6-4e04-9dd5-fa84ffc8d6c2", "node_type": "4", "metadata": {"page_label": "4", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "58f60dce160f0d8a5f3e6a647acf0989549c3543fda488e973e6a9a59d3aea65", "class_name": "RelatedNodeInfo"}}, "text": "4 A. Majeedi et al.\nvideo and predicting sub-scores for individual steps. Similar to IRIS, RICA2\nalso considers rubric in a deep model. However, RICA2adapts a graph network,\ntreats sub-scores as latent embeddings, predicts the final score, and further\nquantifies prediction uncertainty. These differences allow RICA2to be trained on\nmajor public datasets with only final scores, and to output calibrated uncertainty\nestimates, both of which cannot be achieved by IRIS.\nModeling uncertainty with stochastic embedding. Stochastic embedding,\ninitially introduced in NLP [27,45], treats each embedding as a distribution. This\napproach has gained recent attention for modeling uncertainty in deep models. Oh\net al. [28] considered probabilistic embeddings for metric learning and proposed to\nmodel uncertainty based on the stochasticity of embeddings. This idea was further\nadopted in many vision tasks, including face verification [39], age estimation [20],\npose estimation [41], and cross-modal retrieval [6]. Another related line of work is\nthe conditional variational autoencoder [40], where a probabilistic representation\nof the input is used for a prediction task. Our approach shares a similar idea of\nusing stochastic embeddings to model uncertainty yet is specifically designed for\nAQA. Our method significantly extends prior idea to embed action steps on a\ngraph structure, and to propagate these stochastic embeddings on the graph.\nGraph neural networks (GNNs). GNNs [9,19,37] offer a powerful tool to\nleverage the relational inductive bias inherent in data [3,53]. This inductive bias\nis beneficial to aggregate a global representation from a group of local ones [36].\nRecently, Zhou et al. [60] proposed a hierarchical graph convolutional network for\nAQA, in which a GNN was used for video representation learning. In contrast,\nwe adapt graph networks to model score rubrics used by observational methods.\n3 AQA with Score Rubric and Uncertainty Modeling\nOur goal is to assess the quality of an action within an input video. Let Xbe the\nvideo with the action and Yas its quality score. Our method further considers\nthe structure of the action and a scoring rubric based on the structure.\nAction steps . We assume that the action in Xcomprises a known, ordered set\nof key steps, denoted as S= (s1, s2, ..., s k). Each s1represents a necessary sub-\naction for successfully executing the action. Further, sis associated with a text\ndescription that elucidates the specifics of the corresponding step, e.g., \u201ca front-\nfacing takeoff\u201d for diving. This assumption is especially well suited for structured\nactions, such as diving or surgery, where the key steps are predetermined and\nfollow a specific sequence. Note that the timing of the key steps is not presumed.\nEven if key steps are unavailable, they can be detected using action recognition\nmethods [4,10] (see supplement Sec. C.4).\nScoringrubric .Wefurtherassumeapre-specifiedscoringrubricbasedonthekey\nsteps \u2014 a common strategy in technical skill assessment [25,34,49]. Specifically,\neach action step skis independently scored, i.e.,sk7\u2192yk. Subsequently, a rule-\nbased rubric is employed to aggregate individual scores {yk}and calculate a final\n1For the sake of brevity, we omit the subscript as long as there is no confusion.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "443de2d6-33f7-4be0-91f0-75bcbeb73812": {"__data__": {"id_": "443de2d6-33f7-4be0-91f0-75bcbeb73812", "embedding": null, "metadata": {"page_label": "5", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "758e8005-2195-4b9c-a3d4-9c170af3e57f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "0113f8c6a1bcf2e7c2cb2b3219df9191adf3333afb1891af7c63a206ac2daef5", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 5\nClip 1\nClip N\u2026\nVideo Feature Extraction\nI3DStep\nDescription\nText Feature Extraction\u2026\n1D-ConvMLP\nSelf AttentionCross Attention\n (xN)\nI3D Features\u20269.0\n2.02.03.02.0\n1 TwistForward\n2.5 Soms.\nPike\nEntry\nDifficulty\nDegree3.531.529.5\n29.8\u2211\n23.522.5Multiple \nJudge Scoresf(   ,                )\nStep\nDescriptions [    ,   ,    ,    ]\nStochastic\nEmbeddings\nPropagation of \nEmbeddings(a) AQA Scoring Rubric (b) GNN on DAG (c) Embedding Function f\nStochastic \nEmbeddings \nStep\nScoresLLMText Embeddings\nScore \nDistributionSampling\nFig. 2: Overview of RICA2.Leveraging scoring rubrics (a), RICA2integrates a graph\nrepresentation of action step and rubric with uncertainty modeling (b). Specifically,\nRICA2takes an input of the video and its key action steps, encodes the input into\nembeddings (c), refines the embeddings through a deep probabilistic model, and outputs\nan action score in tandem with its uncertainty estimate.\nqualityscore Y,i.e.,{yk} 7\u2192Y,inwhichstepsmightbegroupedintointermediate\nstages (see an example in Fig. 2 (a-b)). This rubric follows a deterministic yet\noften non-injective mapping, e.g. a many-to-one mapping such as summation.\nMethod overview . We now present RICA2\u2014 a deep probabilistic model for\nAQA that leverages known action steps and incorporates the scoring rubric for\nmodeling. Importantly, RICA2accounts for prediction uncertainty, i.e., when\nthe model prediction canandcannotbe trusted. Fig. 2 presents an overview of\nRICA2. It consists of two main model components: (a) a graph neural network\nthat integrates the key steps and scoring rubric (Sec. 3.1); and (b) stochastic\nembeddings defined on the graph to capture prediction uncertainty (Sec. 3.2),\ncoupled with (c) a learning scheme under the variational information bottleneck\nframework (Sec. 3.3). In what follows, we delve into the details of RICA2.\n3.1 Integrating Actions Steps and Scoring Rubric with Graph\nSteps and rubric as graph . We encode action steps and the corresponding\nscoring rubric with a directed acyclic graph (DAG). This DAG is denoted as\nG= (V,E)withVas the set of nodes and Eas the set of directed edges. V\nconsists of three types of nodes: (1) the leaf nodes, denoted as Vs, correspond\nto individual action steps performed in the input video X; (2) the intermediate\nnodes capturing possible intermediate stages in the scoring criteria; and (3)\na designated root node Vrrepresenting the final score of the action. Further,\nthe edges Eindicate the scoring rubric, connecting steps (leaf nodes) to stages\n(intermediate nodes), and stages (intermediate nodes) to the final score (root\nnode). We note that Gvaries for every input video X(assuming a single action),\nas different steps might be performed. Fig. 2 (a-b) show the example in diving\nwhere the key steps and scoring rubric are encoded using our DAG. Additional\nexamples can be found in our supplement Fig. B.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0067458d-4541-49bf-8b96-c63f5536a1bd": {"__data__": {"id_": "0067458d-4541-49bf-8b96-c63f5536a1bd", "embedding": null, "metadata": {"page_label": "6", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72f17178-e61b-4d5c-aa01-b91594ce398d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "1cba2876f9b1418df30b5928fdff3bbb35a2365c3bc6c497b8afde4462d5076e", "class_name": "RelatedNodeInfo"}}, "text": "6 A. Majeedi et al.\nLearning for quality assessment. Our approach involves a two-step process\nfor quality assessment. First, we employ an embedding function f, designed to\nmap individual steps into a latent space representing action quality. Secondly, we\nleverage the key step embeddings {Zs}along with the score rubric encoded in G\nto learn a scoring function h. These functions fandhare defined as follows:\nf:X,G 7\u2192 { Zs};h:{Zs},G 7\u2192 Y, (1)\nwhere {Zs}are the embeddings for the set of steps SinX, corresponding to the\nleaf nodes {Vs}on the DAG.\n3.2 Modeling Score Uncertainty with Stochastic Embeddings\nTo model the prediction uncertainty, we adopt stochastic step embeddings defined\non the leaf nodes, such that Zs\u2208RD\u223cp(Zs|X,{Vs}). Unlike deterministic\nembeddings, where Zswould be a fixed vector, stochastic embedding charac-\nterizes the distribution of Zs, allowing for uncertainty control. Specifically, we\nmodel p(Zs|X, Vs)as a Gaussian distribution in RDwith mean \u00b5sand diagonal\ncovariance \u03a3s. The embedding function fis thus tasked to predict the mean and\ncovariance for the key steps S,i.e.,{\u00b5s, \u03a3s}=f(X,{Vs}).\nPropagating stochastic embeddings on the graph. Our scoring function h\ntakes the stochastic embeddings Zsfor leaf nodes in G(provided by f), further\ncomputes the embeddings for all nodes in G, and finally decodes a quality score\nYfrom the embedding Zrof the root node Vr. To this end, we propose an\nextension of graph neural networks (GNNs), in which stochastic embeddings\nZsare propagated from leaf nodes Vsto the root node Vrbased on the graph\nstructured informed by the scoring rubric of a particular task. Key to this\nGNN lies in a lightweight MLP that operates on each node, taking as input the\nembeddings of its direct predecessors, and generating a new embedding that is\nfurther propagated to its successors. This scoring function his thus given by\nZs\u223c N (\u00b5s, \u03a3s),\u2200s\u2208S| {z }\nSampling from leaf nodes;Z\u00acs=G\u0010\n\u03a3Vj\u2208P(V\u00acs)Zj\u0011\n| {z }\nPropagating on the DAG; \u02c6Y=S(Zr)|{z}\nDeocoding the score(2)\nwhere V\u00acsdenotes a non-leaf node with its embedding Z\u00acs.P(V\u00acs)is the set\nof predecessors of V\u00acs,G(\u00b7)is the MLP aggregating features from predecessors,\nandS(\u00b7)is another MLP decoding the final score \u02c6Yfrom the root node Vr.\nIt is important to note that each leaf embedding Zsis stochastic, characterized\nby a Gaussian distribution ( p(Zs|X,G) =N(\u00b5s, \u03a3s)), with parameters predicted\nbyf. The non-leaf embeddings are however deterministic given samples from leaf\ndistributions. This design is motivated by our assumption of the scoring rubric,\nwhere uncertainty lies only in assessing action steps and identical individual\nscores will yield the same final score.\n3.3 Learning with Variational Information Bottleneck\nWith stochastic embeddings, training of RICA2is a challenge. We design a\ntraining scheme under the variational information bottleneck framework.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c044fcdf-0ec4-415c-b84f-71ee16c7b1fd": {"__data__": {"id_": "c044fcdf-0ec4-415c-b84f-71ee16c7b1fd", "embedding": null, "metadata": {"page_label": "7", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cc2e4ec-af32-4ad5-9c8d-80611b5a6198", "node_type": "4", "metadata": {"page_label": "7", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "a0109e2a258b4241233cee727132451f926b55e21d3d0bc91b1547337710e6ce", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 7\nVariational information bottleneck (VIB). To train our model p(Y|X,G)\nwith stochastic step embeddings {Zs}, we adopt the information bottleneck\nprinciple [43], leading to the maximization of the following objective\nI({Zs};Y|G)\u2212\u03b2I({Zs};X|G), (3)\nwhere Iis the conditional mutual information, and \u03b2 >0controls the tradeoff\nbetween the sufficiency of using step embeddings {Zs}for predicting YgivenG,\nand the size of the embeddings {Zs}derived from XandG.\nWhile mutual information is computationally intractable for high dimensional\n{Zs}, a common solution [1] is to assume Markov property ( p(Z|X, Y,G) =\np(Z|X,G)) and conditional independence ( p({Zs}|X,G) =Q\nsp(Zs|X,G)), fol-\nlowed by the variational approximation for a tractable lower bound\n\u2212LVIB=EZs\u223cp(Zs|X,G),\u2200s\u2208S[logp(Y|{Zs},G)]\u2212\u03b2\u03a3s\u2208SKL(p(Zs|X,G)||p(Zs|G)),(4)\nwhere p(Y|{Zs},G)is modeled by the scoring function h,KLdenotes the Kull-\nback\u2013Leibler divergence, and p(Zs|G)is an approximate marginal prior.\nVIB loss. The first term in Eq. (4) defines the log-likelihood of the score given\nthe input. By assuming that output scores follow a Gaussian with a fixed variance\n\u03c3, this term can be reduced to a mean squared error (MSE) loss\nLMSE =1\nN\u03a3N\ni(\u02c6Yi\u2212Yi)2/\u03c32, (5)\nwhere Yiis the predicted score for a video indexed by i,\u02c6Yiis the corresponding\nground-truth score, and Nis the total number of videos in the training set.\nThe second term in Eq. (4) regularizes the latent space and encodes prediction\nuncertainty. By assuming a marginal prior of N(0, I)forp(Zs|G), we have\nLKL=\u03a3s\u2208SKL(N(\u00b5s(x), \u03a3s(x)||N(0, I))\n=1\n2\u03a3s\u2208S\u03a3D\nj=1\u0000\n(\u00b5s\nj)2+ (\u03c3s\nj)2\u2212log(\u03c3s\nj)2\u22121\u0001\n,(6)\nwhere \u00b5s\njand\u03c3s\nj, respectively, are the j-th dimension of the mean ( \u00b5s(x)) and\nvariance (square root of the diagonal of \u03a3s(x)), for the step s.\nThe VIB loss (LV IB)is thus given by\nLV IB=LMSE +\u03b2LKL. (7)\nLV IBconsists of (a) the MSE loss LMSEfrom the negative log-likelihood of\nthe predicted scores, aiming at minimizing prediction errors; and (b) the KL\ndivergence LKLbetween the predicted Gaussian and the prior, regularizing the\nstochastic embeddings. Further, the coefficient \u03b2balances between two loss terms.\nDuring training, samples are drawn to compute the loss function. The output\nis matched to the Gaussian distribution with its mean equal to the average of\nthe judge scores. The reparameterization trick [18] is used to allow the backprop-\nagation of gradients through the sampling process.\nEstimating uncertainty. The diagonal covariance \u03a3s(X)models the uncer-\ntainty of the predicted quality score of a step sfor an input video X. A larger", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "367c5a8e-a6ce-407c-b659-046e2bf4bf40": {"__data__": {"id_": "367c5a8e-a6ce-407c-b659-046e2bf4bf40", "embedding": null, "metadata": {"page_label": "8", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03af814f-d217-47bc-85b1-be50e3c05c26", "node_type": "4", "metadata": {"page_label": "8", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7535c4b707e5f20427f20c1adc05683e4b1240b6363b7e269b5bdc336f1abe07", "class_name": "RelatedNodeInfo"}}, "text": "8 A. Majeedi et al.\nvalue in its diagonal represents a wider distribution of scores and, hence, a lower\nconfidence in the prediction. Following [20,28] we generate uncertainty scores by\nsumming up the harmonic means of the predicted variances for individual steps\nuncertainty (Y) =\u03a3s\u2208SD/\u03a3D\nj=1(\u03c3s\nj)\u22121, (8)\nwhere Dis the dimensionality of the stochastic embeddings. Again, \u03c3s\njis the j-th\ndimension of the predicted variance.\nStochastic vs. deterministic modeling . An interesting variant of RICA2is\nto disable its stochastic component. Conceptually, this is equal to considering\nstep embeddings Zras vectors and removing the KL loss LKL. We refer to this\ndeterministic version of our model as RICA2\u2020. Without stochastic embeddings,\nRICA2\u2020is unable to estimate prediction uncertainty, yet often yields slightly\nlower prediction errors. This trade-off is also observed in prior works [6,20,39].\nWe include this variant of our model in the experiments.\n3.4 Model Instantiation and Implementation\nVideo and step representation. For an input video X, we adapt a pre-trained\nvideo backbone( e.g., I3D [4]) to extract its clip-level features, which are further\npooled to produce video features (x1, x2, ..., x T)with fixed-length T. To represent\naction steps, we make use of a pre-trained language model [8] (Flan-T5) to extract\ntext features from their step descriptions, resulting in an ordered set of text\nembeddings (s1, s2, ..., s K)forKsteps. Note that the language model is not part\nof RICA2. It is used solely to extract embeddings for text descriptions of the\naction steps (see supplement Tables I-L).\nEmbedding function f.Our embedding function fis realized using a Trans-\nformer model [44] (see Fig. 2(c)). ffirst processes video features (x1, x2, ..., x T)\nwith a self-attention block and text embeddings (s1, s2, ..., s K)using a MLP. It\nfurther makes use of cross-attention blocks (2x) to fuse video and text features,\nwhere video features are used to compute keys and values, and text embeddings\nof steps are projected into queries. Further, fdecodes stochastic embeddings of\nindividual steps by predicting a mean vector \u00b5s\u2208RDand a diagonal covariance\nvector \u03a3s\u2208RDfor each step s.\nScoring function h.With Gaussian distributions for all steps specified by\n{\u00b5s, \u03a3s}, we encode the steps and score rubric into a video-specific DAG G, and\nrealize has a GNN defined on Gfollowing Eq. (2). his parameterized by its\naggregation function G, which is shared among nodes of the same type. Gis\nimplemented using an averaging operation followed by a MLP (2 layers). Finally,\nhdecodes the final score at the root note VrofG.\nTraining with auxiliary losses. While the VIB loss (Eq. (7)) is sufficient\nfor training, it falls short of considering the temporal ordering of steps. This is\nbecause of the conditional independence assumption needed for the derivation of\nVIB, i.e.,p({Zs}|X,G) =Q\nsp(Zs|X,G), where the ordering of {Zs}is discarded.\nTo bridge the gap, we incorporate an auxiliary loss term LAuxinspired by [2].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab2579d4-3745-43fb-b017-c89068d358c9": {"__data__": {"id_": "ab2579d4-3745-43fb-b017-c89068d358c9", "embedding": null, "metadata": {"page_label": "9", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99d081a5-6478-42cd-b7bc-2659021026cd", "node_type": "4", "metadata": {"page_label": "9", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "f405cd1783c51a6f4273ec57ad7a66ccd81a4a4911523322e4ce3cfd4cd4d74c", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 9\nSpecifically, we re-purpose the last cross-attention map ( RK\u00d7T) from fas astep\ndetector. This is done by computing a temporally-weighted center across the\nattention of each action step to every video time step ( i.e., column-wise). We\nthen enforce that (a) this center is co-located with the peak of the attention\nalong video time steps using a sparsity loss [2]; and (b) all centers follow the\ntemporal ordering of corresponding action steps using a ranking loss [2]. These\ntwo terms are summed up as the auxiliary loss, and further added to the VIB\nloss with a small weight (0.1). In our ablation, we empirically verify that adding\nthe auxiliary loss leads to a minor performance boost.\nInference with sampling. At the inference time, we enhance robustness by\nsampling 20 times and averaging their predictions to compute the final score.\n4 Experiments and Results\nDatasets. Our evaluations are primarily reported on three publicly available\nbenchmark datasets, namely FineDiving [52], MTL-AQA [31], and JIGSAWS [11]\nin the main paper. In supplement Sec. B, we also include results on the Cataract-\n101 [38] with cataract surgery videos.\nEvaluation metrics. For all our experiments, we consider metrics on both the\naccuracy of the prediction and the calibration of the uncertainty estimates.\n\u2022Foraccuracy , we use two widely adopted metrics for AQA [2,42,52], namely\nSpearman\u2019s rank correlation ( SRCC) and relative L2 distance ( R\u21132).SRCC\nmeasures how well the predicted scores are ranked w.r.t. the ground truth, while\nR\u21132summarizes the prediction errors. A model with more accurate predictions\nwill have higher SRCCand lower R\u21132.\n\u2022Forcalibration , we report the uncertainty versus error curve following [20,28].\nTo plot this curve, test samples are sorted by increasing uncertainty and divided\ninto 10 equal-sized bins. The mean absolute error (MAE) is then computed for\nitems in each bin. We also follow [20,28] in employing Kendall\u2019s tau ( \u03c4) [17], a\nnumerical measure ranging from -1 to 1 to quantify the correlation between the\nuncertainties and the prediction errors. A higher \u03c4indicates better calibration,\nsignifying that a model\u2019s uncertainty better aligns with prediction errors.\nBaselines. RICA2is benchmarked against a set of strong baselines, including\nexemplar-free methods such as DAE [57], USDL and MUSDL [42], and exemplar-\nbased ones such as CoRE [56], TPT [2] and TSA [52]. We further include\nthe deterministic version of our model RICA2\u2020, which trades the ability of\nuncertainty estimation for a minor boost in accuracy. Several baselines adopt a\ndirect regression approach, without providing a confidence or uncertainty measure\nfor predictions. USDL [42] and TPT [2] implicitly offer a confidence value. In these\nworks, the probability of the predicted score bin serves as a proxy for uncertainty,\ncomputed as ( 1.0\u2212confidence). DAE [57] outputs the standard deviation of the\nscore distribution, which represents uncertainty.\nWe seek to ensure a fair comparison yet recognize that methods in our\nbenchmark may consider different settings and/or various types of input. Most", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f743df6b-2bbc-46e5-ba0f-5b591f166afd": {"__data__": {"id_": "f743df6b-2bbc-46e5-ba0f-5b591f166afd", "embedding": null, "metadata": {"page_label": "10", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2833c901-7347-47c2-9990-cbc9b494c379", "node_type": "4", "metadata": {"page_label": "10", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "63a1a91a8c228379de7651dfb9e4e28e14c94e6109323386de497e30fbafe8ac", "class_name": "RelatedNodeInfo"}}, "text": "10 A. Majeedi et al.\nprior exemplar-free methods only consider a video as input. While RICA2does\nnot utilize exemplars, it takes additional input of step information, i.e., step\npresence and their temporal ordering. On the other hand, previous exemplar-\nbased methods also require the step information as used by RICA2, in addition\nto an input video and an exemplar database. Notably, step information is used\nto select exemplars, leading to improved results. For example, for diving videos,\nCoRE [56], TPT [2] and TSA [52] use the diving number (DN) encoding steps\nand their ordering. Further, TSA [52] also requires the timing of individual steps\nduring training. While it is infeasible to standardize the settings of all methods,\nwe compare to the best reported results in our experiments.\n4.1 Results on FineDiving\nDataset. FineDiving [52] is the largest public dataset for AQA, with 3000\nvideo samples capturing various diving actions. The dataset covers 52 different\naction types, 29 sub-action types, and 23 difficulty degree types, providing a\nrich and diverse set of examples for AQA. While this dataset contains temporal\nannotations for the steps, which can be used to improve the performance of AQA\nas demonstrated in [52], we do not use these annotations for RICA2.\nExperiment setup. We adhere to the experimental setup of the most recent\nbaseline [52] using their train-test split, with 2251 videos for training and 749\nvideos for testing. We follow the input video settings used in [52] for RICA2and\nthe baselines. Specifically, for each video, we uniformly sample 96frames, which\nare segmented into 9overlapping clips, each containing 16consecutive frames.\nWe refer to supplement Sec. A.1 for further implementation details.\nResults. Tab. 1a presents our results on FineDiving. Both our stochastic and\ndeterministic versions (RICA2and RICA2\u2020) outperform the state-of-the-art\nTPT [2], an exemplar-based method. RICA2shows a relative margin of 0.7%\n/ 1.4% on SRCC/R\u21132, and RICA2\u2020has a relative margin of 0.9% / 9.6% on\nSRCC/R\u21132. This improvement is more pronounced when compared with the\nexemplar-free methods (MUSDL, DAE-MT) showcasing a significant relative\ngain of 4.9%, 1.5% on SRCCand 29.8%, 21.6% on R\u21132. While the deterministic\nRICA2\u2020has slightly higher accuracy, our stochastic RICA2demonstrates superior\ncalibration of its uncertainty estimate ( \u03c4RICA2= 0.64vs.\u03c4TPT=\u22120.56). Fig. 3a\nfurther shows uncertainty calibration results. Uncertainty estimates from RICA2\nhaveaclearupwardtrend,indicatingahighercalibrationlevel.WhileMUSDL[42]\nalso exhibits a reasonable level of calibration ( \u03c4MUSDL = 0.47vs.\u03c4RICA2= 0.64),\nthe errors are significantly higher than RICA2across all uncertainty levels.\n4.2 Results on MTL-AQA\nDataset. MTL-AQA [31] is one of the most commonly used datasets for AQA. It\nconsists of 1412 samples collected from 16 events with diverse views. The dataset\nhas a rich set of annotations, including the steps performed during the dive, the\ndifficulty score associated with the dive, and the individual judge scores.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24492c3a-16d5-4649-92cf-5fa94c2e15a5": {"__data__": {"id_": "24492c3a-16d5-4649-92cf-5fa94c2e15a5", "embedding": null, "metadata": {"page_label": "11", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee867fbe-595f-45e6-8966-2359ac7fe8bd", "node_type": "4", "metadata": {"page_label": "11", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "68f764ae58a959279dde2bcc35025672dc1a365b1c18ee5aae9fa04ca3013f74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41629879-0232-4170-ac29-2cf84c2e14d0", "node_type": "1", "metadata": {}, "hash": "f3a95f85271a3a8bb261e646fdf203e647830f9f481462c8fae1bf97fc54530f", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 11\nTable 1: Main results on (a) FineDiving and (b) MTL-AQA. Prediction accuracy\n(SRCCandR\u21132) and uncertainty calibration ( \u03c4) metrics are reported. We compare\nour method with exemplar-based and exemplar-free baselines.\n(a)Results on FineDiving\nMetrics\nSRCC (\u2191)R\u21132(\u2193)\u03c4(\u2191)\nExemplar\nbasedCoRe [56] 0.9061 0.3615 -\nTSA [52] 0.9203 0.3420 -\nTPT [2] 0.9333 0.2877 -0.5556\nExemplar\nfreeUSDL [42,52] 0.8913 0.3822 0.3778\nMUSDL [42,52] 0.8978 0.3704 0.4667\nDAE [57] 0.8820 0.4919 -0.1999\nDAE-MT [57] 0.9285 0.3320 -0.4667\nRICA2(Ours) 0.9402 0.2838 0.6444\nRICA2\u2020(Ours) 0.9421 0.2600 -(b)Results on MTL-AQA\nMetrics\nSRCC (\u2191)R\u21132(\u2193)\u03c4(\u2191)\nExemplar\nbasedTSA-Net [48] 0.9422 - -\nCoRe [56] 0.9512 0.2600 -\nDAE-CoRe [57] 0.9589 - -\nTPT [2] 0.9607 0.2378 -0.1111\nExemplar\nfreeC3D-AVG-MTL [31] 0.9044 - -\nUSDL [42] 0.9231 0.4680 0.1556\nMUSDL [42] 0.9273 0.4510 -0.0667\nDAE [57] 0.9231 - -\nDAE-MT [57] 0.9490 0.2738 -0.4222\nRICA2(Ours) 0.9594 0.2580 0.6000\nRICA2\u2020(Ours) 0.9620 0.2280 -\n0 2 4 6 8\nUncertainty \n357911Mean Absolute error\nTPT ( =-0.56)\nUSDL ( =0.38)\nMUSDL ( =0.47)\nOurs ( =0.64)\n(a)Uncertainty calibration on FineDiving\n0 2 4 6 8\nUncertainty \n357911Mean Absolute error\nTPT ( =-0.11)\nUSDL ( =0.16)\nMUSDL ( =-0.07)\nOurs ( =0.60)\n (b)Uncertainty calibration on MTL-AQA\nFig. 3: Uncertainty vs. prediction error (MAE) on (a) Finediving and (b) MTL-\nAQA. Results are reported on the test splits, with the X-axis as the uncertainty bin\nindex (uncertainty increases from left to right) and the Y-axis as the MAE in the bin. In\ncomparison to baselines, RICA2has improved calibration with lower prediction errors.\nExperiment setup. We follow the evaluation protocol of [2,51,56], dividing the\ndataset into the standard train set of 1059 videos and a test set of 353 videos.\nFurther, we use the same input video settings as TPT [2] in our experiments to\nensure a fair comparison. Specifically, for each video, we uniformly sample 103\nframes segmented into 20overlapping clips, each containing 8continuous frames.\nPlease refer to supplement Sec. A.2 for more details.\nResults. Tab. 1b summarizes our results on MTL-AQA. Similar to FineDiving,\nRICA2shows state-of-the-art results on MTL-AQA across all evaluation metrics.\nSpecifically, our RICA2\u2020outperforms the best exemplar-free model (DAE-MT)\nby a relative margin of 1.4% /16.7%onSRCC/R\u21132.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41629879-0232-4170-ac29-2cf84c2e14d0": {"__data__": {"id_": "41629879-0232-4170-ac29-2cf84c2e14d0", "embedding": null, "metadata": {"page_label": "11", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee867fbe-595f-45e6-8966-2359ac7fe8bd", "node_type": "4", "metadata": {"page_label": "11", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "68f764ae58a959279dde2bcc35025672dc1a365b1c18ee5aae9fa04ca3013f74", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24492c3a-16d5-4649-92cf-5fa94c2e15a5", "node_type": "1", "metadata": {"page_label": "11", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "26653282af21868349b3222b211b23cad537196dc9e75db91a08e3abf88e803f", "class_name": "RelatedNodeInfo"}}, "text": "Experiment setup. We follow the evaluation protocol of [2,51,56], dividing the\ndataset into the standard train set of 1059 videos and a test set of 353 videos.\nFurther, we use the same input video settings as TPT [2] in our experiments to\nensure a fair comparison. Specifically, for each video, we uniformly sample 103\nframes segmented into 20overlapping clips, each containing 8continuous frames.\nPlease refer to supplement Sec. A.2 for more details.\nResults. Tab. 1b summarizes our results on MTL-AQA. Similar to FineDiving,\nRICA2shows state-of-the-art results on MTL-AQA across all evaluation metrics.\nSpecifically, our RICA2\u2020outperforms the best exemplar-free model (DAE-MT)\nby a relative margin of 1.4% /16.7%onSRCC/R\u21132. When compared with the\ncompetitive exemplar-based TPT, our has slightly better SRCC(SRCC TPT=\n0.9607vsSRCCRICA2\u2020= 0.9620) and R\u21132(+4.1%relative margin). Again,\ncompared to previous methods, our stochastic model shows improved calibration\n(\u03c4RICA2= 0.60vs.\u03c4USDL = 0.16vs.\u03c4TPT=\u22120.11) as shown in Fig. 3b.", "mimetype": "text/plain", "start_char_idx": 1651, "end_char_idx": 2678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c612bde8-f4b8-4a61-96f0-2bf38978f703": {"__data__": {"id_": "c612bde8-f4b8-4a61-96f0-2bf38978f703", "embedding": null, "metadata": {"page_label": "12", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8215dadd-cb31-4bb3-85e7-4fc6b7ea4b38", "node_type": "4", "metadata": {"page_label": "12", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "ce8ca1056ea5f1c9a988c81aee1d4cc31f98b307ddfb00dba150703b9cfeccf8", "class_name": "RelatedNodeInfo"}}, "text": "12 A. Majeedi et al.\nTable 2: Results on JIGSAWS [11] dataset . Only prediction accuracy ( SRCC) is\nconsidered due to the limited sample size. RICA2outperforms all prior approaches.\nTask\nS NP KT Avg\nExemplar\nbasedCoRe [56] 0.84 0.86 0.86 0.85\nTPT [2] 0.88 0.88 0.910.89\nExemplar\nfreeST-GCN [55] 0.31 0.39 0.58 0.43\nTSN [47] 0.34 0.23 0.72 0.46\nJRG [30] 0.36 0.54 0.75 0.57\nUSDL [42] 0.64 0.63 0.61 0.63\nMUSDL [42] 0.71 0.69 0.71 0.70\nDAE [57] 0.73 0.72 0.72 0.72\nDAE-MT [57] 0.78 0.74 0.74 0.76\nRICA2\u2020(Ours)0.880.930.880.90\nRICA2(Ours) 0.920.940.900.92\n4.3 Results on JIGSAWS\nDataset. In addition to diving videos, we also evaluate RICA2on JIGSAWS [11]\n\u2014 a robotic surgical video dataset. The dataset includes three tasks: \u201cSuturing\n(S),\u201d with 39 recordings, \u201cNeedle Passing (NP),\u201d with 26 recordings and \u201cKnot\nTying (KT)\u201d with 36 recordings. JIGSAWS is widely used for action quality\nassessment, despite its small scale.\nExperiment setup. Due to the limited number of samples in the dataset (as few\nas 7 videos in the test set), cross-validation is often considered for evaluation on\nJIGSAWS. To ensure a fair comparison, we follow the commonly adopted splits\nfrom [42], and the input video setting from [2]. Specifically, for each video, we\nuniformly sample 160frames which are segmented into 20non-overlapping clips.\nWe opt to not include score calibration curves due to the limited sample size of\nthe test sets. Additionally, the key steps in JIGSAWS are general motions ( e.g.\nreaching for the needle, orienting the needle, etc.) and thus cannot be localized\nto any specific section of the video. Thus, we do not use the auxiliary losses for\nthis experiment. More details are described in supplement Sec. A.3.\nResults. Tab. 2 summarizes our results on JIGSAWS. Similar to previous\ndatasets, our models exhibit notable advancements over the previous state-of-\nthe-art model TPT [42], showcasing substantial improvements of 1.1% (RICA2)\nto 3.4% (RICA2\u2020) in terms of average SRCCrelative to the exemplar-based\nstate-of-the-art TPT [2]. When compared to the exemplar-free methods, our\napproach demonstrates an impressive 18.4% (RICA2) to 21.0% (RICA2\u2020) relative\ngain in average SRCCcompared to the latest method DAE-MT [57].\n4.4 Ablation Studies\nTo understand our model design choices, we conduct ablation studies on the\nMTL-AQA [31] dataset. Additional ablations are in supplement Sec. C.2.\nExperiment setup. To simplify our experiments, we opt for running our\nablations using fixed I3D features. This allows us to precisely evaluate the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2539, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dba21da-bf24-4309-ae62-7e1fdfae8f4e": {"__data__": {"id_": "3dba21da-bf24-4309-ae62-7e1fdfae8f4e", "embedding": null, "metadata": {"page_label": "13", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6940fce2-b1ed-4a72-83f6-4767da1f0e0e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "a3d6529f878ab85d99c28a88d2f815823598d70e6bec3a13bf0405e7953200af", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 13\nTable 3: Ablation studies of model components on MTL-AQA dataset. * indicates\nthat the text embeddings were frozen during training.\nStep\nRep.DAG\n(Rubric)LKLLAuxMetrics\nSRCC (\u2191)R\u21132(\u2193)\u03c4(\u2191)Avg. Rank (\u2193)\nRandom \u00d7 \u00d7 \u00d7 0.9426 0.3882 - 5.50\nText \u00d7 \u00d7 \u00d7 0.9431 0.3509 - 4.50\nText \u2713 \u00d7 \u00d7 0.9430 0.3336 - 4.50\nText* \u2713 \u00d7 \u00d7 0.9437 0.3335 - 3.50\nText* \u2713 \u2713 \u00d70.9448 0.3329 0.4222 1.83\nText* \u2713 \u2713 \u2713 0.9460 0.3303 0.4222 1.17\ncontribution of different components of our model. Specifically, we choose I3D\nweights from an intermediate checkpoint of our trained model and extract features\nfor all videos with the frozen backbone.\nBase model. Our ablation constructs a base model using randomly initialized\nstep embeddings, an averaging of these embeddings after cross attention with the\nvideo features, followed by an MLP for scoring. This base model is trained using\nonly the MSE loss. We then gradually add modules from RICA2and study their\neffects. Tab. 3 presents our results using the same features and training epochs,\nwith our base model in row 1.\nText embeddings as step representations. We first replace randomly ini-\ntialized step representations with the text embeddings of step descriptions. This\nleads to a major boost in R\u21132(Tab. 3 row 1 vs. row 2), by leveraging knowledge\nencoded in the LLM [8]. Further, we find that freezing the text embeddings leads\nto comparable results and faster convergence (Tab. 3 row 3 vs. row 4).\nDoes the scoring rubric help? We also investigate the effects of encoding\nsteps and rubric as a DAG\u2014a key design of our model. Adding the DAG results\nin a noteworthy boost in R\u21132(Tab. 3 row 2 vs. row 3). This improvement can be\nascribed to the DAG\u2019s proficiency in dissecting the action quality across steps.\nEffects of loss functions. We now study the loss terms. Our loss function has\nthree terms (a) the MSE loss ( LMSE) to minimize prediction error, (b) the KL\nloss (LKL) to regularize the stochastic embeddings, and (c) the auxiliary loss\n(LAux) to ensure temporal ordering of steps. Adding the KL loss LKLyields\nsimilar results in SRCCandR\u21132, yet enables calibrated uncertainty estimation.\nFurther attaching the auxiliary loss LAuxleads to improvement in both SRCC\nandR\u21132, while maintaining the calibration performance.\nEvaluating the cross-attention maps. To gain insight into RICA2, we now\nexamine the cross-attention maps between the step representations and video\nfeatures in our learned embedding function f. Fig. 4 visualizes the attention map\non two test videos on FineDiving. These maps reveal that a step representation\nis likely to attend to video features during which the step occurs, indicating that\nRICA2learns to encode the temporal location of individual steps.\nWe further evaluate this localization ability following the Pointing game\nprotocol [58], widely considered in weakly supervised / unsupervised localization\ntasks [50,61]. Pointing Game compares a generated heatmap with an annotated", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e72370a-716d-44fe-b21d-891ef79d7513": {"__data__": {"id_": "6e72370a-716d-44fe-b21d-891ef79d7513", "embedding": null, "metadata": {"page_label": "14", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e391a2a-e379-454e-94c4-5b1739a68571", "node_type": "4", "metadata": {"page_label": "14", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "6fc8dd1053002421d8b7965cc9d553316902ea95945cbe0757ee726a88a3e4e1", "class_name": "RelatedNodeInfo"}}, "text": "14 A. Majeedi et al.\nForward3.5 Soms. PikeEntry\nArm Back1.5 Twist2 Soms. PikeEntry\nFig. 4: Visualization of the cross-attention maps . Y-axis : attention value; Y-axis:\nclip indices (time). Each curve shows an attention map from a step representation to\nthe temporal video features. The frames shown above are aligned with the timing of\nthe corresponding attention plot. Curves and steps are colored accordingly.\ntime interval and counts the chance of the heatmap\u2019s peak falling into the specified\ninterval. Our evaluation focuses on the FineDiving dataset since it is the only\ndataset providing annotated time intervals for individual steps. When evaluated\non the full test set, attention maps from our model attain an accuracy of 61.4%in\nthe Pointing game protocol, significantly outperforming the chance level accuracy\nof30.7%(given each video has 3.26steps on average). Note that we did not use\nany annotated segmentation data for training.\n5 Conclusion and Discussion\nIn this paper, we present a deep probabilistic model for action quality assessment\nin videos. Our key innovation is to integrate score rubrics and to model prediction\nuncertainty. Specifically, we propose to adapt stochastic embeddings to quantify\nthe uncertainty of individual steps, and to decode action scores using a variant\nof graph neural network operating on a DAG encoding the score rubric. Our\nmethod offers an exemplar-free approach for AQA, achieves new state-of-the-art\nresults in terms of prediction accuracy on public benchmarks, and demonstrates\nsuperior calibration of the output uncertainty estimates. We believe that our\nwork provides a solid step towards AQA. We hope that our method and findings\ncan shed light on the challenging problem of trustworthy video recognition.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7e0b7ee-c012-4892-b947-9cb922f6c50c": {"__data__": {"id_": "e7e0b7ee-c012-4892-b947-9cb922f6c50c", "embedding": null, "metadata": {"page_label": "15", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee9e35e6-eebc-407d-bc7f-c64f5ba77840", "node_type": "4", "metadata": {"page_label": "15", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "ffb154dfbbf2931ba17bcc1105dc128cb9a5c6ae5093c6e0cc5cdd54c33256c8", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 15\nAcknowledgement : This work was supported by the UW Madison Office of the\nVice Chancellor for Research with funding from the Wisconsin Alumni Research\nFoundation, by National Science Foundation under Grant No. CNS 2333491, and\nby the Army Research Lab under contract number W911NF-2020221.\nSupplement\nIn this supplement, we describe (1) technical, implementation, and experiment\ndetails for individual datasets, as well as our loss function (Sec. A); (2) additional\nresultsforsurgicalskillassessmentonCataract-101dataset(Sec.B);(3)additional\nablations including the study of architectural design and loss coefficients, choice\nof text embeddings and additional cross-attention plots, and the consideration of\naction recognition methods (Sec. C); (4) details of the scoring rubric considered\nin the model, with examples from FINA diving manual (Sec. D); and (5) further\ndiscussion of our work (Sec. E). We hope this document complements our paper.\nFor sections, figures, and tables, we use numbers ( e.g., Sec. 1) to refer to the\nmain paper and capital letters ( e.g., Sec. A) to refer to this supplement.\nA Technical, Implementation, and Experiment Details\nWe describe implementation and experiment details for 4 datasets considered\nin our paper, including FineDiving [52], MTL-AQA [31], JIGSAWS [11], and\nCataract-101 [38]. We further present technical details of our loss functions.\nA.1 Details for FineDiving\nWe follow the implementation from TSA [52] to process the input videos. Specifi-\ncally, we uniformly sample 96frames from each video, segmented into 9overlap-\nping clips of 16consecutive frames, with a stride of 10. The frames are resized to\na resolution of 200\u00d7112. During training, we employ a random crop of 112\u00d7112,\nwhile a center crop of size 112\u00d7112is performed during testing.\nWe use the I3D backbone [4] to extract video features, following prior works [2,\n52]. We generate text descriptions of steps with the help of GPT-4 [29], as shown\nin Table J. These descriptions are embedded using Flan-T5 XXL [7,54].\nFor training our model, we utilize AdamW [24] optimizer with a linear warmup\nfor5epochs and a total of 350epochs. The training batch size is set to 8, while the\nlearning rates for the I3D backbone, transformer blocks, and the head (DAG) are\nset to 1\u00d710\u22125,3\u00d710\u22125, and 5\u00d710\u22124, respectively. We also experimented with\nlearning rate decay, yet did not find it helpful with our long training schedule.\nA.2 Details for MTL-AQA\nWe follow TPT [2] for processing the input videos. Specifically, we uniformly\nsample 103frames from each video, segmented into 20 overlapping clips of 8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98fa0f83-1498-4b5e-a643-f6cd0e044b7c": {"__data__": {"id_": "98fa0f83-1498-4b5e-a643-f6cd0e044b7c", "embedding": null, "metadata": {"page_label": "16", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fe12c2f-1224-4fca-a3c8-2522dab95b4a", "node_type": "4", "metadata": {"page_label": "16", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "f98235d1d508b461f6fcdf6b0bcca19eb46d292f734eb9fd65fe17ed8ab46935", "class_name": "RelatedNodeInfo"}}, "text": "16 A. Majeedi et al.\nconsecutive frames with a stride of 5. The frames are resized to a resolution of\n455\u00d7256. During training, we employ a random crop of 224\u00d7224, while a center\ncrop of size 224\u00d7224is performed during testing.\nAgain, we use the I3D backbone [4] to extract video features, generate text\ndescriptions of individual steps with the help of GPT-4 [29] (see Table I), and\nfurther embed these descriptions using Flan-T5 XXL [7,54]\nFor training, we use the AdamW [24] optimizer with a linear warmup for 5\nepochs and a total of 350epochs. Other hyperparameters are kept the same as\nfor FineDiving (batch size of 8with learning rates for I3D backbone, transformer\nblocks, and the head (DAG) as 1\u00d710\u22125,3\u00d710\u22125, and 5\u00d710\u22124, respectively).\nA.3 Details for JIGSAWS\nWe adopt the video input configuration from TPT [2]. We uniformly sample 160\nframes from each video, and these frames are split into 20non-overlapping clips\nof8consecutive frames with a stride of 8. The frames are resized to a resolution\nof455\u00d7288. During training, we employ a random crop of size 224\u00d7224, while\nduring testing, a center crop of the same size ( 224\u00d7224) is performed.\nWe follow the same protocol to extract video features (using I3D) and step\nembeddings (using Flan-T5 XXL), as FineDiving and MTL-AQA experiments.\nStep descriptions generated with the help of GPT-4 are shown in Table K.\nFor training, we utilize AdamW [24] optimizer with a linear warmup for the\ninitial 5epochs and a total of 350epochs. Due to the smaller size of the dataset,\na small training batch size of 2is employed. The learning rates for the I3D\nbackbone, transformer blocks, and the head (DAG) are set to 1\u00d710\u22125,3\u00d710\u22125,\nand5\u00d710\u22124, respectively, maintaining consistency with the previous settings.\nWe note that in the JIGSAWS dataset, the steps are not performed in any\nspecific order. Moreover, steps are repeated multiple times within each video.\nConsequently, we do not employ the auxiliary losses ( LAux) of ranking and\nsparsity for our experiments on JIGSAWS.\nA.4 Cataract-101\nVideos in this dataset record complex surgical procedures (cataract surgery) and\nare thus significantly longer than other datasets (10 minutes vs. a few seconds).\nConsequently, we consider a stronger video backbone \u2014 SlowFast- 8\u00d78-R50 [10].\nWe split an input video into sliding windows (clips) of 64frames with a temporal\nstride of 16frames. For each clip, we resize all frames such that their shortest\nside is kept at 256. After a center-crop of size 224\u00d7224, the clips are fed into\nthe SlowFast model pre-trained on Kinetics. The extracted video features are\nfurther used as input to our model for training and inference. It is worth noting\nthat we do not update the SlowFast model during training, and thus, these video\nfeatures remain fixed.\nWe follow the same protocol to extract step embeddings (using Flan-T5 XXL),\nwith step descriptions in Table L. We utilize AdamW [24] optimizer with a linear", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe594650-284e-4529-8ef6-00ae696ba5c6": {"__data__": {"id_": "fe594650-284e-4529-8ef6-00ae696ba5c6", "embedding": null, "metadata": {"page_label": "17", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4547b7b1-f4d9-4d20-a999-2e7a93890a3e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "998615d6510cc61790a7f935e61620c04b617f0f841c9bd047f6ac606690ba06", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 17\nwarmup for 5epochs and a total of 350epochs for training. The batch size is 2\nand the learning rate is 1\u00d710\u22124.\nA.5 Details of Loss Functions\nAs discussed in Sec. 3.3 and 3.4, our loss function consists of the VIB loss ( LVIB)\nand the auxiliary loss ( LAux). Further, LVIBcomprises the MSE loss ( LMSE)\nand the KL loss ( LKL), and LAuxincludes the ranking loss ( Lrank) and the\nsparsity loss ( Lsparsity). Our overall loss is thus given by\nL=LMSE +\u03b2LKL+\u03b3(Lrank+Lsparsity ), (9)\nwhere \u03b2and\u03b3are the loss weights. \u03b2controls the bottleneck effect, and \u03b3\nmanages additional regularization ( e.g., temporal ordering).\nThe auxiliary losses of ranking ( Lrank) and sparsity ( Lsparsity) are designed to\nenforce the ordering of the step representation. Specifically, we follow TPT [2] and\nconsider the last cross-attention map from our embedding function f. Concretely,\ngiven Sstep representations as the queries, i.e.Q\u2208RS\u00d7D, and video features\ndefined over Ttime steps as the keys, i.e.K\u2208RT\u00d7D, we denote their cross-\nattention map as A\u2208RS\u00d7T. Considering the cross-attention map for each step\n(i.e. a row in A), a corresponding temporally-weighted center ( \u00af\u03b1s), as a detector\nof the step\u2019s temporal location, is calculated as\n\u00af\u03b1s=\u03a3T\nt=1t\u00b7As,t, (10)\nwhere \u03b1s,tis the similarity between a step representation and a video feature at\ntime step t. Our auxiliary losses are defined on top of these centers.\nThe sparsity loss Lsparsityis defined to discourage the spread of query atten-\ntion densities. Lsparsityis given by\nLsparsity =\u03a3S\ns=1\u03a3T\nt=1|t\u2212\u00af\u03b1s| \u00b7\u03b1s,t (11)\nThe ranking loss Lrankis designed to encourage all centers to follow the\npre-specified step ordering. Lrankis written as\nLrank =\u03a3S\u22121\ns=1max(0 ,\u00af\u03b1s\u2212\u00af\u03b1s+1+m) + max(0 ,1\u2212\u00af\u03b11+m) + max(0 ,\u00af\u03b1S\u2212T+m)(12)\nB Results on Cataract-101\nWe now present our results on Cataract-101, a video dataset for surgical skill\nassessment. These results are omitted from the main paper due to lack of space.\nDataset. Cataract-101 [38] is a publicly available dataset of 101 cataract surgery\nvideos, with each recorded procedure annotated with frame-level labels and\nthe performing surgeon\u2019s corresponding skill score. The skill scores are discrete,\nlabeled as either expert or novice.\nExperiment setup. We train our model on 80 randomly picked videos and test\non the remaining 21 videos. Due to the significantly longer duration of videos", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "478e91ac-4668-44f6-863e-6a953e6d3f84": {"__data__": {"id_": "478e91ac-4668-44f6-863e-6a953e6d3f84", "embedding": null, "metadata": {"page_label": "18", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d9eaa18-311c-42a6-b3d3-005448de5845", "node_type": "4", "metadata": {"page_label": "18", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "aaf438ba98401e2b47c149fe204700969c8ad4c76bd61bb0af5783aa82115cf8", "class_name": "RelatedNodeInfo"}}, "text": "18 A. Majeedi et al.\nTable A: Ablation on loss weight \u03b2.We report the accuracy and calibration metrics.\nMetrics\n\u03b2SRCC (\u2191)R\u21132(\u2193)\u03c4(\u2191)\n10\u221210.9450 0.3614 0.5556\n10\u221220.9456 0.3529 0.4667\n10\u221230.9463 0.3478 0.3333\n10\u221240.9451 0.3289 0.3333\n10\u221250.9449 0.3393 0.4222\n(exceeding 10 minutes compared to a few seconds for diving videos), we perform\nour experiments with pre-extracted features. In this case, we choose a more\nadvanced model for feature extraction. Specifically, we utilize SlowFast [10] model\npre-trained on Kinetics [16] to extract features for the videos and train our model\non the extracted features. Given the binary labels, we switch from the mean\nsquared error (MSE) loss to binary cross entropy (BCE) loss, which is compatible\nwith our maximum log-likelihood interpretation of the loss.\nWe report the average accuracy as our evaluation metric and compare our\nresults to a baseline method TUSA [23] specifically designed for surgical skill\nassessment. TUSA is trained and evaluated using the same features as our method.\nResults and discussion. Both TUSA and our method reach an impressive\n100% accuracy. Randomizing train-test splits leads to similar perfect accuracy.\nOur analysis of this dataset shows the duration of the surgery is a good predictor\nof the surgeon\u2019s skills; expert surgeons often perform this routine surgery faster\nthan novice surgeons.\nC Additional Ablation Studies\nWe further discuss additional ablation studies. Similar to the ablations in the\nmain paper (Sec. 4.4), we run the experiments on the MTL-AQA [31] dataset\nwith the same pre-extracted I3D features unless otherwise specified.\nC.1 Effects of Loss Coefficients\nWe first evaluate the effects of loss weights \u03b2and\u03b3. To this end, we fix \u03b3and\nvary\u03b2, which balances the objectives of minimizing prediction error and ensuring\nuncertainty estimation. Table A shows the results. Lower values of \u03b2often lead to\nminorly improved accuracy metrics, yet higher values of \u03b2allow better calibration.\nThrough the experiments, we empirically observe that an optimal balance can\nbe attained with a training schedule in which \u03b2is initially set to 10\u22125and\nincrementally increased to a maximum value of \u03b2= 0.005during the training\nprocess. All results for RICA2in our study are reported using this annealing\nscheme, which was also discussed in prior work [18].", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12fcc714-3ebc-4f55-99f1-63ea40dfe7b5": {"__data__": {"id_": "12fcc714-3ebc-4f55-99f1-63ea40dfe7b5", "embedding": null, "metadata": {"page_label": "19", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8eed8365-5a18-41cf-b181-6b695ea371bf", "node_type": "4", "metadata": {"page_label": "19", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "f6bc68269bed4b849b8245c9d11afc23bf2b6da1aaad3d323d91fc9ef26c51f4", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 19\nTable B: Ablation on design choices . We vary the design of major components\nin RICA2and report the results on MTL-AQA. #Convs denotes the number of 1D\nconvolution blocks, #Enc denotes number of encoder blocks and #Dec denotes the\nnumber of decoder blocks\nEmbed\nDim#Convs #Enc #DecDAG\n(Rubric)LsparLrankLKLMetrics\nSRCC (\u2191)R\u21132(\u2193)\u03c4(\u2191)Avg. Rank (\u2193)\n256 2 0 0 \u00d7 \u00d7 \u00d7 \u00d7 0.9267 0.4213 - 6.67\n512 2 2 0 \u00d7 \u00d7 \u00d7 \u00d7 0.9263 0.3843 - 6.33\n512 2 2 0 \u00d7 \u00d7 \u00d7 \u00d7 0.9272 0.4090 - 6.00\n512 2 2 2 \u00d7 \u00d7 \u00d7 \u00d7 0.9444 0.3707 - 4.33\n512 2 2 2 \u2713 \u00d7 \u00d7 \u00d7 0.9437 0.3335 - 4.00\n512 2 2 2 \u2713 \u2713 \u00d7 \u00d7 0.9440 0.3562 - 4.33\n512 2 2 2 \u2713 \u2713 \u2713 \u00d70.9451 0.3458 - 3.33\n512 2 2 2 \u2713 \u2713 \u2713 \u2713 0.9460 0.3303 0.4222 1.00\nWe further experiment with different values for \u03b3, which denotes the weight\nfor the auxiliary losses of ranking and sparsity. We empirically find our results\nare insensitive to different values and set \u03b3= 0.1for our experiments.\nC.2 Design Choices\nWe vary the design of our model and examine their contributions to final per-\nformance. Specifically, our model extracts video features and step embeddings,\nfurther encodes individual features, uses cross-attention Transformer blocks to\nfuse them and decode step representation, and finally decodes a distribution of\nscores using a DAG representing the rubric. We study the design of encoding\n(convolution and self-attention) and decoding modules (cross-attention), as well\nas the choice of loss terms.\nTable B shows the results. In addition to what is presented in the main\npaper, we highlight some key choices to boost performance in Table B: (a) using\nembedding dim of512over256and utilizing 2decoder blocks over 1decoder\nblock; and (b) combining all loss terms. We can clearly notice our full model\n(shown in the last row of Table B) leads to the best performance in accuracy\nand score calibration. Specifically, this model combines two 1D-conv blocks, 2\nself-attention blocks, integration of text queries, and 2decoder blocks, followed\nby the DAG, and is optimized with LKLand the auxiliary losses.\nC.3 Effects of Text Queries\nTo assess the influence of various language models, we leverage text embeddings\nderived from three different models: OpenClip [14], E5large[46], and Flan-T5\nXXL[7,54]. The corresponding results are detailed in Table C. Our empirical\nfindings indicate that embeddings derived from Flan-T5 XXL [7,54] result in the\nbest performance on accuracy metrics ( SRCCandR\u21132).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9cb75a1-e882-4067-8021-911ef5816868": {"__data__": {"id_": "d9cb75a1-e882-4067-8021-911ef5816868", "embedding": null, "metadata": {"page_label": "20", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1f32d52-8058-4af1-9c54-b1d11ba9607e", "node_type": "4", "metadata": {"page_label": "20", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "00a4a4978bb5c051faa4eef58fa482aeaad27a4a890ba0c98ee4a55f83654e5d", "class_name": "RelatedNodeInfo"}}, "text": "20 A. Majeedi et al.\nTable C: Ablation on text embeddings. We experiment with different language\nmodels and report results on MTL-AQA. Dimdenotes the text embedding dimensions.\nText Model DimMetrics\nSRCC (\u2191)R\u21132(\u2193) \u03c4(\u2191)\nOpenClip [14] 512 0.9455 0.4339 0.2444\nE5large[46] 1024 0.9431 0.3410 0.4222\nFlan-T5 XXL [7] 4096 0.9460 0.3303 0.4222\nC.4 What if Steps Are Not Available?\nWhen step information is not available at inference time, action recognition\nmethods offer a solution for identifying steps in the input video. To explore\nthis scenario, we experimented on the FineDiving dataset [52], utilizing pre-\nextracted I3D features. For this task, we designed a simple action recognition\nmodel comprising a 2-layer MLP projection block, 2 Transformer blocks, and 2\nMLPs. This model was trained on the training split to predict the dive number\n(i.e. the steps in a video). Subsequently, we evaluated the trained model on the\ntest set, achieving an average step recognition accuracy of 82%. The generated\nstep predictions on the test set were then incorporated into RICA2, replacing\nthe ground truth step presence information. Using the model-predicted step\ninformation resulted in an SRCCandR\u21132of 0.9379 and 0.2779 respectively\nwhile the ground truth SRCCandR\u21132are 0.9389 and 0.2750. These results\ndemonstrate the feasibility of leveraging action recognition methods to provide\nstep information for RICA2during inference.\nC.5 Additional Visualization of Our Results\nWe present additional visualization of sample results in Fig. A. These samples\nare from the test set of FineDiving, and the visualization follows the same format\nas Fig. 4 of our main paper.\nD Score Rubric and Our DAG Representation\nA key component of RICA2is the integration of a scoring rubric in the form of a\ndirected acyclic graph (DAG). As mentioned in Sec. 3, we assume that each video\ncontains a set of steps, and each action step is independently scored. Subsequently,\na rule-based rubric is employed to aggregate individual scores and calculate a\nfinal quality score, in which steps might be grouped into intermediate stages. Our\nDAG representation applies to a broad range of technical skill assessment tools.\nWe use the FINA diving manual [15] as an example to further illustrate our\nrepresentation. For an individual dive, a sequence of action items is predetermined,\nwith various combinations leading to different difficulty degrees (see examples in\nTable G). A panel of 7 to 11 judges will assess the dive, each assigning a score on\na scale of 0-10. Judges will follow specified guidelines to evaluate the performance", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a16c1f7-15cf-4a2c-9a79-747544364717": {"__data__": {"id_": "2a16c1f7-15cf-4a2c-9a79-747544364717", "embedding": null, "metadata": {"page_label": "21", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97f710bb-1917-44de-a9fc-10ed1f455931", "node_type": "4", "metadata": {"page_label": "21", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "866c88fd2ad86b403daa5fbbd42aa7a0a2189cd95bf8e70ccc58e9f639970735", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 21\nBack 1.5 Twist 2.5 Soms. Pike Entry\nInward 3.5 Soms. Tuck Entry\nFig. A: Visualization of the cross-attention maps on two test videos from Fine-\nDiving [52] with the Y-axis as attention value and the X-axis as clip indices (time). Each\ncurve shows an attention map from a step representation to the temporal video features.\nThe frames shown are aligned with the time axis of the corresponding attention plot.\nCurves and steps are colored accordingly\nof the approach and takeoff (Table D), the flight (Table E), and the entry into\nthe water (Table F) to arrive at a final score. Other considerations e.g. technique,\nexecution and overall performance may be taken into account. The top two and\nthe bottom two scores are discarded. The remaining scores are summed up and\nmultiplied by the difficulty degree, yielding the final rating. Table H illustrates a\ncompleted diving sheet from a real diving event.\nWe note that RICA2does not employ the detailed exact rubric of FINA.\nInstead, our method follows its general structures by assuming key steps, the\nscoring of these steps, and the combination of individual scores \u2014 a central\nconcept in technical skill assessment [25,34,49].\nE Further Discussions\nE.1 Benchmark Settings\nWe compare the setting of the baseline exemplar-based methods. Exemplar-based\nmethods rely on exemplar videos and their corresponding scores during both the\ntraining and testing phases. Latest methods, such as TPT [2], TSA [52], and\nCoRE [56], utilize dive numbers to select the exemplar videos and their associated\nquality scores. Notably, these dive numbers uniquely encode the presence and\nsequenceofstepsbeingexecuted,asshowninTable7ofFineDiving[52].Moreover,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68531499-bb42-473e-a587-3de70f2999af": {"__data__": {"id_": "68531499-bb42-473e-a587-3de70f2999af", "embedding": null, "metadata": {"page_label": "22", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76afa07b-51fb-4260-833a-2119d0af8043", "node_type": "4", "metadata": {"page_label": "22", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7265e712e94ee2ee587e8228f9b1a6c8d2d05bce22103a92578efc634af16d5e", "class_name": "RelatedNodeInfo"}}, "text": "22 A. Majeedi et al.\nArm Forward1 Twist2 Soms. PikeEntryTakeoffFlightFinishFinal score\nAction: 6142DForward2.5 Soms. PikeEntryTakeoffFlightFinishFinal score\nAction: 5152BArm Reverse3 Soms. TuckEntryTakeoffFlightFinishFinal score\nAction: 636C2.5 Soms. Pike1 Twist\nFig. B:Sample DAGs for three diving actions in Finediving.\nTSA incorporates not only the step presence but also precise start and end time\nstamps of each performed step.\nIn sharp contrast, our method takes an input of step presence and ordering\nduringtrainingandinference,withouttheneedforexemplarvideos,theirscoresor\ntimestamp data. Different from prior methods, our method additionally considers\ntext descriptions of steps, obtained by intuitively expanding step names into\nsimple sentences as shown in Tables I, J, and K. While our method assumes a\npre-specified scoring rubric, this rubric is incorporated into our model design,\nand not as part of the input.\nE.2 Trustworthy Visual Recognition\nOver the last decade, we have witnessed major advances in visual recognition\nwith deep learning, leading to significantly improved results on public datasets.\nDespite the superior performance of these deep models, one remaining question\nis how users can trust their output, knowing that mistakes can be made by\nthese models. This trustworthy visual recognition has multiple facets. We argue\ntwo of the key aspects are to provide credible confidence of the output and to\nconsider a human-interpretable decision-making process. Our work in this paper\ntakes a step towards these two critical aspects while addressing the challenging\nproblem of video-based AQA. We presented a deep model for AQA that integrates\nthe human score rubric and models the output confidence. Our method could\nfacilitate high-stakes applications of AQA, including competitive sports and\nhealthcare, where understanding and trusting the model\u2019s decisions are crucial,\nand low-confident samples can be readily passed to human experts.\nE.3 Ethical Concerns\nOur work focuses on the technical aspect of action quality assessment, and as\nsuch we do not anticipate major ethical concerns.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4db6adb3-311a-40fc-a2e6-ef228e62e821": {"__data__": {"id_": "4db6adb3-311a-40fc-a2e6-ef228e62e821", "embedding": null, "metadata": {"page_label": "23", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a86645d2-43eb-4032-9082-0ad4ee2a96bd", "node_type": "4", "metadata": {"page_label": "23", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "07691e8f89ca80daa1a244f7f52c2a137045077b24de7c05e3120cf0a9c0b683", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ba217a4-b060-4688-9c76-1a24414bc785", "node_type": "1", "metadata": {}, "hash": "7d96813b297da713a26d90c2309c790bdfe1015cafe1c5e8a9adf5282db74b16", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 23\nTable D: Guidelines for judging the entry into water [15].\n \n \n23 \nFINA High Diving  Manual  \nCommon faults in Take -Offs  \n   \nBalance  - The take -off should be from a balanced position, which allows maximum height and \ndesirable angle of take -off resulting in correct distance from the platform . A high diver leaning forward \nwith his or her toes hanging over the end of the platform is an example of poor balance, and usually \nnegatively impacts the remainder of the dive. Similarly, a high diver who is leaning back at the take -\noff is out of balance and may cause a dive to be too close to the platform  and should also incur a \npenalty. \nJudges guide on approach and take-off: \nFault  Range of deduction  Comments  \nUnbalance take -off \u00bd - 2 points   \nImproper angle of take -off \u00bd - 2 points   \nArmstand unbalance position  \u00bd - 2 points   \nArmstand no balance at all   Deduction 2 points  \nArmstand no control, no vertical   Deduction 1 point  \n6.2 Flight  \nA judge must evaluate several different elements during the flight of a dive. The height a high diver \nachieves from the platform and the distance away from the platform are two of these elements. Body \nposition is another element. Is a high diver\u2019s body position correct as defined by the dive being \nperformed? Finally, the overall form of the high diver must be considered, as well as speed of rotation \nand twist mechanics in twisting dives. Are the high diver\u2019s toes pointed and the body as tight as it \ncould be? A judge has much to evaluate in those one or two seconds while a high diver is in the air.  \nHeight \nAs mentioned in the take -off section of this manual, the height a high diver achieves on a dive is \ndetermined by the take -off from the platform. A reasonable amount of height is desirable, keeping in \nmind the type of dive being performed and the age le vel of the high diver. Lack of height may be \ncaused by poor balance, angle of take -off, poorly coordinated movements, or lack of strength. When \na high diver fails to reach a reasonable height, points shall be deducted. Where a high diver obtains \nimpressive  height, it may affect the overall impression of the dive and result in a reward for good \ntechnique.  \nTable E: Guidelines for judging the approach and take-off [15].\n \n \n30 \nFINA High Diving  Manual  \nAdditional guidelines relating to  body position  \n1. Where  a dive is performed  clearly  in a position  other  than that announced  the dive shall be \ndeemed unsatisfactory. The highest award for such a dive is 2 points. This  should  be declared  \nby the Referee  but should  be observed  by the judges  regardless  of such  a declaration.  \n2. Where a dive is performed with a break in position during the flight , the judges  shall award up \nto a maximum of 4 \u00bd points, according to their opinion.  \n3. In all flying  dives  a straight  position  \nshall be clearly  shown  and that position  \nshall be assumed from the take -off. \nWhen the straight position is not  shown \nfor at least one quarter of a somersault \n(90o) the judges  shall award up to a \nmaximum of 4 \u00bd points, according to \ntheir opinion  even if the Referee has not \ndeclared before to give the signal  \n \n \n \n \n \n \nForm  \nIn addition to the common form errors described above, a judge should look for  the following errors, \nwhich are common to all dives. Deductions should be based on the  judge\u2019s opinion of the severity \nof the  flaw. \n1. The high diver\u2019s feet are flat (toes not pointed).  \n2. The legs and arms are loose or bent at inappropriate times during the flight.  \n3. The legs come apart during the dive.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ba217a4-b060-4688-9c76-1a24414bc785": {"__data__": {"id_": "2ba217a4-b060-4688-9c76-1a24414bc785", "embedding": null, "metadata": {"page_label": "23", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a86645d2-43eb-4032-9082-0ad4ee2a96bd", "node_type": "4", "metadata": {"page_label": "23", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "07691e8f89ca80daa1a244f7f52c2a137045077b24de7c05e3120cf0a9c0b683", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db6adb3-311a-40fc-a2e6-ef228e62e821", "node_type": "1", "metadata": {"page_label": "23", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "4c74ca397288be54a76a01b6ff4fa74349873863ed388c91e6a88c1331361e8c", "class_name": "RelatedNodeInfo"}}, "text": "In all flying  dives  a straight  position  \nshall be clearly  shown  and that position  \nshall be assumed from the take -off. \nWhen the straight position is not  shown \nfor at least one quarter of a somersault \n(90o) the judges  shall award up to a \nmaximum of 4 \u00bd points, according to \ntheir opinion  even if the Referee has not \ndeclared before to give the signal  \n \n \n \n \n \n \nForm  \nIn addition to the common form errors described above, a judge should look for  the following errors, \nwhich are common to all dives. Deductions should be based on the  judge\u2019s opinion of the severity \nof the  flaw. \n1. The high diver\u2019s feet are flat (toes not pointed).  \n2. The legs and arms are loose or bent at inappropriate times during the flight.  \n3. The legs come apart during the dive.  \n \nJudges guide for  Flight \nFault  Range of  deduction  \nInsufficient height  \u00bd to 2 points  \nDive is too close to the platform (but does not hit  the \nplatform)  \u00bd to 2  points (according to opinion)  \nDive is unsafely close to the platform with the  head (but \ndoes not hit the platform)  2 maximum award  \nDive hits the platform with feet or hands (does not affect  \nthe dive)  4 \u00bd maximum award  \nDive hits the platform with the head   2 maximum award  \n  \n90 degrees  \nTable F: Guidelines for judging the flight [15].\n \n \n34 \nFINA High Diving  Manual  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJudges guide for Entry  \n  \nFault  Range of deduction  Comments  \nDive not vertical on entry  Judge\u2019s discretion   \nDive which is more than 5 degrees off \nvertical   It can\u2019t be classified as very good \ndive \nDive which is more than 35 degrees \noff vertical   Is classified as deficient dive  or \nlower  (2 \u00bd - 4 \u00bd) \nDive twisted  on the entry  Judge\u2019s discretion   \nDive twisted on the entry more or less \nthan 5\u00b0   It can\u2019t be classified as very good or \nexcellent dive  \nDive twisted on the entry more or less \nthan 15\u00b0   Is classified as deficient dive or \nmore (2 \u00bd - 4 \u00bd) \nDive twisted on entry more or less \nthan 35\u00b0   Is classified as deficient dive or  \nlower  (2 \u00bd - 4 \u00bd) \nDive twisted on entry more or less \nthan 90\u00b0  Failed dive   \nArms are  not in the correct position at \nthe entry  \u00bd - 2 points   \nArms are above the shoulder s at the \nentry  4 \u00bd maximum   \n302 A  \n5141 C", "mimetype": "text/plain", "start_char_idx": 2878, "end_char_idx": 5155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9b7b3f9-5f9c-46e0-b437-c4a2db9bd90e": {"__data__": {"id_": "e9b7b3f9-5f9c-46e0-b437-c4a2db9bd90e", "embedding": null, "metadata": {"page_label": "24", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4186645d-8c0c-46cc-8b1f-4fd87e88f0a4", "node_type": "4", "metadata": {"page_label": "24", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "19948d98d4c2c9cb463ee634d30c3f399943a9a65129c0e6a5283bfdb4ed4ce3", "class_name": "RelatedNodeInfo"}}, "text": "24 A. Majeedi et al.\nTable G: How the difficulty degree for a dive is determined.\n \nSPRINGBOARD 1 METER 3 METER \nSTR PIKE TUCK FREE STR PIKE TUCK FREE \nForward Group A  B C D A B C D \n101  Forward Dive  1.4 1.3 1.2 - 1.6 1.5 1.4 - \n102  Forward Somersault  1.6 1.5 1.4 - 1.7 1.6 1.5 - \n103  Forward 1\u00bd Somersaults  2.0 1.7 1.6 - 1.9 1.6 1.5 - \n104  Forward 2 Somersaults  2.6 2.3 2.2 - 2.4 2.1 2.0 - \n105  Forward 2\u00bd Somersaults  - 2.6 2.4 - 2.8 2.4 2.2 - \n106  Forward 3 Somersaults - 3.2 2.9 - - 2.8 2.5 - \n107  Forward 3\u00bd Somersaults - 3.3 3.0 - - 3.1 2.8 - \n108  Forward 4 Somersaults  - - 4.0 - - 3.8 3.4 - \n109  Forward 4\u00bd Somersaults  - - 4.3 - - 4.2 3.8 - \n112  Forward Flying Somersault  - 1.7 1.6 - - 1.8 1.7 - \n113  Forward Flying 1\u00bd Somersaults - 1.9 1.8 - - 1.8 1.7 - \n115  Forward Flying 2\u00bd Somersaults  - - - - - 2.7 2.5 - \nTable H: Sample scoring sheet from a diving event\nWritten DescriptionLevel\n5/7.5/10  Dive No.\n& Pos. Ltr.\n(A,B,C,D)Dive\nOrderPosition\n(S,P,T,F)D.D.Judges' AwardsCumulative\nTotal\nAwardJudges\nTotal1 2 3 4 5 6 7 8 9\n32.00\n32.00Forward 1.5 Somersault 1 103B P 1.6 6.5 7.0 6.5 7.0 6.5 0.0 0.0 0.0 0.0 20.0\n41.80\n73.80Forward 2.5 Somersault 2 105C T 2.2 6.0 5.5 6.5 6.5 6.5 0.0 0.0 0.0 0.0 19.0\nBack Dive 3 201B P 1.8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aedee2b-5a4d-41b1-8c55-88d0c0de21c1": {"__data__": {"id_": "3aedee2b-5a4d-41b1-8c55-88d0c0de21c1", "embedding": null, "metadata": {"page_label": "25", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06317ed0-510c-4175-9b10-408e74e9442b", "node_type": "4", "metadata": {"page_label": "25", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "7b02e5fc0f58137353bdda627eae5e6edf0ed5b926d624e96968680c46024ebd", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 25\nTable I: Text descriptions for individual steps in MTL-AQA [31]\nSubaction Description\nFreeIn this position, athletes have the freedom to perform\nany combination of dives from various categories\nwithout any restrictions or limitations\nTuckIn this position, athletes bring their knees to their\nchest and hold onto their shins while maintaining a\ncompact shape throughout their dive\nPikeIn this position, athletes maintain a straight body\nwith their legs extended and their toes pointed out\nwhile bending at the waist to bring their hands to-\nward their toes\nArmstandIn this position, athletes start by standing on their\nhands on the edge of the diving board and perform\ntheir dive while maintaining this handstand position\nInwardsIn this rotation type, athletes perform a forward-\nfacing takeoff and rotate inward toward the diving\nboard as they execute their dive\nReverseIn this rotation type, athletes perform a backward-\nfacing takeoff and rotate backward away from the\ndiving board as they execute their dive\nBackwardIn this rotation type, athletes perform a backward-\nfacing takeoff and rotate backward toward the diving\nboard as they execute their dive\nForwardIn this rotation type, athletes perform a forward-\nfacing takeoff and rotate forward away from the\ndiving board as they execute their dive\n0.5 SomersaultAthletes perform a half rotation in the air during\ntheir dive\n1 SomersaultAthletes perform a full forward or backward rotation\nin the air during their dive\n1.5 SomersaultAthletes perform a full rotation and an additional\nhalf rotation in the air during their dive\n2 SomersaultAthletes perform two full forward or backward rota-\ntions in the air during their dive\n2.5 SomersaultAthletes perform two full rotations and an additional\nhalf rotation in the air during their dive\n3 SomersaultAthletes perform three full forward or backward ro-\ntations in the air during their dive\n3.5 SomersaultAthletes perform three full rotations and an addi-\ntional half rotation in the air during their dive\n4.5 SomersaultAthletes perform four full rotations and an additional\nhalf rotation in the air during their dive\n0.5 TwistAthletes perform a half twist in the air during their\ndive\n1 TwistAthletes perform one full twist in the air during their\ndive\n1.5 TwistAthletes perform one and a half twists in the air\nduring their dive\n2 TwistAthletes perform two full twists in the air during\ntheir dive\n2.5 TwistAthletes perform two and a half twists in the air\nduring their dive\n3 TwistAthletes perform three full twists in the air during\ntheir dive\n3.5 TwistAthletes perform three and a half twists in the air\nduring their dive\nEntryA diving technique involving a entry into the water,\ntypically performed at the end of a dive", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f260554-162d-44f3-a5a3-80f4c4200c44": {"__data__": {"id_": "2f260554-162d-44f3-a5a3-80f4c4200c44", "embedding": null, "metadata": {"page_label": "26", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d847736-78e5-4447-9c9d-bb758f86f3fa", "node_type": "4", "metadata": {"page_label": "26", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "b2feeff76756bd49ebad8959c45b44ab92ee1d43610d7b12e990bcfd769292a6", "class_name": "RelatedNodeInfo"}}, "text": "26 A. Majeedi et al.\nTable J: Text descriptions for individual steps in FineDiving [52]\nSubaction Description\nForwardA diving technique involving a front-facing takeoff and\nentry\nBackA diving technique involving a back-facing takeoff and\nentry\nReverseA diving technique involving a back-facing takeoff and\nentry while rotating forward\nInwardA diving technique involving a front-facing takeoff and\nentry while rotating backwards\nArm ForwardA diving technique involving a front-facing takeoff and\nentry with arms extended and hands meeting above\nthe head\nArm BackA diving technique involving a back-facing takeoff and\nentry with arms extended and hands meeting above\nthe head\nArm ReverseA diving technique involving a back-facing takeoff and\nentry with arms extended and hands meeting above\nthe head while rotating forward\n1 Somersault PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with one somersault\n1.5 Somersaults PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with one and a half\nsomersaults\n2 Somersaults PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with two somersaults\n2.5 Somersaults PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with two and a half\nsomersaults\n3 Somersaults PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with three somersaults\n3.5 Somersaults PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with three and a half\nsomersaults\n4.5 Somersaults PikeA diving technique involving a takeoff and rotating\nforward to form a pike position with four and a half\nsomersaults\n1.5 Somersaults TuckA diving technique involving a takeoff and rotating\nforward to bend at the waist with one and a half\nsomersaults\n2 Somersaults TuckA diving technique involving a takeoff and rotating\nforward to bend at the waist with two somersaults\n2.5 Somersaults TuckA diving technique involving a takeoff and rotating\nforward to bend at the waist with two and a half\nsomersaults\n3 Somersaults TuckA diving technique involving a takeoff and rotating\nforward to bend at the waist with three somersaults\n3.5 Somersaults TuckA diving technique involving a takeoff and rotating\nforward to bend at the waist with three and a half\nsomersaults\n4.5 Somersaults TuckA diving technique involving a takeoff and rotating\nforward to bend at the waist with four and a half\nsomersaults\n0.5 TwistA diving technique involving a takeoff and half a twist\nbefore entering the water\n1 TwistA diving technique involving a takeoff and one full\ntwist before entering the water\n1.5 TwistsA diving technique involving a takeoff and one and a\nhalf twists before entering the water\n2 TwistsA diving technique involving a takeoff and two full\ntwists before entering the water\n2.5 TwistsA diving technique involving a takeoff and two and a\nhalf twists before entering the water\n3 TwistsA diving technique involving a takeoff with three twists\nbefore entering the water\n3.5 TwistsA diving technique involving a takeoff with three and\na half twists before entering the water\nEntryA diving technique involving a entry into the water,\ntypically performed at the end of a dive\n0.5 Somersault PikeA diving technique involving a take-off with half a\nsomersault in the pike position before entering the\nwater", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22d20070-131f-4366-a1ea-a17f6dad4fae": {"__data__": {"id_": "22d20070-131f-4366-a1ea-a17f6dad4fae", "embedding": null, "metadata": {"page_label": "27", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e29873e0-4173-4a03-97d9-77929ed8694f", "node_type": "4", "metadata": {"page_label": "27", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "a08abad53ed3ca748545b6889b9a45188f2d6217373a82fce62bd6c133fdf08a", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 27\nTable K: Text descriptions for individual steps in JIGSAWS [11]\nPhase Subaction Description\nSuturingG1 Reaching for needle with right hand\nG2 Positioning a needle to adjust its placement in a particular\nlocation or orientation\nG3Pushing a needle through tissue which involves applying force\nto the needle in order to penetrate and pass through bodily\ntissue\nG4 Transfer a needle from the left hand to the right hand\nG5Moving to the center with the needle in grip which involves\nholding and manipulating the needle to direct it towards the\ncentral area of a target or site\nG6To pull a suture with the left hand is to use the left hand\nto apply tension and draw a length of suture thread through\ntissue\nG8 Orienting a needle involves adjusting the position, angle, or\ndirection of the needle\nG9The action of using the right hand to assist in tightening a\nsuture which involves using the right hand to apply additional\ntension or pressure to the suture thread\nG10 To loosen additional suture which involves manipulating the\nsuture thread in order to reduce the tension or pressure that\nit is exerting on tissue\nG11Dropping the suture at the end and moving to the end points\nwhich involves releasing the suture thread from one hand and\nrepositioning oneself or the needle to prepare for the next\nstep in a medical procedure\nKnot TyingG1 Reaching for needle with right hand\nG11Dropping the suture at the end and moving to the end points\nwhich involves releasing the suture thread from one hand and\nrepositioning oneself or the needle to prepare for the next\nstep in a medical procedure\nG12 Reaching for a needle with the left hand involves extending\nthe left arm and grasping the needle with the hand\nG13Making a C-loop around the right hand involves manipulating\nthe suture thread in a circular motion to form a loop that\nencircles the fingers or hand of the right hand\nG14Reaching for a suture with the right hand involves extending\nthe right arm and grasping the suture material with the hand\nG15Pulling a suture with both hands involves using both hands\nto apply tension and draw a length of suture thread through\ntissue\nNeedle PassingG1 Reaching for needle with right hand\nG2 Positioning a needle to adjust its placement in a particular\nlocation or orientation\nG3Pushing a needle through tissue which involves applying force\nto the needle in order to penetrate and pass through bodily\ntissue\nG4Transfer a needle from the left hand to the right hand which\ninvolves moving the needle from one hand to the other\nG5Moving to the center with the needle in grip involves holding\nand manipulating the needle to direct it towards the central\narea of a target or site\nG6To pull a suture with the left hand is to use the left hand\nto apply tension and draw a length of suture thread through\ntissue\nG8 Orienting a needle involves adjusting the position, angle, or\ndirection of the needle\nG11Dropping the suture at the end and moving to the end points\nwhich involves releasing the suture thread from one hand and\nrepositioning oneself or the needle to prepare for the next\nstep in a medical procedure", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1a23053-8e21-4423-8ccf-617dd0c2bb6c": {"__data__": {"id_": "d1a23053-8e21-4423-8ccf-617dd0c2bb6c", "embedding": null, "metadata": {"page_label": "28", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d098b17-e09b-4ab3-89d0-9e20acdd2c53", "node_type": "4", "metadata": {"page_label": "28", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "9e4fc098d74995b739fdc675e48ae0ebcf42ab840f12c8958ef8106f168aa307", "class_name": "RelatedNodeInfo"}}, "text": "28 A. Majeedi et al.\nTable L: Text descriptions for individual steps in Cataract [38]\nSubaction Description\nIncisionA sharp blade is used to create a precise cut through the\ncornea, which provides intraocular access for instruments. The\nparacentesis is followed by a clear cornea incision which is\nless than 3 mm wide and is large enough to insert the phaco\nhandpiece\nViscous Agent InjectionViscous agent is injected to widen the anterior chamber and to\nprotectthecornealendotheliumandtheintraocularstructures.\nThis is repeated before Phase 8 but is indistinguishable\nRhexisThe anterior capsule of the lens is opened. The surgeon begins\nwith a central radial cut. At the end of the cut, a tear is built\nand allows the anterior capsule to fold over itself. This tear\nis grasped and a flap is carried around in a circular way\nHydrodissectionThe surgeon injects electrolyte solution and epinephrin under\nthe rhexis to separate the peripheral cortex of the lens from\nthe capsule. This facilitates the rotation of the nucleus and\nhydrates the peripheral cortex\nPhacoemulsificiationWith ultrasound power, the phaco tip emulsifies the anterior\ncentral cortex. A deep central linear groove through the nu-\ncleus is made and the lens is cracked into two parts. The lens\nis rotated and chopped into pieces, which can be emulsified.\nDuring this procedure, it is essential to keep the posterior\ncapsule intact\nIrrigation and Aspiration Remaining parts of the cortex are extracted\nCapsule Polishing The posterior capsule is polished in order to avoid opacifica-\ntion of the capsule\nLens Implant Setting-Up The folded artificial lens is inserted. The lens is slowly unfold-\ning and is pushed into the capsular bag\nViscous Agent Removal Viscous elastic agent is removed from anterior chamber and\ncapsule bag\nTonifying and AntibioticsThe corneal incision is hydrated with electrolyte solution\nand antibiotics are injected. This induces temporary stromal\nswelling and closure of incision. Only if it leaks, a suture is\nrequired", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b12957e3-ec7c-488d-a3ac-9158adf3abe3": {"__data__": {"id_": "b12957e3-ec7c-488d-a3ac-9158adf3abe3", "embedding": null, "metadata": {"page_label": "29", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "782a59d3-dff4-402a-af77-b351076cc7e1", "node_type": "4", "metadata": {"page_label": "29", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "6cefa46454c978e11a3d03a38f816becd70dc24fa21ca2eb5668dca2e14e67de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1a16835-965f-43cd-b950-574f9793b380", "node_type": "1", "metadata": {}, "hash": "50e2f691dfdcfadd53363fb6793cc6fed00c2c1ea0442ccacfb059d978e8dec8", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 29\nReferences\n1.Alemi, A.A., Fischer, I., Dillon, J.V., Murphy, K.: Deep variational information\nbottleneck. In: International Conference on Learning Representations (2016)\n2.Bai, Y., Zhou, D., Zhang, S., Wang, J., Ding, E., Guan, Y., Long, Y., Wang,\nJ.: Action quality assessment with temporal parsing transformer. In: Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327,\n2022, Proceedings, Part IV. pp. 422\u2013438. Springer (2022)\n3.Battaglia, P.W., Hamrick, J.B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Ma-\nlinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., et al.: Relational\ninductivebiases,deeplearning,andgraphnetworks.arXivpreprintarXiv:1806.01261\n(2018)\n4.Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the\nkinetics dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 6299\u20136308 (2017)\n5.Chen, C.H., Hu, Y.H., Yen, T.Y., Radwin, R.G.: Automated video exposure assess-\nment of repetitive hand activity level for a load transfer task. Human factors 55(2),\n298\u2013308 (2013)\n6.Chun, S., Oh, S.J., De Rezende, R.S., Kalantidis, Y., Larlus, D.: Probabilistic\nembeddings for cross-modal retrieval. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. pp. 8415\u20138424 (2021)\n7.Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang,\nX., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen,\nX., Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A.,\nYu, H., Petrov, S., Chi, E.H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V.,\nWei, J.: Scaling instruction-finetuned language models (2022)\n8.Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang,\nX., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen,\nX., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang,\nS., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.H.,\nDean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V., Wei, J.: Scaling instruction-\nfinetuned language models. Journal of Machine Learning Research 25(70), 1\u201353\n(2024), http://jmlr.org/papers/v25/23-0870.html\n9.Duvenaud, D.K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-\nGuzik, A., Adams, R.P.: Convolutional networks on graphs for learning molecular\nfingerprints. Advances in neural information processing systems 28(2015)\n10.Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition.\nIn: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp.\n6202\u20136211 (2019).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2740, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1a16835-965f-43cd-b950-574f9793b380": {"__data__": {"id_": "b1a16835-965f-43cd-b950-574f9793b380", "embedding": null, "metadata": {"page_label": "29", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "782a59d3-dff4-402a-af77-b351076cc7e1", "node_type": "4", "metadata": {"page_label": "29", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "6cefa46454c978e11a3d03a38f816becd70dc24fa21ca2eb5668dca2e14e67de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b12957e3-ec7c-488d-a3ac-9158adf3abe3", "node_type": "1", "metadata": {"page_label": "29", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "f092d4a873670590d508ae209f5852c52431766c7c33cf74ed2735a4a87db67c", "class_name": "RelatedNodeInfo"}}, "text": "Journal of Machine Learning Research 25(70), 1\u201353\n(2024), http://jmlr.org/papers/v25/23-0870.html\n9.Duvenaud, D.K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-\nGuzik, A., Adams, R.P.: Convolutional networks on graphs for learning molecular\nfingerprints. Advances in neural information processing systems 28(2015)\n10.Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition.\nIn: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp.\n6202\u20136211 (2019). https://doi.org/10.1109/ICCV.2019.00630\n11.Gao, Y., Vedula, S.S., Reiley, C.E., Ahmidi, N., Varadarajan, B., Lin, H.C., Tao,\nL., Zappella, L., B\u00e9jar, B., Yuh, D.D., et al.: Jhu-isi gesture and skill assessment\nworking set (jigsaws): A surgical activity dataset for human motion modeling. In:\nModeling and Monitoring of Computer Assisted Interventions (M2CAI) \u2013 MICCAI\nWorkshop (2014)\n12.Gordon, A.S.: Automated video assessment of human performance. In: Proceedings\nof AI-ED. vol. 2 (1995)\n13.Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural\nnetworks. In: International conference on machine learning. pp. 1321\u20131330. PMLR\n(2017)", "mimetype": "text/plain", "start_char_idx": 2221, "end_char_idx": 3400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51ee5aac-d2cd-4816-85fd-da138dbce7fb": {"__data__": {"id_": "51ee5aac-d2cd-4816-85fd-da138dbce7fb", "embedding": null, "metadata": {"page_label": "30", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e3f4960-b758-4016-b571-16e5c0a0e28b", "node_type": "4", "metadata": {"page_label": "30", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "1ee051604e53048f59fc37334109c9793ca320f12c9d102e82d67c86a86381df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8240af4-753d-4e20-9848-ca6ffbe7a807", "node_type": "1", "metadata": {}, "hash": "9b8212f21696b03aedc6d4e44d7b77c16a9608be485122b6af5ab843025793f8", "class_name": "RelatedNodeInfo"}}, "text": "30 A. Majeedi et al.\n14.Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave,\nA., Shankar, V., Namkoong, H., Miller, J., et al.: Openclip. Zenodo 4, 5 (2021)\n15.International Swimming Federation (FINA): Fina high diving officials man-\nual. https://resources.fina.org/fina/document/2021/02/03/916b4d2d-1ac9-\n4128-9a42-27abe131b77b/2019-10-14_fina_high_diving_officials_manual.\npdf(2019), accessed on March 12, 2024\n16.Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,\nS., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The\nkinetics human action video dataset (2017)\n17.Kendall, M.G.: A new measure of rank correlation. Biometrika 30(1/2), 81\u201393\n(1938)\n18.Kingma, D.P., Welling, M.: Auto-encoding variational bayes. In: International\nConference on Learning Representations (2014)\n19.Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional\nnetworks. In: International Conference on Learning Representations (2017), https:\n//openreview.net/forum?id=SJU4ayYgl\n20.Li, W., Huang, X., Lu, J., Feng, J., Zhou, J.: Learning probabilistic ordinal embed-\ndingsforuncertainty-awareregression.In:ProceedingsoftheIEEE/CVFConference\non Computer Vision and Pattern Recognition. pp. 13896\u201313905 (2021)\n21.Likert, R.: A technique for the measurement of attitudes. Archives of psychology\n(1932)\n22.Liu, D., Li, Q., Jiang, T., Wang, Y., Miao, R., Shan, F., Li, Z.: Towards unified\nsurgicalskillassessment.In:ProceedingsoftheIEEE/CVFConferenceonComputer\nVision and Pattern Recognition. pp. 9522\u20139531 (2021)\n23.Liu, D., Li, Q., Jiang, T., Wang, Y., Miao, R., Shan, F., Li, Z.: Towards unified\nsurgical skill assessment (2021)\n24.Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th Inter-\nnational Conference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019 (2019)\n25.Martin, J., Regehr, G., Reznick, R., Macrae, H., Murnaghan, J., Hutchison, C.,\nBrown, M.: Objective structured assessment of technical skill (osats) for surgical\nresidents. British journal of surgery 84(2), 273\u2013278 (1997)\n26.Matsuyama, H., Kawaguchi, N., Lim, B.Y.: Iris: Interpretable rubric-informed\nsegmentation for action quality assessment. In: Proceedings of the 28th International\nConference on Intelligent User Interfaces. pp. 368\u2013378 (2023)\n27.Neelakantan, A., Shankar, J., Passos, A., McCallum, A.: Efficient non-parametric\nestimation of multiple embeddings per word in vector space. In: Moschitti, A., Pang,\nB., Daelemans, W. (eds.) Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar,\nA meeting of SIGDAT, a Special Interest Group of the ACL. pp. 1059\u20131069. ACL\n(2014)\n28.Oh, S.J., Gallagher, A.C., Murphy, K.P., Schroff, F., Pan, J., Roth, J.: Modeling\nuncertainty with hedged instance embeddings. In: International Conference on\nLearningRepresentations(2019), https://openreview.net/forum?id=r1xQQhAqKX\n29.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8240af4-753d-4e20-9848-ca6ffbe7a807": {"__data__": {"id_": "d8240af4-753d-4e20-9848-ca6ffbe7a807", "embedding": null, "metadata": {"page_label": "30", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e3f4960-b758-4016-b571-16e5c0a0e28b", "node_type": "4", "metadata": {"page_label": "30", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "1ee051604e53048f59fc37334109c9793ca320f12c9d102e82d67c86a86381df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51ee5aac-d2cd-4816-85fd-da138dbce7fb", "node_type": "1", "metadata": {"page_label": "30", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "a7158f311a98ff96da5eab3a1efb141a4c370d982035111dd21b5d24c7f41eb3", "class_name": "RelatedNodeInfo"}}, "text": "In: Moschitti, A., Pang,\nB., Daelemans, W. (eds.) Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar,\nA meeting of SIGDAT, a Special Interest Group of the ACL. pp. 1059\u20131069. ACL\n(2014)\n28.Oh, S.J., Gallagher, A.C., Murphy, K.P., Schroff, F., Pan, J., Roth, J.: Modeling\nuncertainty with hedged instance embeddings. In: International Conference on\nLearningRepresentations(2019), https://openreview.net/forum?id=r1xQQhAqKX\n29. OpenAI: GPT-4 technical report (2023)\n30.Pan, J.H., Gao, J., Zheng, W.S.: Action assessment by joint relation graphs. In:\nProceedings of the IEEE/CVF international conference on computer vision. pp.\n6331\u20136340 (2019)\n31.Parmar, P., Morris, B.T.: What and how well you performed? a multitask learning\napproachtoactionqualityassessment.In:ProceedingsoftheIEEE/CVFConference\non Computer Vision and Pattern Recognition. pp. 304\u2013313 (2019)", "mimetype": "text/plain", "start_char_idx": 2497, "end_char_idx": 3438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f43b6392-a696-451f-9c0a-d8e3ddffece7": {"__data__": {"id_": "f43b6392-a696-451f-9c0a-d8e3ddffece7", "embedding": null, "metadata": {"page_label": "31", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2167cc40-4b83-47d7-8d92-6c0dc2593687", "node_type": "4", "metadata": {"page_label": "31", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "977b20c5baa9d0e47190654ecba6e2b356d67621a2212c0336dc8161f6bd4269", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc2e5346-f6ae-4c55-9025-4436a0029892", "node_type": "1", "metadata": {}, "hash": "09550e3060ec58efda5c5d0388f792b1593d72225eeacc8b8d08e40e88a855bd", "class_name": "RelatedNodeInfo"}}, "text": "RICA2: Rubric-Informed, Calibrated Assessment of Actions 31\n32. Parmar, P., Tran Morris, B.: Learning to score olympic events. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp.\n20\u201328 (2017)\n33.Pirsiavash, H., Vondrick, C., Torralba, A.: Assessing the quality of actions. In:\nComputer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part VI 13. pp. 556\u2013571. Springer (2014)\n34.Prassas, S., Kwon, Y.H., Sands, W.A.: Biomechanical research in artistic gymnastics:\na review. Sports Biomechanics 5(2), 261\u2013291 (2006)\n35.Qiu, Y., Wang, J., Jin, Z., Chen, H., Zhang, M., Guo, L.: Pose-guided matching\nbased on deep learning for assessing quality of action on rehabilitation training.\nBiomedical Signal Processing and Control 72, 103323 (2022)\n36.Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia, P.,\nLillicrap, T.: A simple neural network module for relational reasoning. Advances in\nneural information processing systems 30(2017)\n37.Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G.: The graph\nneural network model. IEEE transactions on neural networks 20(1), 61\u201380 (2008)\n38.Schoeffmann, K., Taschwer, M., Sarny, S., M\u00fcnzer, B., Primus, M.J., Putzgruber,\nD.: Cataract-101: video dataset of 101 cataract surgeries. In: C\u00e9sar, P., Zink, M.,\nMurray, N. (eds.) Proceedings of the 9th ACM Multimedia Systems Conference,\nMMSys 2018, Amsterdam, The Netherlands, June 12-15, 2018. pp. 421\u2013425. ACM\n(2018)\n39.Shi, Y., Jain, A.K.: Probabilistic face embeddings. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 6902\u20136911 (2019)\n40.Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep\nconditional generative models. Advances in neural information processing systems\n28(2015)\n41.Sun, J.J., Zhao, J., Chen, L.C., Schroff, F., Adam, H., Liu, T.: View-invariant\nprobabilistic embedding for human pose. In: Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16.\npp. 53\u201370. Springer (2020)\n42.Tang, Y., Ni, Z., Zhou, J., Zhang, D., Lu, J., Wu, Y., Zhou, J.: Uncertainty-aware\nscore distribution learning for action quality assessment. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. pp. 9839\u20139848\n(2020)\n43.TISHBY, N.: The information bottleneck method. In: Proc. of the 37th Allerton\nConference on Communication and Computation, 1999 (1999)\n44.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, \u0141., Polosukhin, I.: Attention is all you need. Advances in neural information\nprocessing systems 30(2017)\n45.Vilnis, L., McCallum, A.: Word representations via gaussian embedding.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc2e5346-f6ae-4c55-9025-4436a0029892": {"__data__": {"id_": "dc2e5346-f6ae-4c55-9025-4436a0029892", "embedding": null, "metadata": {"page_label": "31", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2167cc40-4b83-47d7-8d92-6c0dc2593687", "node_type": "4", "metadata": {"page_label": "31", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "977b20c5baa9d0e47190654ecba6e2b356d67621a2212c0336dc8161f6bd4269", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f43b6392-a696-451f-9c0a-d8e3ddffece7", "node_type": "1", "metadata": {"page_label": "31", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "4a220316eeb4d26248b06612ea74a74d1c7f1b43ea65d49e07df59406bd275a2", "class_name": "RelatedNodeInfo"}}, "text": "In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. pp. 9839\u20139848\n(2020)\n43.TISHBY, N.: The information bottleneck method. In: Proc. of the 37th Allerton\nConference on Communication and Computation, 1999 (1999)\n44.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, \u0141., Polosukhin, I.: Attention is all you need. Advances in neural information\nprocessing systems 30(2017)\n45.Vilnis, L., McCallum, A.: Word representations via gaussian embedding. In: Inter-\nnational Conference on Learning Representations (2015)\n46.Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., Wei,\nF.: Text embeddings by weakly-supervised contrastive pre-training (2022)\n47.Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal\nsegment networks: Towards good practices for deep action recognition. In: European\nconference on computer vision. pp. 20\u201336. Springer (2016)\n48.Wang, S., Yang, D., Zhai, P., Chen, C., Zhang, L.: Tsa-net: Tube self-attention\nnetworkforactionqualityassessment.In:Proceedingsofthe29thACMInternational\nConference on Multimedia. pp. 4902\u20134910 (2021)\n49.Waters, T.R., Putz-Anderson, V., Garg, A.: Applications manual for the revised\nniosh lifting equation (1994)", "mimetype": "text/plain", "start_char_idx": 2277, "end_char_idx": 3556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ad1f3a7-42f3-413b-af0d-7a3e1e9e797d": {"__data__": {"id_": "0ad1f3a7-42f3-413b-af0d-7a3e1e9e797d", "embedding": null, "metadata": {"page_label": "32", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7056b59-4dfb-417c-838f-d626167f0a78", "node_type": "4", "metadata": {"page_label": "32", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}, "hash": "b65e08c11a4f9596946c9e3e469beba3425f281c5d99f6df2044d080ea15f8ff", "class_name": "RelatedNodeInfo"}}, "text": "32 A. Majeedi et al.\n50.Xiao, F., Sigal, L., Jae Lee, Y.: Weakly-Supervised Visual Grounding of Phrases\nWith Linguistic Structures. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. pp. 5945\u20135954 (2017)\n51.Xu, C., Fu, Y., Zhang, B., Chen, Z., Jiang, Y.G., Xue, X.: Learning to score figure\nskating sport videos. IEEE transactions on circuits and systems for video technology\n30(12), 4578\u20134590 (2019)\n52.Xu, J., Rao, Y., Yu, X., Chen, G., Zhou, J., Lu, J.: Finediving: A fine-grained dataset\nfor procedure-aware action quality assessment. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 2949\u20132958 (2022)\n53.Xu, K., Li, J., Zhang, M., Du, S.S., ichi Kawarabayashi, K., Jegelka, S.: What can\nneural networks reason about? In: International Conference on Learning Represen-\ntations (2020), https://openreview.net/forum?id=rJxbJeHFPS\n54.XXL, F.T.: (Nov 14th), available at: https://huggingface.co/google/flan-t5-\nxxl(Nov. 2023)\n55.Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for\nskeleton-based action recognition. In: Proceedings of the AAAI conference on\nartificial intelligence. vol. 32 (2018)\n56.Yu, X., Rao, Y., Zhao, W., Lu, J., Zhou, J.: Group-aware contrastive regression\nfor action quality assessment. In: 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV). pp. 7899\u20137908. IEEE Computer Society, Los Alamitos,\nCA, USA (oct 2021)\n57.Zhang, B., Chen, J., Xu, Y., Zhang, H., Yang, X., Geng, X.: Auto-encoding\nscore distribution regression for action quality assessment. Neural Computing and\nApplications pp. 1\u201314 (2023)\n58.Zhang, J., Bargal, S.A., Lin, Z., Brandt, J., Shen, X., Sclaroff, S.: Top-Down\nNeural Attention by Excitation Backprop. International Journal of Computer\nVision 126(10), 1084\u20131102 (Oct 2018)\n59.Zhou, C., Huang, Y.: Uncertainty-driven action quality assessment. arXiv preprint\narXiv:2207.14513 (2022)\n60.Zhou, K., Ma, Y., Shum, H.P.H., Liang, X.: Hierarchical graph convolutional\nnetworks for action quality assessment. IEEE Transactions on Circuits and Systems\nfor Video Technology pp. 1\u20131 (2023)\n61.Zhu, Y., Zhou, Y., Ye, Q., Qiu, Q., Jiao, J.: Soft Proposal Networks for Weakly Su-\npervised Object Localization. In: Proceedings of the IEEE International Conference\non Computer Vision. pp. 1841\u20131850 (2017)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"cf4d1930-a979-4f3a-b327-5fb4d21620f5": {"doc_hash": "32fc0c77aaef98935a28c68f3361cd02bdcccaa5955cbb1602ad1a8aadb77ee6", "ref_doc_id": "eed2da4e-525c-4b99-8661-e2e2fe5e591f"}, "28d0d73f-4ca9-4068-95ca-811fcf5e954f": {"doc_hash": "a0d509a568bc0f13ce747c04ab6ae07a75bc88dcce3580c61e54315580f5857d", "ref_doc_id": "7777fc36-c020-4733-a508-10f829ab52cb"}, "6e409526-8d9f-4ae8-b9dd-54ab38d8956f": {"doc_hash": "750ec69d72889778f3906fa788097bf8fdd16b309f8e83ec29685d8d5e2d6621", "ref_doc_id": "f86c2140-1ce0-4d9f-9a94-1b34d315622a"}, "1d6f4c5e-f753-4c6c-90e5-8a5d61beca95": {"doc_hash": "36c7d296a0ede58c8c957b5b6c36c914f9d15ea5ccdea5c8bf4853702f9d9be6", "ref_doc_id": "6f6f45d7-cf06-4331-b46d-a91e0286cecd"}, "da85bd08-e678-4b8f-9303-fa07ed507789": {"doc_hash": "272214b7dec15b06e4128e4aa7b36d90afbf3cbba6ce1228747dc7d7a213cc2f", "ref_doc_id": "d68a8964-64ff-427c-88ff-94cc0279be9b"}, "56123766-ad09-4aee-9ac8-39c206d5c833": {"doc_hash": "0baa68aacb6931a34a372a8220ff4bc565ab6df5397c01b7f0f6c2eef2d35eea", "ref_doc_id": "3caa7c3f-3a64-4fba-9945-6376d0cc5e46"}, "f06e4faa-5c9d-4dc6-9929-e6bb84f5a950": {"doc_hash": "765db636071d5255edcb8395284845f087585ece1b1eaa1a05bec8e75a94db03", "ref_doc_id": "3562449d-3892-47a6-85ec-331b7ef9e6fb"}, "dd302b04-d174-45e1-8d61-1e9898867131": {"doc_hash": "d5eb1b05dfa52d7c07fd85fd4000622afbd5afb6e8b834eb045a0bf7bdf8689d", "ref_doc_id": "c22366ef-212a-4fad-b03c-ead4c4bb36e4"}, "7ea4623c-f737-4078-8668-43c626cf33b1": {"doc_hash": "47c792fb2463a01a477d4251fa344228c67ea56d857536b222b4f64c36c7c5a4", "ref_doc_id": "c22366ef-212a-4fad-b03c-ead4c4bb36e4"}, "a55e2f7a-e341-4d29-95df-c82ca1a35285": {"doc_hash": "5facf0b714f8ecf20b5fba19362e742d5c5ecd42d1093d968f68a748749cb216", "ref_doc_id": "bcbe8023-a431-4f98-8588-367448f77f40"}, "e51f29a3-e2d3-40e3-87f8-8461870f437e": {"doc_hash": "995e8f5b5a023709eca0854e41c2b1f1b6113a7b5c2ab10a20e343414e768932", "ref_doc_id": "bcbe8023-a431-4f98-8588-367448f77f40"}, "15554537-b5cf-49e3-9833-81924da2ffb1": {"doc_hash": "0702c6de7f16e2b6f9b3273a0371811c785ff72d6804755279eb57775d460abe", "ref_doc_id": "25b0cbe2-a016-42b0-ad32-bd4e2d16b4c5"}, "f754099e-f3ec-4316-947a-52725f2724f7": {"doc_hash": "65a30e78e1f4c999d15b2c430754bcf7664211b79dc485923b4a7534899479d3", "ref_doc_id": "25b0cbe2-a016-42b0-ad32-bd4e2d16b4c5"}, "c0335f57-bd08-4bd3-b7e8-8f429f376fb1": {"doc_hash": "2dfc15023e945f2eb3d77b8bd60067aea386f729398e18efde0d7e696dbdd797", "ref_doc_id": "5108d8cd-9c76-4a63-87d1-5ed724affa87"}, "acc4cf1c-475b-4622-9027-4b3555b45466": {"doc_hash": "ba8b7c554b06fdeab67cfd10be18d74907eda91732f48b80eb9120b81b337d02", "ref_doc_id": "4a2971fa-17f5-4fc9-a0e2-b5157e05635c"}, "b4e278da-f7e5-44dc-bb0c-9dcc92619a9e": {"doc_hash": "b6ebbea2dde3ce47ba5dfdeb9f7868fceaaf1ac431554bcd94690c7e8ad7ca15", "ref_doc_id": "170c0da2-9c4c-4b95-ad57-2de6b8ee262b"}, "2f4e6455-6045-4617-8cd4-094ab3f449dc": {"doc_hash": "9d93d49af7867771af6f594c80fbc193355868dbe4e513729481f1fe8a08f62d", "ref_doc_id": "170c0da2-9c4c-4b95-ad57-2de6b8ee262b"}, "a7a2e3ea-e3af-4b8c-b0dc-e3212f8df20a": {"doc_hash": "1716cd11e6c1fb5f6fa4636397c85011ccdf908ceff45dcd7b5d21f5cf443dc5", "ref_doc_id": "170c0da2-9c4c-4b95-ad57-2de6b8ee262b"}, "ebd0f469-eeaa-4439-ab70-d4f5e07ddbb7": {"doc_hash": "6b652d9186245ba4bc37ec02ee8e3996052c0be8cb141f0daf04ecfb81a1cd2c", "ref_doc_id": "11909171-aa5f-4797-afa7-f0a42f099fec"}, "72c3682b-82d4-43bb-9d50-8c0af0c8e969": {"doc_hash": "36eaacf1bffe5bb087a98766dbc8a8fa8731c7f724e8ab7a5b324beb87307414", "ref_doc_id": "11909171-aa5f-4797-afa7-f0a42f099fec"}, "41e0740c-7900-4363-81f9-151ee1ed08b4": {"doc_hash": "694e3232fa41de41309d49ca971f746536772b78906ba24b4c650f5f084162b3", "ref_doc_id": "28a00dfc-e0b7-460e-a13e-9a18a375b027"}, "9aa3fdd8-eeaa-40ff-ba7e-6dd0587b8145": {"doc_hash": "f6b647099c5061bc4048611868f6e7a7dcd8bdc43ae95973fe2467692bbaac23", "ref_doc_id": "c91681d8-f0aa-4b82-a09f-677a047669b4"}, "c837539e-cf42-48af-8559-8fb5aaa95263": {"doc_hash": "7635f725c197744e217b5aa75566581bc50a46c84db6ff941848727a80babef6", "ref_doc_id": "2157e198-ca13-4dda-832f-ab06c9941905"}, "4da55d8f-6015-4fcb-a2da-ed023c8f9d3e": {"doc_hash": "87a452f8efc499af245a3ad2f83b80e0e3e0aef64b31d252dbda572abe6ed83e", "ref_doc_id": "849c87ee-1a10-402f-9c15-526ced6333dd"}, "f337e99f-6b50-4244-b6ba-d9f3bf87caf2": {"doc_hash": "78d695f42fe0b422f52b8085cc85919c63872794f4fbecb899c262d7a284d9eb", "ref_doc_id": "849c87ee-1a10-402f-9c15-526ced6333dd"}, "59b00f39-92ba-4abd-a84f-9d21694ef09b": {"doc_hash": "412bb7bce76bd6956a01201a31c87a290d13c0984ab546bde39c343961142045", "ref_doc_id": "f5c08f50-f87f-4888-afd8-7bf9455cccd5"}, "2b938455-04fd-45fd-b715-8cb9e35c8276": {"doc_hash": "b598e786e90cbf9906931f164030017906e41adcd5687a680b111f49fdf47f80", "ref_doc_id": "43d2a3c4-b08b-4b17-8a6e-2caa53e215b3"}, "036e2208-93d9-4619-b0a3-eb6e5d23be37": {"doc_hash": "4562b173d3e59952bfdc65448b3a108751feaf95842676f5421f8761a71bd403", "ref_doc_id": "43d2a3c4-b08b-4b17-8a6e-2caa53e215b3"}, "7fe560a9-5a9d-4d25-93d2-97872a1f74f5": {"doc_hash": "fb1cf853da771beb9b19fc03c41a8f3fbf73a87393253554f04a965a97373b5a", "ref_doc_id": "18652fc4-e458-4978-8e4f-c40595a1e447"}, "c7be9412-e657-450c-8c1b-5bc5b748fa76": {"doc_hash": "90b89791f6c24c99da5058a53f930785e4a9b26f613fab85ee5ea1bc57c35a4c", "ref_doc_id": "18652fc4-e458-4978-8e4f-c40595a1e447"}, "88c43357-a485-4077-8b71-c1c4561a25d4": {"doc_hash": "6fa949cb149c8afec4812dbfe2024ff7e9f0e1bbb3a9ad16ac537f1143229908", "ref_doc_id": "86700f1d-e8b7-4f46-9a08-28feafe4e597"}, "7bd6c1bc-d781-4809-a7bf-4bca566ae187": {"doc_hash": "908435d837b6fa2af91ca1ef384a11e743434ec9b0cac8984c54112cb2836f84", "ref_doc_id": "a6c816e5-234e-44af-af10-3c30e3040d2c"}, "5f099a76-4b64-48c2-a38e-191c92f62c3f": {"doc_hash": "8fd78d4851a2c624d725b9142e59e5505a550134e6c0ea37724e1c8888abde99", "ref_doc_id": "bffa0247-445a-4879-9764-43d456e665b3"}, "285ced6c-d49f-4772-a669-85f038fd62d1": {"doc_hash": "5822d39b1dd2d5b2f74b73eeeee8985ff7d9e54018000bbcf95ef9a2e820cbd1", "ref_doc_id": "220e706a-84d9-4816-8cd8-ac85cc98b1cb"}, "20e94ebe-d4ed-4ad7-a6a9-fe92ef8a1db5": {"doc_hash": "cb42b935d5cac1504fdf1ef2fb4d3fef80e2707eadb65669fa192194fc91b9eb", "ref_doc_id": "220e706a-84d9-4816-8cd8-ac85cc98b1cb"}, "a300dcbc-ee74-4493-adc2-e2652c7eb1c9": {"doc_hash": "8ce1df47aaf77d579adf85badcb3d4f03528ee23d0f9d5e8504f6707b77631f4", "ref_doc_id": "2d05d065-454f-4d27-b3f6-7757b378e924"}, "fc29145a-472c-4582-9056-f85fcbe6b4d1": {"doc_hash": "cd7a924f87c38875cc9eeb27c582d155fc367b9d2d5deae6182627fb049257fe", "ref_doc_id": "e187671c-580a-4e56-9b1e-e69b7794150a"}, "77181f23-c61b-45be-a8b0-d2917134b3c4": {"doc_hash": "5c10e5ba40d1c4074fa69741e2b1cb12f0237907ab48e0a39a2bd98f0e2e01ff", "ref_doc_id": "e187671c-580a-4e56-9b1e-e69b7794150a"}, "e4f045dc-3404-4f0c-ada8-73ede9b0ea7f": {"doc_hash": "ac818d94703af475d95c9e3a0a7b53f1624738546d1dcebddd7f88de7f4166eb", "ref_doc_id": "e187671c-580a-4e56-9b1e-e69b7794150a"}, "593a1adf-6011-4407-aad0-e3d92bc8fe54": {"doc_hash": "b8430c48decc4d72326c6e839fa8c4d7427eb3c921fe0822e3652a57db700366", "ref_doc_id": "d692da94-4d4b-4121-ac84-d45039a6112f"}, "956c4cfb-e930-4627-9ee8-ff41f174e26f": {"doc_hash": "6166ea9979ed7d7a1d64377a3ab6c9d6080e0b13592bb68374711025cc20cfbe", "ref_doc_id": "d692da94-4d4b-4121-ac84-d45039a6112f"}, "72a09241-c77e-4d88-b2e9-90b3704e8956": {"doc_hash": "4b46fbecb9e6d6ae3468c608d825444d59d3b1a630661f9ff95354f7e3923cc8", "ref_doc_id": "6421fc3d-f80f-409d-a277-10b22f986828"}, "26f0fb54-6415-483a-ab09-d16aa17974e1": {"doc_hash": "b3d27f975dc8d56305d23f8cdc18049e7043b0d171ba43ed61672b456f38d5db", "ref_doc_id": "4b831842-611e-42c8-8355-073bf1a175ee"}, "e0dbd966-58b2-4926-aeba-14a7434f0f78": {"doc_hash": "e79fddb35cdc811d3b7f45730d13b91041ab4c021f2f734b3efb910db3a49e3d", "ref_doc_id": "44e62ba5-bdd5-44bd-8357-77fd59d5f236"}, "93c66004-e75f-451d-b72e-abb3ee4c6b21": {"doc_hash": "33ae0e1e68a6fc2f6b7229adac54a4367c85df71cf6e623d9b13d96dfe5ca362", "ref_doc_id": "ae68b3c8-cf0f-410a-9f04-59e911ddec88"}, "815d8f04-4092-40d3-8816-b0f1701a1efc": {"doc_hash": "58f60dce160f0d8a5f3e6a647acf0989549c3543fda488e973e6a9a59d3aea65", "ref_doc_id": "8ab73041-59b6-4e04-9dd5-fa84ffc8d6c2"}, "443de2d6-33f7-4be0-91f0-75bcbeb73812": {"doc_hash": "0113f8c6a1bcf2e7c2cb2b3219df9191adf3333afb1891af7c63a206ac2daef5", "ref_doc_id": "758e8005-2195-4b9c-a3d4-9c170af3e57f"}, "0067458d-4541-49bf-8b96-c63f5536a1bd": {"doc_hash": "1cba2876f9b1418df30b5928fdff3bbb35a2365c3bc6c497b8afde4462d5076e", "ref_doc_id": "72f17178-e61b-4d5c-aa01-b91594ce398d"}, "c044fcdf-0ec4-415c-b84f-71ee16c7b1fd": {"doc_hash": "a0109e2a258b4241233cee727132451f926b55e21d3d0bc91b1547337710e6ce", "ref_doc_id": "2cc2e4ec-af32-4ad5-9c8d-80611b5a6198"}, "367c5a8e-a6ce-407c-b659-046e2bf4bf40": {"doc_hash": "7535c4b707e5f20427f20c1adc05683e4b1240b6363b7e269b5bdc336f1abe07", "ref_doc_id": "03af814f-d217-47bc-85b1-be50e3c05c26"}, "ab2579d4-3745-43fb-b017-c89068d358c9": {"doc_hash": "f405cd1783c51a6f4273ec57ad7a66ccd81a4a4911523322e4ce3cfd4cd4d74c", "ref_doc_id": "99d081a5-6478-42cd-b7bc-2659021026cd"}, "f743df6b-2bbc-46e5-ba0f-5b591f166afd": {"doc_hash": "63a1a91a8c228379de7651dfb9e4e28e14c94e6109323386de497e30fbafe8ac", "ref_doc_id": "2833c901-7347-47c2-9990-cbc9b494c379"}, "24492c3a-16d5-4649-92cf-5fa94c2e15a5": {"doc_hash": "26653282af21868349b3222b211b23cad537196dc9e75db91a08e3abf88e803f", "ref_doc_id": "ee867fbe-595f-45e6-8966-2359ac7fe8bd"}, "41629879-0232-4170-ac29-2cf84c2e14d0": {"doc_hash": "7a2fb492a36efeb89698f1f41856e06322035e2a8b50ded79cb44194a4283ad0", "ref_doc_id": "ee867fbe-595f-45e6-8966-2359ac7fe8bd"}, "c612bde8-f4b8-4a61-96f0-2bf38978f703": {"doc_hash": "ce8ca1056ea5f1c9a988c81aee1d4cc31f98b307ddfb00dba150703b9cfeccf8", "ref_doc_id": "8215dadd-cb31-4bb3-85e7-4fc6b7ea4b38"}, "3dba21da-bf24-4309-ae62-7e1fdfae8f4e": {"doc_hash": "a3d6529f878ab85d99c28a88d2f815823598d70e6bec3a13bf0405e7953200af", "ref_doc_id": "6940fce2-b1ed-4a72-83f6-4767da1f0e0e"}, "6e72370a-716d-44fe-b21d-891ef79d7513": {"doc_hash": "6fc8dd1053002421d8b7965cc9d553316902ea95945cbe0757ee726a88a3e4e1", "ref_doc_id": "1e391a2a-e379-454e-94c4-5b1739a68571"}, "e7e0b7ee-c012-4892-b947-9cb922f6c50c": {"doc_hash": "ffb154dfbbf2931ba17bcc1105dc128cb9a5c6ae5093c6e0cc5cdd54c33256c8", "ref_doc_id": "ee9e35e6-eebc-407d-bc7f-c64f5ba77840"}, "98fa0f83-1498-4b5e-a643-f6cd0e044b7c": {"doc_hash": "f98235d1d508b461f6fcdf6b0bcca19eb46d292f734eb9fd65fe17ed8ab46935", "ref_doc_id": "6fe12c2f-1224-4fca-a3c8-2522dab95b4a"}, "fe594650-284e-4529-8ef6-00ae696ba5c6": {"doc_hash": "998615d6510cc61790a7f935e61620c04b617f0f841c9bd047f6ac606690ba06", "ref_doc_id": "4547b7b1-f4d9-4d20-a999-2e7a93890a3e"}, "478e91ac-4668-44f6-863e-6a953e6d3f84": {"doc_hash": "aaf438ba98401e2b47c149fe204700969c8ad4c76bd61bb0af5783aa82115cf8", "ref_doc_id": "6d9eaa18-311c-42a6-b3d3-005448de5845"}, "12fcc714-3ebc-4f55-99f1-63ea40dfe7b5": {"doc_hash": "f6bc68269bed4b849b8245c9d11afc23bf2b6da1aaad3d323d91fc9ef26c51f4", "ref_doc_id": "8eed8365-5a18-41cf-b181-6b695ea371bf"}, "d9cb75a1-e882-4067-8021-911ef5816868": {"doc_hash": "00a4a4978bb5c051faa4eef58fa482aeaad27a4a890ba0c98ee4a55f83654e5d", "ref_doc_id": "a1f32d52-8058-4af1-9c54-b1d11ba9607e"}, "2a16c1f7-15cf-4a2c-9a79-747544364717": {"doc_hash": "866c88fd2ad86b403daa5fbbd42aa7a0a2189cd95bf8e70ccc58e9f639970735", "ref_doc_id": "97f710bb-1917-44de-a9fc-10ed1f455931"}, "68531499-bb42-473e-a587-3de70f2999af": {"doc_hash": "7265e712e94ee2ee587e8228f9b1a6c8d2d05bce22103a92578efc634af16d5e", "ref_doc_id": "76afa07b-51fb-4260-833a-2119d0af8043"}, "4db6adb3-311a-40fc-a2e6-ef228e62e821": {"doc_hash": "4c74ca397288be54a76a01b6ff4fa74349873863ed388c91e6a88c1331361e8c", "ref_doc_id": "a86645d2-43eb-4032-9082-0ad4ee2a96bd"}, "2ba217a4-b060-4688-9c76-1a24414bc785": {"doc_hash": "a421dc0b076bdd42239f4d4e2fe4d9d972c821d351e9260fdd3b1d1999d690f1", "ref_doc_id": "a86645d2-43eb-4032-9082-0ad4ee2a96bd"}, "e9b7b3f9-5f9c-46e0-b437-c4a2db9bd90e": {"doc_hash": "19948d98d4c2c9cb463ee634d30c3f399943a9a65129c0e6a5283bfdb4ed4ce3", "ref_doc_id": "4186645d-8c0c-46cc-8b1f-4fd87e88f0a4"}, "3aedee2b-5a4d-41b1-8c55-88d0c0de21c1": {"doc_hash": "7b02e5fc0f58137353bdda627eae5e6edf0ed5b926d624e96968680c46024ebd", "ref_doc_id": "06317ed0-510c-4175-9b10-408e74e9442b"}, "2f260554-162d-44f3-a5a3-80f4c4200c44": {"doc_hash": "b2feeff76756bd49ebad8959c45b44ab92ee1d43610d7b12e990bcfd769292a6", "ref_doc_id": "2d847736-78e5-4447-9c9d-bb758f86f3fa"}, "22d20070-131f-4366-a1ea-a17f6dad4fae": {"doc_hash": "a08abad53ed3ca748545b6889b9a45188f2d6217373a82fce62bd6c133fdf08a", "ref_doc_id": "e29873e0-4173-4a03-97d9-77929ed8694f"}, "d1a23053-8e21-4423-8ccf-617dd0c2bb6c": {"doc_hash": "9e4fc098d74995b739fdc675e48ae0ebcf42ab840f12c8958ef8106f168aa307", "ref_doc_id": "4d098b17-e09b-4ab3-89d0-9e20acdd2c53"}, "b12957e3-ec7c-488d-a3ac-9158adf3abe3": {"doc_hash": "f092d4a873670590d508ae209f5852c52431766c7c33cf74ed2735a4a87db67c", "ref_doc_id": "782a59d3-dff4-402a-af77-b351076cc7e1"}, "b1a16835-965f-43cd-b950-574f9793b380": {"doc_hash": "339cb48dad6fabb03914c08cb5289bd7a97f754c672de70c5243ffbb35d54ab1", "ref_doc_id": "782a59d3-dff4-402a-af77-b351076cc7e1"}, "51ee5aac-d2cd-4816-85fd-da138dbce7fb": {"doc_hash": "a7158f311a98ff96da5eab3a1efb141a4c370d982035111dd21b5d24c7f41eb3", "ref_doc_id": "1e3f4960-b758-4016-b571-16e5c0a0e28b"}, "d8240af4-753d-4e20-9848-ca6ffbe7a807": {"doc_hash": "a471dc3af830892197057946f0234f3ab9e8f1484cb75a5316e1d36d3792aa4e", "ref_doc_id": "1e3f4960-b758-4016-b571-16e5c0a0e28b"}, "f43b6392-a696-451f-9c0a-d8e3ddffece7": {"doc_hash": "4a220316eeb4d26248b06612ea74a74d1c7f1b43ea65d49e07df59406bd275a2", "ref_doc_id": "2167cc40-4b83-47d7-8d92-6c0dc2593687"}, "dc2e5346-f6ae-4c55-9025-4436a0029892": {"doc_hash": "6b6fd340269bd807833473a236164cc1681969d44572c4c86d9f69743ddc2f1b", "ref_doc_id": "2167cc40-4b83-47d7-8d92-6c0dc2593687"}, "0ad1f3a7-42f3-413b-af0d-7a3e1e9e797d": {"doc_hash": "b65e08c11a4f9596946c9e3e469beba3425f281c5d99f6df2044d080ea15f8ff", "ref_doc_id": "c7056b59-4dfb-417c-838f-d626167f0a78"}}, "docstore/ref_doc_info": {"eed2da4e-525c-4b99-8661-e2e2fe5e591f": {"node_ids": ["cf4d1930-a979-4f3a-b327-5fb4d21620f5"], "metadata": {"page_label": "1", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "7777fc36-c020-4733-a508-10f829ab52cb": {"node_ids": ["28d0d73f-4ca9-4068-95ca-811fcf5e954f"], "metadata": {"page_label": "2", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "f86c2140-1ce0-4d9f-9a94-1b34d315622a": {"node_ids": ["6e409526-8d9f-4ae8-b9dd-54ab38d8956f"], "metadata": {"page_label": "3", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "6f6f45d7-cf06-4331-b46d-a91e0286cecd": {"node_ids": ["1d6f4c5e-f753-4c6c-90e5-8a5d61beca95"], "metadata": {"page_label": "4", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "d68a8964-64ff-427c-88ff-94cc0279be9b": {"node_ids": ["da85bd08-e678-4b8f-9303-fa07ed507789"], "metadata": {"page_label": "5", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "3caa7c3f-3a64-4fba-9945-6376d0cc5e46": {"node_ids": ["56123766-ad09-4aee-9ac8-39c206d5c833"], "metadata": {"page_label": "6", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "3562449d-3892-47a6-85ec-331b7ef9e6fb": {"node_ids": ["f06e4faa-5c9d-4dc6-9929-e6bb84f5a950"], "metadata": {"page_label": "7", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "c22366ef-212a-4fad-b03c-ead4c4bb36e4": {"node_ids": ["dd302b04-d174-45e1-8d61-1e9898867131", "7ea4623c-f737-4078-8668-43c626cf33b1"], "metadata": {"page_label": "8", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "bcbe8023-a431-4f98-8588-367448f77f40": {"node_ids": ["a55e2f7a-e341-4d29-95df-c82ca1a35285", "e51f29a3-e2d3-40e3-87f8-8461870f437e"], "metadata": {"page_label": "9", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "25b0cbe2-a016-42b0-ad32-bd4e2d16b4c5": {"node_ids": ["15554537-b5cf-49e3-9833-81924da2ffb1", "f754099e-f3ec-4316-947a-52725f2724f7"], "metadata": {"page_label": "10", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "5108d8cd-9c76-4a63-87d1-5ed724affa87": {"node_ids": ["c0335f57-bd08-4bd3-b7e8-8f429f376fb1"], "metadata": {"page_label": "11", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "4a2971fa-17f5-4fc9-a0e2-b5157e05635c": {"node_ids": ["acc4cf1c-475b-4622-9027-4b3555b45466"], "metadata": {"page_label": "12", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "170c0da2-9c4c-4b95-ad57-2de6b8ee262b": {"node_ids": ["b4e278da-f7e5-44dc-bb0c-9dcc92619a9e", "2f4e6455-6045-4617-8cd4-094ab3f449dc", "a7a2e3ea-e3af-4b8c-b0dc-e3212f8df20a"], "metadata": {"page_label": "13", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "11909171-aa5f-4797-afa7-f0a42f099fec": {"node_ids": ["ebd0f469-eeaa-4439-ab70-d4f5e07ddbb7", "72c3682b-82d4-43bb-9d50-8c0af0c8e969"], "metadata": {"page_label": "14", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "28a00dfc-e0b7-460e-a13e-9a18a375b027": {"node_ids": ["41e0740c-7900-4363-81f9-151ee1ed08b4"], "metadata": {"page_label": "15", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "c91681d8-f0aa-4b82-a09f-677a047669b4": {"node_ids": ["9aa3fdd8-eeaa-40ff-ba7e-6dd0587b8145"], "metadata": {"page_label": "16", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "2157e198-ca13-4dda-832f-ab06c9941905": {"node_ids": ["c837539e-cf42-48af-8559-8fb5aaa95263"], "metadata": {"page_label": "17", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "849c87ee-1a10-402f-9c15-526ced6333dd": {"node_ids": ["4da55d8f-6015-4fcb-a2da-ed023c8f9d3e", "f337e99f-6b50-4244-b6ba-d9f3bf87caf2"], "metadata": {"page_label": "18", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "f5c08f50-f87f-4888-afd8-7bf9455cccd5": {"node_ids": ["59b00f39-92ba-4abd-a84f-9d21694ef09b"], "metadata": {"page_label": "19", "file_name": "compression.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/compression.pdf", "file_type": "application/pdf", "file_size": 2552238, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "43d2a3c4-b08b-4b17-8a6e-2caa53e215b3": {"node_ids": ["2b938455-04fd-45fd-b715-8cb9e35c8276", "036e2208-93d9-4619-b0a3-eb6e5d23be37"], "metadata": {"page_label": "207", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "18652fc4-e458-4978-8e4f-c40595a1e447": {"node_ids": ["7fe560a9-5a9d-4d25-93d2-97872a1f74f5", "c7be9412-e657-450c-8c1b-5bc5b748fa76"], "metadata": {"page_label": "208", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "86700f1d-e8b7-4f46-9a08-28feafe4e597": {"node_ids": ["88c43357-a485-4077-8b71-c1c4561a25d4"], "metadata": {"page_label": "209", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "a6c816e5-234e-44af-af10-3c30e3040d2c": {"node_ids": ["7bd6c1bc-d781-4809-a7bf-4bca566ae187"], "metadata": {"page_label": "210", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "bffa0247-445a-4879-9764-43d456e665b3": {"node_ids": ["5f099a76-4b64-48c2-a38e-191c92f62c3f"], "metadata": {"page_label": "211", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "220e706a-84d9-4816-8cd8-ac85cc98b1cb": {"node_ids": ["285ced6c-d49f-4772-a669-85f038fd62d1", "20e94ebe-d4ed-4ad7-a6a9-fe92ef8a1db5"], "metadata": {"page_label": "212", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "2d05d065-454f-4d27-b3f6-7757b378e924": {"node_ids": ["a300dcbc-ee74-4493-adc2-e2652c7eb1c9"], "metadata": {"page_label": "213", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "e187671c-580a-4e56-9b1e-e69b7794150a": {"node_ids": ["fc29145a-472c-4582-9056-f85fcbe6b4d1", "77181f23-c61b-45be-a8b0-d2917134b3c4", "e4f045dc-3404-4f0c-ada8-73ede9b0ea7f"], "metadata": {"page_label": "214", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "d692da94-4d4b-4121-ac84-d45039a6112f": {"node_ids": ["593a1adf-6011-4407-aad0-e3d92bc8fe54", "956c4cfb-e930-4627-9ee8-ff41f174e26f"], "metadata": {"page_label": "215", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "6421fc3d-f80f-409d-a277-10b22f986828": {"node_ids": ["72a09241-c77e-4d88-b2e9-90b3704e8956"], "metadata": {"page_label": "216", "file_name": "nitcad.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/nitcad.pdf", "file_type": "application/pdf", "file_size": 1756758, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "4b831842-611e-42c8-8355-073bf1a175ee": {"node_ids": ["26f0fb54-6415-483a-ab09-d16aa17974e1"], "metadata": {"page_label": "1", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "44e62ba5-bdd5-44bd-8357-77fd59d5f236": {"node_ids": ["e0dbd966-58b2-4926-aeba-14a7434f0f78"], "metadata": {"page_label": "2", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "ae68b3c8-cf0f-410a-9f04-59e911ddec88": {"node_ids": ["93c66004-e75f-451d-b72e-abb3ee4c6b21"], "metadata": {"page_label": "3", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "8ab73041-59b6-4e04-9dd5-fa84ffc8d6c2": {"node_ids": ["815d8f04-4092-40d3-8816-b0f1701a1efc"], "metadata": {"page_label": "4", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "758e8005-2195-4b9c-a3d4-9c170af3e57f": {"node_ids": ["443de2d6-33f7-4be0-91f0-75bcbeb73812"], "metadata": {"page_label": "5", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "72f17178-e61b-4d5c-aa01-b91594ce398d": {"node_ids": ["0067458d-4541-49bf-8b96-c63f5536a1bd"], "metadata": {"page_label": "6", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "2cc2e4ec-af32-4ad5-9c8d-80611b5a6198": {"node_ids": ["c044fcdf-0ec4-415c-b84f-71ee16c7b1fd"], "metadata": {"page_label": "7", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "03af814f-d217-47bc-85b1-be50e3c05c26": {"node_ids": ["367c5a8e-a6ce-407c-b659-046e2bf4bf40"], "metadata": {"page_label": "8", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "99d081a5-6478-42cd-b7bc-2659021026cd": {"node_ids": ["ab2579d4-3745-43fb-b017-c89068d358c9"], "metadata": {"page_label": "9", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "2833c901-7347-47c2-9990-cbc9b494c379": {"node_ids": ["f743df6b-2bbc-46e5-ba0f-5b591f166afd"], "metadata": {"page_label": "10", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "ee867fbe-595f-45e6-8966-2359ac7fe8bd": {"node_ids": ["24492c3a-16d5-4649-92cf-5fa94c2e15a5", "41629879-0232-4170-ac29-2cf84c2e14d0"], "metadata": {"page_label": "11", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "8215dadd-cb31-4bb3-85e7-4fc6b7ea4b38": {"node_ids": ["c612bde8-f4b8-4a61-96f0-2bf38978f703"], "metadata": {"page_label": "12", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "6940fce2-b1ed-4a72-83f6-4767da1f0e0e": {"node_ids": ["3dba21da-bf24-4309-ae62-7e1fdfae8f4e"], "metadata": {"page_label": "13", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "1e391a2a-e379-454e-94c4-5b1739a68571": {"node_ids": ["6e72370a-716d-44fe-b21d-891ef79d7513"], "metadata": {"page_label": "14", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "ee9e35e6-eebc-407d-bc7f-c64f5ba77840": {"node_ids": ["e7e0b7ee-c012-4892-b947-9cb922f6c50c"], "metadata": {"page_label": "15", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "6fe12c2f-1224-4fca-a3c8-2522dab95b4a": {"node_ids": ["98fa0f83-1498-4b5e-a643-f6cd0e044b7c"], "metadata": {"page_label": "16", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "4547b7b1-f4d9-4d20-a999-2e7a93890a3e": {"node_ids": ["fe594650-284e-4529-8ef6-00ae696ba5c6"], "metadata": {"page_label": "17", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "6d9eaa18-311c-42a6-b3d3-005448de5845": {"node_ids": ["478e91ac-4668-44f6-863e-6a953e6d3f84"], "metadata": {"page_label": "18", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "8eed8365-5a18-41cf-b181-6b695ea371bf": {"node_ids": ["12fcc714-3ebc-4f55-99f1-63ea40dfe7b5"], "metadata": {"page_label": "19", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "a1f32d52-8058-4af1-9c54-b1d11ba9607e": {"node_ids": ["d9cb75a1-e882-4067-8021-911ef5816868"], "metadata": {"page_label": "20", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "97f710bb-1917-44de-a9fc-10ed1f455931": {"node_ids": ["2a16c1f7-15cf-4a2c-9a79-747544364717"], "metadata": {"page_label": "21", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "76afa07b-51fb-4260-833a-2119d0af8043": {"node_ids": ["68531499-bb42-473e-a587-3de70f2999af"], "metadata": {"page_label": "22", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "a86645d2-43eb-4032-9082-0ad4ee2a96bd": {"node_ids": ["4db6adb3-311a-40fc-a2e6-ef228e62e821", "2ba217a4-b060-4688-9c76-1a24414bc785"], "metadata": {"page_label": "23", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "4186645d-8c0c-46cc-8b1f-4fd87e88f0a4": {"node_ids": ["e9b7b3f9-5f9c-46e0-b437-c4a2db9bd90e"], "metadata": {"page_label": "24", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "06317ed0-510c-4175-9b10-408e74e9442b": {"node_ids": ["3aedee2b-5a4d-41b1-8c55-88d0c0de21c1"], "metadata": {"page_label": "25", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "2d847736-78e5-4447-9c9d-bb758f86f3fa": {"node_ids": ["2f260554-162d-44f3-a5a3-80f4c4200c44"], "metadata": {"page_label": "26", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "e29873e0-4173-4a03-97d9-77929ed8694f": {"node_ids": ["22d20070-131f-4366-a1ea-a17f6dad4fae"], "metadata": {"page_label": "27", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "4d098b17-e09b-4ab3-89d0-9e20acdd2c53": {"node_ids": ["d1a23053-8e21-4423-8ccf-617dd0c2bb6c"], "metadata": {"page_label": "28", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "782a59d3-dff4-402a-af77-b351076cc7e1": {"node_ids": ["b12957e3-ec7c-488d-a3ac-9158adf3abe3", "b1a16835-965f-43cd-b950-574f9793b380"], "metadata": {"page_label": "29", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "1e3f4960-b758-4016-b571-16e5c0a0e28b": {"node_ids": ["51ee5aac-d2cd-4816-85fd-da138dbce7fb", "d8240af4-753d-4e20-9848-ca6ffbe7a807"], "metadata": {"page_label": "30", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "2167cc40-4b83-47d7-8d92-6c0dc2593687": {"node_ids": ["f43b6392-a696-451f-9c0a-d8e3ddffece7", "dc2e5346-f6ae-4c55-9025-4436a0029892"], "metadata": {"page_label": "31", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}, "c7056b59-4dfb-417c-838f-d626167f0a78": {"node_ids": ["0ad1f3a7-42f3-413b-af0d-7a3e1e9e797d"], "metadata": {"page_label": "32", "file_name": "rica.pdf", "file_path": "/hdd4/srinath2/InsightHire/scholar_docs/rica.pdf", "file_type": "application/pdf", "file_size": 3931829, "creation_date": "2025-03-15", "last_modified_date": "2025-03-15"}}}}